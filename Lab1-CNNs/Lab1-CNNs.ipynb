{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d97f7c5d-46f3-4cbd-80ad-f1e50cd65096",
   "metadata": {},
   "source": [
    "# Deep Learning Applications: Laboratory #1\n",
    "\n",
    "In this first laboratory we will work relatively simple architectures to get a feel for working with Deep Models. This notebook is designed to work with PyTorch, but as I said in the introductory lecture: please feel free to use and experiment with whatever tools you like.\n",
    "\n",
    "**Important Notes**:\n",
    "1. Be sure to **document** all of your decisions, as well as your intermediate and final results. Make sure your conclusions and analyses are clearly presented. Don't make us dig into your code or walls of printed results to try to draw conclusions from your code.\n",
    "2. If you use code from someone else (e.g. Github, Stack Overflow, ChatGPT, etc) you **must be transparent about it**. Document your sources and explain how you adapted any partial solutions to creat **your** solution.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ed8906-bd19-4b4f-8b79-4feae355ffd6",
   "metadata": {},
   "source": [
    "## Exercise 1: Warming Up\n",
    "In this series of exercises I want you to try to duplicate (on a small scale) the results of the ResNet paper:\n",
    "\n",
    "> [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385), Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, CVPR 2016.\n",
    "\n",
    "We will do this in steps using a Multilayer Perceptron on MNIST.\n",
    "\n",
    "Recall that the main message of the ResNet paper is that **deeper** networks do not **guarantee** more reduction in training loss (or in validation accuracy). Below you will incrementally build a sequence of experiments to verify this for an MLP. A few guidelines:\n",
    "\n",
    "+ I have provided some **starter** code at the beginning. **NONE** of this code should survive in your solutions. Not only is it **very** badly written, it is also written in my functional style that also obfuscates what it's doing (in part to **discourage** your reuse!). It's just to get you *started*.\n",
    "+ These exercises ask you to compare **multiple** training runs, so it is **really** important that you factor this into your **pipeline**. Using [Tensorboard](https://pytorch.org/tutorials/recipes/recipes/tensorboard_with_pytorch.html) is a **very** good idea -- or, even better [Weights and Biases](https://wandb.ai/site).\n",
    "+ You may work and submit your solutions in **groups of at most two**. Share your ideas with everyone, but the solutions you submit *must be your own*.\n",
    "\n",
    "First some boilerplate to get you started, then on to the actual exercises!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb2b6d1-3df0-464c-9a5f-8c611257a971",
   "metadata": {},
   "source": [
    "### Preface: Some code to get you started\n",
    "\n",
    "What follows is some **very simple** code for training an MLP on MNIST. The point of this code is to get you up and running (and to verify that your Python environment has all needed dependencies).\n",
    "\n",
    "**Note**: As you read through my code and execute it, this would be a good time to think about *abstracting* **your** model definition, and training and evaluation pipelines in order to make it easier to compare performance of different models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6e5e3249349f22d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Programmi\\anaconda3\\envs\\DLA\\python.exe\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab3a8282-2322-4dca-b76e-2f3863bc75fb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T08:09:01.009823Z",
     "start_time": "2025-06-30T08:09:01.006763Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Start with some standard imports.\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import reduce\n",
    "import torch\n",
    "from torchvision.datasets import MNIST\n",
    "from torch.utils.data import Subset\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33cc12cc-8422-47bf-8d8e-0950ac05ae96",
   "metadata": {},
   "source": [
    "#### Data preparation\n",
    "\n",
    "Here is some basic dataset loading, validation splitting code to get you started working with MNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "272a69db-0416-444a-9be4-5f055ff48bbb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Standard MNIST transform.\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "# Load MNIST train and test.\n",
    "ds_train = MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "ds_test = MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Split train into train and validation.\n",
    "val_size = 5000\n",
    "I = np.random.permutation(len(ds_train))\n",
    "ds_val = Subset(ds_train, I[:val_size]) #estraggo elementi per la valutazione, crea un subset dal dataset\n",
    "ds_train = Subset(ds_train, I[val_size:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e05e96-7707-4490-98b8-50cb5e330af1",
   "metadata": {},
   "source": [
    "#### Boilerplate training and evaluation code\n",
    "\n",
    "This is some **very** rough training, evaluation, and plotting code. Again, just to get you started. I will be *very* disappointed if any of this code makes it into your final submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dbcce348-f603-4d57-b9a8-5b1c6eba28ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Function to train a model for a single epoch over the data loader.\n",
    "def train_epoch(model, dl, opt, epoch='Unknown', device='cpu'):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    for (xs, ys) in tqdm(dl, desc=f'Training epoch {epoch}', leave=True):\n",
    "        xs = xs.to(device)\n",
    "        ys = ys.to(device)\n",
    "        opt.zero_grad() # importante azzerare gradienti prima di fare gradient descent\n",
    "        logits = model(xs)\n",
    "        loss = F.cross_entropy(logits, ys)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        losses.append(loss.item())\n",
    "    return np.mean(losses)\n",
    "\n",
    "# Function to evaluate model over all samples in the data loader.\n",
    "def evaluate_model(model, dl, device='cpu'):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    gts = []\n",
    "    for (xs, ys) in tqdm(dl, desc='Evaluating', leave=False):\n",
    "        xs = xs.to(device)\n",
    "        preds = torch.argmax(model(xs), dim=1)\n",
    "        gts.append(ys)\n",
    "        predictions.append(preds.detach().cpu().numpy()) #detach(), il modello crea un grafo di calcolo, necessario per calcolare \n",
    "        #gradiente in modo automatico, se noi faccimao altri calcoli succede che torch.argmax viene aggiunto al grafo e se provo \n",
    "        #a portare questo caloclo sulla cpu o si porta tutto e on si puo fare oppure si fa detach e si stacca solo il tensore e lo si porta sulla cpu\n",
    "        \n",
    "    # Return accuracy score and classification report.\n",
    "    return (accuracy_score(np.hstack(gts), np.hstack(predictions)),\n",
    "            classification_report(np.hstack(gts), np.hstack(predictions), zero_division=0, digits=3))\n",
    "\n",
    "# Simple function to plot the loss curve and validation accuracy.\n",
    "def plot_validation_curves(losses_and_accs):\n",
    "    losses = [x for (x, _) in losses_and_accs]\n",
    "    accs = [x for (_, x) in losses_and_accs]\n",
    "    plt.figure(figsize=(16, 8))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(losses)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Average Training Loss per Epoch')\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(accs)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Validation Accuracy')\n",
    "    plt.title(f'Best Accuracy = {np.max(accs)} @ epoch {np.argmax(accs)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875008c3-306c-4e39-a845-d7bda7862621",
   "metadata": {},
   "source": [
    "#### A basic, parameterized MLP\n",
    "\n",
    "This is a very basic implementation of a Multilayer Perceptron. Don't waste too much time trying to figure out how it works -- the important detail is that it allows you to pass in a list of input, hidden layer, and output *widths*. **Your** implementation should also support this for the exercises to come."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c1e503a-37df-4fb9-94e7-85d0adb494bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, layer_sizes):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([nn.Linear(nin, nout) for (nin, nout) in zip(layer_sizes[:-1], layer_sizes[1:])])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return reduce(lambda f, g: lambda x: g(F.relu(f(x))), self.layers, lambda x: x.flatten(1))(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae06e26-8fa3-414e-a502-8d1c18ba9eb7",
   "metadata": {},
   "source": [
    "#### A *very* minimal training pipeline.\n",
    "\n",
    "Here is some basic training and evaluation code to get you started.\n",
    "\n",
    "**Important**: I cannot stress enough that this is a **terrible** example of how to implement a training pipeline. You can do better!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc89e48f-d8f3-4122-842d-1ff389499854",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Training hyperparameters.\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "epochs = 100\n",
    "lr = 0.0001\n",
    "batch_size = 128\n",
    "\n",
    "# Architecture hyperparameters.\n",
    "input_size = 28*28\n",
    "width = 16\n",
    "depth = 2\n",
    "\n",
    "# Dataloaders.\n",
    "dl_train = torch.utils.data.DataLoader(ds_train, batch_size, shuffle=True, num_workers=4)\n",
    "dl_val   = torch.utils.data.DataLoader(ds_val, batch_size, num_workers=4)\n",
    "dl_test  = torch.utils.data.DataLoader(ds_test, batch_size, shuffle=True, num_workers=4)\n",
    "\n",
    "# Instantiate model and optimizer.\n",
    "model_mlp = MLP([input_size] + [width]*depth + [10]).to(device)\n",
    "opt = torch.optim.Adam(params=model_mlp.parameters(), lr=lr)\n",
    "\n",
    "# Training loop.\n",
    "losses_and_accs = []\n",
    "for epoch in range(epochs):\n",
    "    loss = train_epoch(model_mlp, dl_train, opt, epoch, device=device)\n",
    "    (val_acc, _) = evaluate_model(model_mlp, dl_val, device=device)\n",
    "    losses_and_accs.append((loss, val_acc))\n",
    "\n",
    "# And finally plot the curves.\n",
    "plot_validation_curves(losses_and_accs)\n",
    "print(f'Accuracy report on TEST:\\n {evaluate_model(model_mlp, dl_test, device=device)[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d02a84-a699-4a10-bf22-a0448133360a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "#A better trainig loop\n",
    "def train_model(model, epochs, opt, dl_tria, dl_val, logdir, device='cuda', verbose=False):\n",
    "    writer = SummaryWriter(logdir)\n",
    "    for epoch in range(epochs)\n",
    "        loss = train_epoch(model, dl_train. opt, epoch, device=device)\n",
    "        (val_acc, _) = evaluate_model(model, dl_val, device=device)\n",
    "        writer.add_scalar('Loss/train', loss, epoch)\n",
    "        writer.add_scalar('Acc/val', val_acc, epoch)\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2cad13-ee2c-4e43-b5c7-31760da8c2df",
   "metadata": {},
   "source": [
    "### Exercise 1.1: A baseline MLP\n",
    "\n",
    "Implement a *simple* Multilayer Perceptron to classify the 10 digits of MNIST (e.g. two *narrow* layers). Use my code above as inspiration, but implement your own training pipeline -- you will need it later. Train this model to convergence, monitoring (at least) the loss and accuracy on the training and validation sets for every epoch. Below I include a basic implementation to get you started -- remember that you should write your *own* pipeline!\n",
    "\n",
    "**Note**: This would be a good time to think about *abstracting* your model definition, and training and evaluation pipelines in order to make it easier to compare performance of different models.\n",
    "\n",
    "**Important**: Given the *many* runs you will need to do, and the need to *compare* performance between them, this would **also** be a great point to study how **Tensorboard** or **Weights and Biases** can be used for performance monitoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "002a62c9-6ece-4bd2-ba25-4c14600a077d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.20.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>D:\\Desktop\\Università\\Magistrale AI\\Deep Learning Application\\Laboratories\\LAB_1\\wandb\\run-20250627_114151-mdro8kal</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/leonardobiondi-universit-degli-studi-di-firenze/mnist-basic-mlp/runs/mdro8kal' target=\"_blank\">wandering-durian-1</a></strong> to <a href='https://wandb.ai/leonardobiondi-universit-degli-studi-di-firenze/mnist-basic-mlp' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/leonardobiondi-universit-degli-studi-di-firenze/mnist-basic-mlp' target=\"_blank\">https://wandb.ai/leonardobiondi-universit-degli-studi-di-firenze/mnist-basic-mlp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/leonardobiondi-universit-degli-studi-di-firenze/mnist-basic-mlp/runs/mdro8kal' target=\"_blank\">https://wandb.ai/leonardobiondi-universit-degli-studi-di-firenze/mnist-basic-mlp/runs/mdro8kal</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Dataset sizes - Train: 55000, Val: 5000, Test: 10000\n",
      "Model parameters: 118,282\n",
      "Starting training...\n",
      "\n",
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 860/860 [00:12<00:00, 66.62it/s]\n",
      "Validating: 100%|██████████| 79/79 [00:00<00:00, 85.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2634, Train Acc: 91.97%\n",
      "Val Loss: 0.1378, Val Acc: 96.00%\n",
      "\n",
      "Epoch 2/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 860/860 [00:12<00:00, 70.21it/s]\n",
      "Validating: 100%|██████████| 79/79 [00:00<00:00, 86.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1121, Train Acc: 96.51%\n",
      "Val Loss: 0.1052, Val Acc: 96.58%\n",
      "\n",
      "Epoch 3/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 860/860 [00:13<00:00, 64.66it/s]\n",
      "Validating: 100%|██████████| 79/79 [00:00<00:00, 84.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0797, Train Acc: 97.51%\n",
      "Val Loss: 0.0955, Val Acc: 97.14%\n",
      "\n",
      "Epoch 4/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 860/860 [00:13<00:00, 64.17it/s]\n",
      "Validating: 100%|██████████| 79/79 [00:01<00:00, 78.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0588, Train Acc: 98.09%\n",
      "Val Loss: 0.0928, Val Acc: 97.22%\n",
      "\n",
      "Epoch 5/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 860/860 [00:13<00:00, 65.11it/s]\n",
      "Validating: 100%|██████████| 79/79 [00:00<00:00, 87.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0468, Train Acc: 98.47%\n",
      "Val Loss: 0.0896, Val Acc: 97.56%\n",
      "\n",
      "Epoch 6/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 860/860 [00:12<00:00, 71.14it/s]\n",
      "Validating: 100%|██████████| 79/79 [00:00<00:00, 87.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0407, Train Acc: 98.66%\n",
      "Val Loss: 0.0779, Val Acc: 97.66%\n",
      "\n",
      "Epoch 7/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 860/860 [00:12<00:00, 70.54it/s]\n",
      "Validating: 100%|██████████| 79/79 [00:00<00:00, 80.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0320, Train Acc: 98.92%\n",
      "Val Loss: 0.0900, Val Acc: 97.40%\n",
      "\n",
      "Epoch 8/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 860/860 [00:12<00:00, 70.30it/s]\n",
      "Validating: 100%|██████████| 79/79 [00:00<00:00, 83.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0271, Train Acc: 99.08%\n",
      "Val Loss: 0.0778, Val Acc: 97.88%\n",
      "\n",
      "Epoch 9/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 860/860 [00:12<00:00, 71.36it/s]\n",
      "Validating: 100%|██████████| 79/79 [00:00<00:00, 82.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0262, Train Acc: 99.13%\n",
      "Val Loss: 0.0892, Val Acc: 97.52%\n",
      "\n",
      "Epoch 10/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 860/860 [00:12<00:00, 71.03it/s]\n",
      "Validating: 100%|██████████| 79/79 [00:00<00:00, 86.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0246, Train Acc: 99.18%\n",
      "Val Loss: 0.0913, Val Acc: 97.86%\n",
      "\n",
      "Epoch 11/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 860/860 [00:12<00:00, 71.04it/s]\n",
      "Validating: 100%|██████████| 79/79 [00:00<00:00, 83.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0193, Train Acc: 99.36%\n",
      "Val Loss: 0.1019, Val Acc: 97.58%\n",
      "\n",
      "Epoch 12/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 860/860 [00:12<00:00, 70.49it/s]\n",
      "Validating: 100%|██████████| 79/79 [00:00<00:00, 84.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0185, Train Acc: 99.35%\n",
      "Val Loss: 0.0972, Val Acc: 97.68%\n",
      "\n",
      "Epoch 13/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 860/860 [00:12<00:00, 68.37it/s]\n",
      "Validating: 100%|██████████| 79/79 [00:01<00:00, 59.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0190, Train Acc: 99.34%\n",
      "Val Loss: 0.1192, Val Acc: 97.44%\n",
      "\n",
      "Epoch 14/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 860/860 [00:13<00:00, 62.79it/s]\n",
      "Validating: 100%|██████████| 79/79 [00:00<00:00, 80.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0178, Train Acc: 99.41%\n",
      "Val Loss: 0.1005, Val Acc: 97.82%\n",
      "\n",
      "Epoch 15/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 860/860 [00:13<00:00, 65.44it/s]\n",
      "Validating: 100%|██████████| 79/79 [00:00<00:00, 86.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0155, Train Acc: 99.49%\n",
      "Val Loss: 0.1055, Val Acc: 97.76%\n",
      "\n",
      "Epoch 16/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 860/860 [00:11<00:00, 72.70it/s]\n",
      "Validating: 100%|██████████| 79/79 [00:00<00:00, 84.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0133, Train Acc: 99.59%\n",
      "Val Loss: 0.1139, Val Acc: 97.54%\n",
      "\n",
      "Epoch 17/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 860/860 [00:12<00:00, 71.56it/s]\n",
      "Validating: 100%|██████████| 79/79 [00:00<00:00, 86.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0158, Train Acc: 99.49%\n",
      "Val Loss: 0.1064, Val Acc: 97.96%\n",
      "\n",
      "Epoch 18/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 860/860 [00:12<00:00, 69.68it/s]\n",
      "Validating: 100%|██████████| 79/79 [00:00<00:00, 88.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0128, Train Acc: 99.58%\n",
      "Val Loss: 0.1351, Val Acc: 97.72%\n",
      "\n",
      "Epoch 19/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 860/860 [00:12<00:00, 70.53it/s]\n",
      "Validating: 100%|██████████| 79/79 [00:00<00:00, 87.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0132, Train Acc: 99.58%\n",
      "Val Loss: 0.1074, Val Acc: 98.02%\n",
      "\n",
      "Epoch 20/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 860/860 [00:12<00:00, 71.06it/s]\n",
      "Validating: 100%|██████████| 79/79 [00:00<00:00, 87.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0148, Train Acc: 99.51%\n",
      "Val Loss: 0.1288, Val Acc: 97.72%\n",
      "\n",
      "Epoch 21/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 860/860 [00:12<00:00, 71.46it/s]\n",
      "Validating: 100%|██████████| 79/79 [00:00<00:00, 86.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0106, Train Acc: 99.66%\n",
      "Val Loss: 0.1173, Val Acc: 97.72%\n",
      "\n",
      "Epoch 22/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 860/860 [00:12<00:00, 70.44it/s]\n",
      "Validating: 100%|██████████| 79/79 [00:00<00:00, 86.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0128, Train Acc: 99.61%\n",
      "Val Loss: 0.1171, Val Acc: 98.06%\n",
      "\n",
      "Epoch 23/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 860/860 [00:11<00:00, 73.52it/s]\n",
      "Validating: 100%|██████████| 79/79 [00:01<00:00, 67.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0106, Train Acc: 99.67%\n",
      "Val Loss: 0.1100, Val Acc: 97.96%\n",
      "\n",
      "Epoch 24/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 860/860 [00:11<00:00, 73.06it/s]\n",
      "Validating: 100%|██████████| 79/79 [00:00<00:00, 87.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0138, Train Acc: 99.56%\n",
      "Val Loss: 0.1413, Val Acc: 97.54%\n",
      "\n",
      "Epoch 25/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 860/860 [00:12<00:00, 70.27it/s]\n",
      "Validating: 100%|██████████| 79/79 [00:00<00:00, 85.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0134, Train Acc: 99.57%\n",
      "Val Loss: 0.1360, Val Acc: 97.86%\n",
      "\n",
      "Epoch 26/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 860/860 [00:11<00:00, 72.07it/s]\n",
      "Validating: 100%|██████████| 79/79 [00:00<00:00, 85.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0083, Train Acc: 99.73%\n",
      "Val Loss: 0.1160, Val Acc: 97.94%\n",
      "\n",
      "Epoch 27/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 860/860 [00:12<00:00, 69.49it/s]\n",
      "Validating: 100%|██████████| 79/79 [00:00<00:00, 86.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0091, Train Acc: 99.64%\n",
      "Val Loss: 0.1197, Val Acc: 97.94%\n",
      "\n",
      "Epoch 28/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 860/860 [00:12<00:00, 71.59it/s]\n",
      "Validating: 100%|██████████| 79/79 [00:00<00:00, 82.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0121, Train Acc: 99.65%\n",
      "Val Loss: 0.1368, Val Acc: 97.90%\n",
      "\n",
      "Epoch 29/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 860/860 [00:12<00:00, 71.12it/s]\n",
      "Validating: 100%|██████████| 79/79 [00:00<00:00, 86.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0108, Train Acc: 99.69%\n",
      "Val Loss: 0.1448, Val Acc: 97.92%\n",
      "\n",
      "Epoch 30/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 860/860 [00:11<00:00, 73.19it/s]\n",
      "Validating: 100%|██████████| 79/79 [00:01<00:00, 70.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0112, Train Acc: 99.67%\n",
      "Val Loss: 0.1491, Val Acc: 97.72%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 157/157 [00:01<00:00, 85.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Test Results:\n",
      "Test Accuracy: 97.65%\n",
      "Test Loss: 0.1519\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.993     0.988     0.990       980\n",
      "           1      0.988     0.988     0.988      1135\n",
      "           2      0.987     0.964     0.975      1032\n",
      "           3      0.989     0.970     0.980      1010\n",
      "           4      0.979     0.978     0.978       982\n",
      "           5      0.962     0.982     0.972       892\n",
      "           6      0.974     0.974     0.974       958\n",
      "           7      0.974     0.965     0.970      1028\n",
      "           8      0.966     0.977     0.971       974\n",
      "           9      0.953     0.979     0.966      1009\n",
      "\n",
      "    accuracy                          0.977     10000\n",
      "   macro avg      0.976     0.977     0.976     10000\n",
      "weighted avg      0.977     0.977     0.977     10000\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███</td></tr><tr><td>final_test_accuracy</td><td>▁</td></tr><tr><td>final_test_loss</td><td>▁</td></tr><tr><td>train_accuracy</td><td>▁▅▆▇▇▇▇▇▇█████████████████████</td></tr><tr><td>train_loss</td><td>█▄▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▃▅▅▆▇▆▇▆▇▆▇▆▇▇▆█▇█▇▇██▆▇██▇█▇</td></tr><tr><td>val_loss</td><td>▇▄▃▂▂▁▂▁▂▂▃▃▅▃▄▅▄▇▄▆▅▅▄▇▇▅▅▇██</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>30</td></tr><tr><td>final_test_accuracy</td><td>97.65</td></tr><tr><td>final_test_loss</td><td>0.15187</td></tr><tr><td>train_accuracy</td><td>99.66727</td></tr><tr><td>train_loss</td><td>0.01122</td></tr><tr><td>val_accuracy</td><td>97.72</td></tr><tr><td>val_loss</td><td>0.1491</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">wandering-durian-1</strong> at: <a href='https://wandb.ai/leonardobiondi-universit-degli-studi-di-firenze/mnist-basic-mlp/runs/mdro8kal' target=\"_blank\">https://wandb.ai/leonardobiondi-universit-degli-studi-di-firenze/mnist-basic-mlp/runs/mdro8kal</a><br> View project at: <a href='https://wandb.ai/leonardobiondi-universit-degli-studi-di-firenze/mnist-basic-mlp' target=\"_blank\">https://wandb.ai/leonardobiondi-universit-degli-studi-di-firenze/mnist-basic-mlp</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250627_114151-mdro8kal\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training completed! Test accuracy: 97.65%\n"
     ]
    }
   ],
   "source": [
    "# Exercise 1.1: A baseline MLP (Basic Version)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision.datasets import MNIST\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import torchvision.transforms as transforms\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import classification_report\n",
    "import wandb\n",
    "\n",
    "# Simple MLP model - basic version without regularization\n",
    "class BasicMLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Training function\n",
    "def train_one_epoch(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for data, target in tqdm(dataloader, desc=\"Training\"):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        pred = output.argmax(dim=1)\n",
    "        correct += pred.eq(target).sum().item()\n",
    "        total += target.size(0)\n",
    "\n",
    "    return total_loss / len(dataloader), 100.0 * correct / total\n",
    "\n",
    "# Validation function\n",
    "def validate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target in tqdm(dataloader, desc=\"Validating\"):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            pred = output.argmax(dim=1)\n",
    "            correct += pred.eq(target).sum().item()\n",
    "            total += target.size(0)\n",
    "\n",
    "    return total_loss / len(dataloader), 100.0 * correct / total\n",
    "\n",
    "# Test function\n",
    "def test_model(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target in tqdm(dataloader, desc=\"Testing\"):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += criterion(output, target).item()\n",
    "\n",
    "            pred = output.argmax(dim=1)\n",
    "            correct += pred.eq(target).sum().item()\n",
    "            total += target.size(0)\n",
    "\n",
    "            all_preds.extend(pred.cpu().numpy())\n",
    "            all_targets.extend(target.cpu().numpy())\n",
    "\n",
    "    accuracy = 100.0 * correct / total\n",
    "    avg_loss = test_loss / len(dataloader)\n",
    "\n",
    "    return avg_loss, accuracy, all_preds, all_targets\n",
    "\n",
    "# Simple training pipeline\n",
    "def train_model(model, train_loader, val_loader, optimizer, criterion, epochs, device):\n",
    "    print(\"Starting training...\")\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"\\nEpoch {epoch + 1}/{epochs}\")\n",
    "\n",
    "        # Train\n",
    "        train_loss, train_acc = train_one_epoch(model, train_loader, optimizer, criterion, device)\n",
    "\n",
    "        # Validate\n",
    "        val_loss, val_acc = validate(model, val_loader, criterion, device)\n",
    "\n",
    "        # Log to W&B\n",
    "        wandb.log({\n",
    "            \"epoch\": epoch + 1,\n",
    "            \"train_loss\": train_loss,\n",
    "            \"train_accuracy\": train_acc,\n",
    "            \"val_loss\": val_loss,\n",
    "            \"val_accuracy\": val_acc\n",
    "        })\n",
    "\n",
    "        print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
    "        print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
    "\n",
    "def main():\n",
    "    # Model architecture parameters\n",
    "    input_size = 28 * 28 * 1  # MNIST image dimensions (28x28 pixels, 1 channel)\n",
    "    hidden_size = 128         # Hidden layer size\n",
    "    num_classes = 10          # Number of classes (digits 0-9)\n",
    "    \n",
    "    # Training hyperparameters\n",
    "    epochs = 30\n",
    "    batch_size = 64\n",
    "    learning_rate = 0.001\n",
    "    val_size = 5000\n",
    "    \n",
    "    # Initialize W&B\n",
    "    wandb.init(\n",
    "        project=\"mnist-basic-mlp\",\n",
    "        config={\n",
    "            \"epochs\": epochs,\n",
    "            \"batch_size\": batch_size,\n",
    "            \"learning_rate\": learning_rate,\n",
    "            \"hidden_size\": hidden_size,\n",
    "            \"input_size\": input_size,\n",
    "            \"num_classes\": num_classes,\n",
    "            \"architecture\": \"Basic 2-layer MLP\"\n",
    "        }\n",
    "    )\n",
    "\n",
    "    config = wandb.config\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Data preparation\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "\n",
    "    # Load datasets\n",
    "    train_dataset = MNIST('./data', train=True, download=True, transform=transform)\n",
    "    test_dataset = MNIST('./data', train=False, transform=transform)\n",
    "\n",
    "    # Split training set into train and validation\n",
    "    train_size = len(train_dataset) - val_size\n",
    "    train_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])\n",
    "\n",
    "    # Data loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "    print(f\"Dataset sizes - Train: {len(train_dataset)}, Val: {len(val_dataset)}, Test: {len(test_dataset)}\")\n",
    "\n",
    "    # Model setup\n",
    "    model = BasicMLP(input_size, hidden_size, num_classes).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Log model to W&B\n",
    "    wandb.watch(model)\n",
    "\n",
    "    print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "    # Training pipeline\n",
    "    train_model(model, train_loader, val_loader, optimizer, criterion, epochs, device)\n",
    "\n",
    "    # Test final model\n",
    "    test_loss, test_acc, test_preds, test_targets = test_model(model, test_loader, criterion, device)\n",
    "\n",
    "    print(f\"\\nFinal Test Results:\")\n",
    "    print(f\"Test Accuracy: {test_acc:.2f}%\")\n",
    "    print(f\"Test Loss: {test_loss:.4f}\")\n",
    "\n",
    "    # Detailed classification report\n",
    "    print(f\"\\nClassification Report:\")\n",
    "    print(classification_report(test_targets, test_preds,\n",
    "                              target_names=[str(i) for i in range(10)],\n",
    "                              digits=3))\n",
    "\n",
    "    # Log final results\n",
    "    wandb.log({\n",
    "        \"final_test_accuracy\": test_acc,\n",
    "        \"final_test_loss\": test_loss\n",
    "    })\n",
    "\n",
    "    wandb.finish()\n",
    "\n",
    "    print(f\"\\nTraining completed! Test accuracy: {test_acc:.2f}%\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19d96405-36e7-4074-803c-fb02576cd528",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Currently logged in as: leonardobiondi (leonardobiondi-universit-degli-studi-di-firenze) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.20.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>D:\\Desktop\\Università\\Magistrale AI\\Deep Learning Application\\Laboratories\\LAB_1\\wandb\\run-20250627_111129-3a7yqkmp</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/leonardobiondi-universit-degli-studi-di-firenze/mnist-baseline-mlp/runs/3a7yqkmp' target=\"_blank\">winter-haze-8</a></strong> to <a href='https://wandb.ai/leonardobiondi-universit-degli-studi-di-firenze/mnist-baseline-mlp' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/leonardobiondi-universit-degli-studi-di-firenze/mnist-baseline-mlp' target=\"_blank\">https://wandb.ai/leonardobiondi-universit-degli-studi-di-firenze/mnist-baseline-mlp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/leonardobiondi-universit-degli-studi-di-firenze/mnist-baseline-mlp/runs/3a7yqkmp' target=\"_blank\">https://wandb.ai/leonardobiondi-universit-degli-studi-di-firenze/mnist-baseline-mlp/runs/3a7yqkmp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Dataset sizes - Train: 55000, Val: 5000, Test: 10000\n",
      "Model parameters: 118,282\n",
      "Starting training...\n",
      "\n",
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 860/860 [00:11<00:00, 73.09it/s]\n",
      "Validating: 100%|██████████| 79/79 [00:00<00:00, 84.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3252, Train Acc: 90.07%\n",
      "Val Loss: 0.1292, Val Acc: 96.00%\n",
      "Best Val Acc: 96.00%\n",
      "\n",
      "Epoch 2/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 860/860 [00:12<00:00, 70.37it/s]\n",
      "Validating: 100%|██████████| 79/79 [00:00<00:00, 87.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1587, Train Acc: 95.19%\n",
      "Val Loss: 0.1028, Val Acc: 97.10%\n",
      "Best Val Acc: 97.10%\n",
      "\n",
      "Epoch 3/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 860/860 [00:12<00:00, 70.68it/s]\n",
      "Validating: 100%|██████████| 79/79 [00:00<00:00, 81.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1208, Train Acc: 96.33%\n",
      "Val Loss: 0.0908, Val Acc: 97.06%\n",
      "Best Val Acc: 97.10%\n",
      "\n",
      "Epoch 4/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 860/860 [00:12<00:00, 71.50it/s]\n",
      "Validating: 100%|██████████| 79/79 [00:00<00:00, 87.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1040, Train Acc: 96.82%\n",
      "Val Loss: 0.0859, Val Acc: 97.48%\n",
      "Best Val Acc: 97.48%\n",
      "\n",
      "Epoch 5/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 860/860 [00:12<00:00, 71.05it/s]\n",
      "Validating: 100%|██████████| 79/79 [00:00<00:00, 86.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0979, Train Acc: 96.96%\n",
      "Val Loss: 0.0872, Val Acc: 97.42%\n",
      "Best Val Acc: 97.48%\n",
      "\n",
      "Epoch 6/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 860/860 [00:12<00:00, 70.52it/s]\n",
      "Validating: 100%|██████████| 79/79 [00:00<00:00, 87.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0819, Train Acc: 97.47%\n",
      "Val Loss: 0.0731, Val Acc: 97.72%\n",
      "Best Val Acc: 97.72%\n",
      "\n",
      "Epoch 7/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 860/860 [00:12<00:00, 70.56it/s]\n",
      "Validating: 100%|██████████| 79/79 [00:00<00:00, 87.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0786, Train Acc: 97.52%\n",
      "Val Loss: 0.0778, Val Acc: 97.72%\n",
      "Best Val Acc: 97.72%\n",
      "\n",
      "Epoch 8/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 860/860 [00:12<00:00, 70.51it/s]\n",
      "Validating: 100%|██████████| 79/79 [00:00<00:00, 86.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0718, Train Acc: 97.71%\n",
      "Val Loss: 0.0805, Val Acc: 97.82%\n",
      "Best Val Acc: 97.82%\n",
      "\n",
      "Epoch 9/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 860/860 [00:12<00:00, 71.34it/s]\n",
      "Validating: 100%|██████████| 79/79 [00:00<00:00, 86.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0701, Train Acc: 97.76%\n",
      "Val Loss: 0.0839, Val Acc: 97.70%\n",
      "Best Val Acc: 97.82%\n",
      "\n",
      "Epoch 10/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 860/860 [00:12<00:00, 70.32it/s]\n",
      "Validating: 100%|██████████| 79/79 [00:00<00:00, 84.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0641, Train Acc: 97.97%\n",
      "Val Loss: 0.0843, Val Acc: 97.94%\n",
      "Best Val Acc: 97.94%\n",
      "\n",
      "Epoch 11/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 860/860 [00:11<00:00, 72.36it/s]\n",
      "Validating: 100%|██████████| 79/79 [00:01<00:00, 66.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0588, Train Acc: 98.11%\n",
      "Val Loss: 0.0807, Val Acc: 97.80%\n",
      "Best Val Acc: 97.94%\n",
      "\n",
      "Epoch 12/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 860/860 [00:11<00:00, 72.40it/s]\n",
      "Validating: 100%|██████████| 79/79 [00:00<00:00, 87.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0581, Train Acc: 98.13%\n",
      "Val Loss: 0.0730, Val Acc: 98.10%\n",
      "Best Val Acc: 98.10%\n",
      "\n",
      "Epoch 13/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 860/860 [00:12<00:00, 70.79it/s]\n",
      "Validating: 100%|██████████| 79/79 [00:00<00:00, 87.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0557, Train Acc: 98.15%\n",
      "Val Loss: 0.0809, Val Acc: 97.96%\n",
      "Best Val Acc: 98.10%\n",
      "\n",
      "Epoch 14/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 860/860 [00:12<00:00, 69.83it/s]\n",
      "Validating: 100%|██████████| 79/79 [00:00<00:00, 81.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0545, Train Acc: 98.19%\n",
      "Val Loss: 0.0775, Val Acc: 97.92%\n",
      "Best Val Acc: 98.10%\n",
      "\n",
      "Epoch 15/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 860/860 [00:12<00:00, 69.80it/s]\n",
      "Validating: 100%|██████████| 79/79 [00:01<00:00, 78.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0497, Train Acc: 98.41%\n",
      "Val Loss: 0.0873, Val Acc: 97.70%\n",
      "Best Val Acc: 98.10%\n",
      "\n",
      "Epoch 16/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 860/860 [00:13<00:00, 64.52it/s]\n",
      "Validating: 100%|██████████| 79/79 [00:00<00:00, 85.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0481, Train Acc: 98.43%\n",
      "Val Loss: 0.0894, Val Acc: 97.72%\n",
      "Best Val Acc: 98.10%\n",
      "\n",
      "Epoch 17/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 860/860 [00:12<00:00, 71.05it/s]\n",
      "Validating: 100%|██████████| 79/79 [00:00<00:00, 86.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0499, Train Acc: 98.37%\n",
      "Val Loss: 0.0839, Val Acc: 98.04%\n",
      "Best Val Acc: 98.10%\n",
      "\n",
      "Epoch 18/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 860/860 [00:12<00:00, 69.45it/s]\n",
      "Validating: 100%|██████████| 79/79 [00:00<00:00, 86.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0479, Train Acc: 98.48%\n",
      "Val Loss: 0.0810, Val Acc: 97.96%\n",
      "Best Val Acc: 98.10%\n",
      "\n",
      "Epoch 19/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 860/860 [00:12<00:00, 71.24it/s]\n",
      "Validating: 100%|██████████| 79/79 [00:01<00:00, 72.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0442, Train Acc: 98.61%\n",
      "Val Loss: 0.0859, Val Acc: 97.88%\n",
      "Best Val Acc: 98.10%\n",
      "\n",
      "Epoch 20/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 860/860 [00:12<00:00, 68.74it/s]\n",
      "Validating: 100%|██████████| 79/79 [00:00<00:00, 86.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0444, Train Acc: 98.63%\n",
      "Val Loss: 0.0828, Val Acc: 98.04%\n",
      "Best Val Acc: 98.10%\n",
      "\n",
      "Epoch 21/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 860/860 [00:12<00:00, 70.08it/s]\n",
      "Validating: 100%|██████████| 79/79 [00:00<00:00, 82.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0442, Train Acc: 98.55%\n",
      "Val Loss: 0.0932, Val Acc: 97.84%\n",
      "Best Val Acc: 98.10%\n",
      "\n",
      "Epoch 22/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 860/860 [00:12<00:00, 69.07it/s]\n",
      "Validating: 100%|██████████| 79/79 [00:00<00:00, 80.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0423, Train Acc: 98.59%\n",
      "Val Loss: 0.0904, Val Acc: 97.88%\n",
      "Best Val Acc: 98.10%\n",
      "\n",
      "Epoch 23/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 860/860 [00:12<00:00, 70.30it/s]\n",
      "Validating: 100%|██████████| 79/79 [00:00<00:00, 86.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0408, Train Acc: 98.70%\n",
      "Val Loss: 0.0906, Val Acc: 98.18%\n",
      "Best Val Acc: 98.18%\n",
      "\n",
      "Epoch 24/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 860/860 [00:12<00:00, 70.15it/s]\n",
      "Validating: 100%|██████████| 79/79 [00:00<00:00, 80.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0398, Train Acc: 98.67%\n",
      "Val Loss: 0.0854, Val Acc: 97.98%\n",
      "Best Val Acc: 98.18%\n",
      "\n",
      "Epoch 25/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 860/860 [00:12<00:00, 70.30it/s]\n",
      "Validating: 100%|██████████| 79/79 [00:00<00:00, 83.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0403, Train Acc: 98.67%\n",
      "Val Loss: 0.0900, Val Acc: 98.00%\n",
      "Best Val Acc: 98.18%\n",
      "\n",
      "Epoch 26/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 860/860 [00:12<00:00, 69.22it/s]\n",
      "Validating: 100%|██████████| 79/79 [00:00<00:00, 82.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0412, Train Acc: 98.66%\n",
      "Val Loss: 0.0852, Val Acc: 98.00%\n",
      "Best Val Acc: 98.18%\n",
      "\n",
      "Epoch 27/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 860/860 [00:12<00:00, 69.70it/s]\n",
      "Validating: 100%|██████████| 79/79 [00:00<00:00, 82.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0417, Train Acc: 98.67%\n",
      "Val Loss: 0.0825, Val Acc: 98.00%\n",
      "Best Val Acc: 98.18%\n",
      "\n",
      "Epoch 28/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 860/860 [00:12<00:00, 66.47it/s]\n",
      "Validating: 100%|██████████| 79/79 [00:01<00:00, 64.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0387, Train Acc: 98.73%\n",
      "Val Loss: 0.0867, Val Acc: 98.02%\n",
      "Best Val Acc: 98.18%\n",
      "\n",
      "Epoch 29/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 860/860 [00:13<00:00, 64.19it/s]\n",
      "Validating: 100%|██████████| 79/79 [00:01<00:00, 62.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0390, Train Acc: 98.69%\n",
      "Val Loss: 0.0879, Val Acc: 98.14%\n",
      "Best Val Acc: 98.18%\n",
      "\n",
      "Epoch 30/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 860/860 [00:12<00:00, 68.83it/s]\n",
      "Validating: 100%|██████████| 79/79 [00:00<00:00, 84.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0377, Train Acc: 98.76%\n",
      "Val Loss: 0.0954, Val Acc: 98.06%\n",
      "Best Val Acc: 98.18%\n",
      "\n",
      "Loading best model (Val Acc: 98.18%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 157/157 [00:02<00:00, 72.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Test Results:\n",
      "Test Accuracy: 98.12%\n",
      "Test Loss: 0.0747\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.984     0.993     0.988       980\n",
      "           1      0.986     0.995     0.990      1135\n",
      "           2      0.982     0.982     0.982      1032\n",
      "           3      0.980     0.983     0.982      1010\n",
      "           4      0.981     0.981     0.981       982\n",
      "           5      0.984     0.969     0.976       892\n",
      "           6      0.981     0.986     0.984       958\n",
      "           7      0.973     0.978     0.975      1028\n",
      "           8      0.982     0.975     0.979       974\n",
      "           9      0.979     0.968     0.974      1009\n",
      "\n",
      "    accuracy                          0.981     10000\n",
      "   macro avg      0.981     0.981     0.981     10000\n",
      "weighted avg      0.981     0.981     0.981     10000\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>best_val_accuracy</td><td>▁</td></tr><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███</td></tr><tr><td>final_test_accuracy</td><td>▁</td></tr><tr><td>final_test_loss</td><td>▁</td></tr><tr><td>train_accuracy</td><td>▁▅▆▆▇▇▇▇▇▇▇▇██████████████████</td></tr><tr><td>train_loss</td><td>█▄▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▅▄▆▆▇▇▇▆▇▇█▇▇▆▇█▇▇█▇▇█▇▇▇▇▇██</td></tr><tr><td>val_loss</td><td>█▅▃▃▃▁▂▂▂▂▂▁▂▂▃▃▂▂▃▂▄▃▃▃▃▃▂▃▃▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_val_accuracy</td><td>98.18</td></tr><tr><td>epoch</td><td>30</td></tr><tr><td>final_test_accuracy</td><td>98.12</td></tr><tr><td>final_test_loss</td><td>0.07471</td></tr><tr><td>train_accuracy</td><td>98.75636</td></tr><tr><td>train_loss</td><td>0.03767</td></tr><tr><td>val_accuracy</td><td>98.06</td></tr><tr><td>val_loss</td><td>0.09542</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">winter-haze-8</strong> at: <a href='https://wandb.ai/leonardobiondi-universit-degli-studi-di-firenze/mnist-baseline-mlp/runs/3a7yqkmp' target=\"_blank\">https://wandb.ai/leonardobiondi-universit-degli-studi-di-firenze/mnist-baseline-mlp/runs/3a7yqkmp</a><br> View project at: <a href='https://wandb.ai/leonardobiondi-universit-degli-studi-di-firenze/mnist-baseline-mlp' target=\"_blank\">https://wandb.ai/leonardobiondi-universit-degli-studi-di-firenze/mnist-baseline-mlp</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250627_111129-3a7yqkmp\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training completed! Best validation accuracy: 98.18%\n"
     ]
    }
   ],
   "source": [
    "# Exercise 1.1: Improved MLP (with Dropout & Best Model Saving)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision.datasets import MNIST\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import torchvision.transforms as transforms\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import classification_report\n",
    "import wandb\n",
    "\n",
    "# Simple MLP model - following exercise requirement for \"narrow\" layers\n",
    "class BaselineMLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, num_classes)\n",
    "        self.dropout = nn.Dropout(0.2)  # Basic regularization\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Training function\n",
    "def train_one_epoch(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for data, target in tqdm(dataloader, desc=\"Training\"):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        pred = output.argmax(dim=1)\n",
    "        correct += pred.eq(target).sum().item()\n",
    "        total += target.size(0)\n",
    "\n",
    "    return total_loss / len(dataloader), 100.0 * correct / total\n",
    "\n",
    "# Validation function\n",
    "def validate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target in tqdm(dataloader, desc=\"Validating\"):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            pred = output.argmax(dim=1)\n",
    "            correct += pred.eq(target).sum().item()\n",
    "            total += target.size(0)\n",
    "\n",
    "    return total_loss / len(dataloader), 100.0 * correct / total\n",
    "\n",
    "# Test function with detailed metrics\n",
    "def test_model(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target in tqdm(dataloader, desc=\"Testing\"):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += criterion(output, target).item()\n",
    "\n",
    "            pred = output.argmax(dim=1)\n",
    "            correct += pred.eq(target).sum().item()\n",
    "            total += target.size(0)\n",
    "\n",
    "            all_preds.extend(pred.cpu().numpy())\n",
    "            all_targets.extend(target.cpu().numpy())\n",
    "\n",
    "    accuracy = 100.0 * correct / total\n",
    "    avg_loss = test_loss / len(dataloader)\n",
    "\n",
    "    return avg_loss, accuracy, all_preds, all_targets\n",
    "\n",
    "# Complete training pipeline\n",
    "def train_model(model, train_loader, val_loader, optimizer, criterion, config, device):\n",
    "    print(\"Starting training...\")\n",
    "    best_val_acc = 0\n",
    "\n",
    "    for epoch in range(config.epochs):\n",
    "        print(f\"\\nEpoch {epoch + 1}/{config.epochs}\")\n",
    "\n",
    "        # Train\n",
    "        train_loss, train_acc = train_one_epoch(model, train_loader, optimizer, criterion, device)\n",
    "\n",
    "        # Validate\n",
    "        val_loss, val_acc = validate(model, val_loader, criterion, device)\n",
    "\n",
    "        # Track best model\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "\n",
    "        # Log to W&B\n",
    "        wandb.log({\n",
    "            \"epoch\": epoch + 1,\n",
    "            \"train_loss\": train_loss,\n",
    "            \"train_accuracy\": train_acc,\n",
    "            \"val_loss\": val_loss,\n",
    "            \"val_accuracy\": val_acc\n",
    "        })\n",
    "\n",
    "        print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
    "        print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
    "        print(f\"Best Val Acc: {best_val_acc:.2f}%\")\n",
    "\n",
    "    return best_val_acc\n",
    "\n",
    "def main():\n",
    "    # Model architecture parameters\n",
    "    input_size = 28 * 28 * 1  # MNIST image dimensions (28x28 pixels, 1 channel)\n",
    "    hidden_size = 128         # Hidden layer size\n",
    "    num_classes = 10          # Number of classes (digits 0-9)\n",
    "    \n",
    "    # Training hyperparameters\n",
    "    epochs = 30\n",
    "    batch_size = 64\n",
    "    learning_rate = 0.001\n",
    "    val_size = 5000\n",
    "    \n",
    "    # Initialize W&B\n",
    "    wandb.init(\n",
    "        project=\"mnist-baseline-mlp\",\n",
    "        config={\n",
    "            \"epochs\": epochs,\n",
    "            \"batch_size\": batch_size,\n",
    "            \"learning_rate\": learning_rate,\n",
    "            \"hidden_size\": hidden_size,\n",
    "            \"input_size\": input_size,\n",
    "            \"num_classes\": num_classes,\n",
    "            \"architecture\": \"2-layer MLP\"\n",
    "        }\n",
    "    )\n",
    "\n",
    "    config = wandb.config\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Data preparation\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "\n",
    "    # Load datasets\n",
    "    train_dataset = MNIST('./data', train=True, download=True, transform=transform)\n",
    "    test_dataset = MNIST('./data', train=False, transform=transform)\n",
    "\n",
    "    # Split training set into train and validation\n",
    "    train_size = len(train_dataset) - val_size\n",
    "    train_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])\n",
    "\n",
    "    # Data loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "    print(f\"Dataset sizes - Train: {len(train_dataset)}, Val: {len(val_dataset)}, Test: {len(test_dataset)}\")\n",
    "\n",
    "    # Model setup\n",
    "    model = BaselineMLP(input_size, hidden_size, num_classes).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Log model to W&B\n",
    "    wandb.watch(model)\n",
    "\n",
    "    print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "    # Training pipeline\n",
    "    best_val_acc = train_model(model, train_loader, val_loader, optimizer, criterion, config, device)\n",
    "\n",
    "    # Load best model and test\n",
    "    print(f\"\\nLoading best model (Val Acc: {best_val_acc:.2f}%)\")\n",
    "    model.load_state_dict(torch.load('best_model.pth'))\n",
    "\n",
    "    test_loss, test_acc, test_preds, test_targets = test_model(model, test_loader, criterion, device)\n",
    "\n",
    "    print(f\"\\nFinal Test Results:\")\n",
    "    print(f\"Test Accuracy: {test_acc:.2f}%\")\n",
    "    print(f\"Test Loss: {test_loss:.4f}\")\n",
    "\n",
    "    # Detailed classification report\n",
    "    print(f\"\\nClassification Report:\")\n",
    "    print(classification_report(test_targets, test_preds,\n",
    "                              target_names=[str(i) for i in range(10)],\n",
    "                              digits=3))\n",
    "\n",
    "    # Log final results\n",
    "    wandb.log({\n",
    "        \"best_val_accuracy\": best_val_acc,\n",
    "        \"final_test_accuracy\": test_acc,\n",
    "        \"final_test_loss\": test_loss\n",
    "    })\n",
    "\n",
    "    wandb.finish()\n",
    "\n",
    "    print(f\"\\nTraining completed! Best validation accuracy: {best_val_acc:.2f}%\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9415449-214c-4276-9997-c3adc7921f75",
   "metadata": {},
   "source": [
    "## Considerazioni Finali\n",
    "\n",
    "### Risultati a Confronto\n",
    "| Metrica         | Basic MLP | Improved MLP | Differenza |\n",
    "|-----------------|-----------|--------------|------------|\n",
    "| Test Accuracy   | 97.65%    | 98.12%       | +0.47%     |\n",
    "| Test Loss       | 0.1519    | 0.0747       | -50.8%     |\n",
    "| Train Accuracy  | 99.67%    | 98.76%       | -0.91%     |\n",
    "| Val Accuracy    | 97.72%    | 98.18%       | +0.46%     |\n",
    "| Train-Test Gap  | 2.02%     | 0.64%        | -68%       |\n",
    "\n",
    "---\n",
    "\n",
    "### Analisi dell'Overfitting\n",
    "\n",
    "**Basic MLP: Leggero Overfitting**\n",
    "\n",
    "- Train accuracy: 99.67% vs Test: 97.65% (gap del 2.02%)\n",
    "- Validation loss in crescita nelle epoche finali\n",
    "- Il modello memorizza i dati di training senza generalizzare bene\n",
    "\n",
    "  \n",
    "**Enhanced MLP: Generalizzazione Ottima**\n",
    "\n",
    "- Train accuracy: 98.76% vs Test: 98.12% (gap di soli 0.64%)\n",
    "- Curve di learning più stabili\n",
    "- Migliore capacità di generalizzazione su dati non visti\n",
    "\n",
    "---\n",
    "\n",
    "### Impatto della versione migliorata\n",
    "\n",
    "**Dropout (0.2):**\n",
    "\n",
    "- Previene l'overfitting riducendo la dipendenza eccessiva tra i neuroni\n",
    "- Migliora la robustezza del modello durante il training\n",
    "- Trade-off controllato: leggera riduzione della train accuracy per migliore generalizzazione\n",
    "\n",
    "**Best Model Saving:**\n",
    "\n",
    "- Selezione ottimale: usa il modello con migliore validation performance\n",
    "- Evita il deterioramento delle prestazioni nelle epoche finali\n",
    "- Early stopping implicito: ferma l'overfitting al momento giusto\n",
    "\n",
    "---\n",
    "\n",
    "### Qualità delle Predizioni\n",
    "\n",
    "**improved MLP mostra:**\n",
    "\n",
    "- Test loss dimezzata: 0.0747 vs 0.1519 (maggiore confidenza nelle predizioni)\n",
    "- Performance più uniformi tra le classi nel classification report\n",
    "- Stabilità superiore nelle metriche validation\n",
    "\n",
    "---\n",
    "\n",
    "### Conclusione\n",
    "I risultati ottenuti confermano che un **MLP ben addestrato** è già estremamente efficace per la classificazione su **MNIST**.  \n",
    "L'**Improved MLP**, con le sue tecniche di regolarizzazione, raggiunge prestazioni eccellenti (**98.12% di accuratezza**) dimostrando come l'implementazione corretta di pratiche standard possa fare la differenza tra un modello base (Basic MLP con **2.02% di overfitting**) e uno che **generalizza efficacemente**.\n",
    "\n",
    "Tuttavia, rimane spazio per ulteriori miglioramenti.  \n",
    "Architetture più avanzate come le **Convolutional Neural Networks (CNN)** potrebbero sfruttare la **struttura spaziale delle immagini** per raggiungere accuratezze superiori al **99%**, mentre l'implementazione di **early stopping esplicito** (invece del semplice best model saving) potrebbe ottimizzare ulteriormente l'efficienza del training e la stabilità della convergenza.\n",
    "\n",
    "Nel complesso, questo confronto evidenzia come anche su dataset **relativamente semplici** come MNIST, la differenza tra un approccio **naive** e uno **metodologicamente rigoroso** sia **sostanziale e misurabile**, ponendo le basi per affrontare sfide più complesse con maggiore **consapevolezza tecnica**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cbaff86-1467-4e25-b02c-f91f525ba6e6",
   "metadata": {},
   "source": [
    "### Exercise 1.2: Adding Residual Connections\n",
    "\n",
    "Implement a variant of your parameterized MLP network to support **residual** connections. Your network should be defined as a composition of **residual MLP** blocks that have one or more linear layers and add a skip connection from the block input to the output of the final linear layer.\n",
    "\n",
    "**Compare** the performance (in training/validation loss and test accuracy) of your MLP and ResidualMLP for a range of depths. Verify that deeper networks **with** residual connections are easier to train than a network of the same depth **without** residual connections.\n",
    "\n",
    "**For extra style points**: See if you can explain by analyzing the gradient magnitudes on a single training batch *why* this is the case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f21735c0-5315-404e-add4-8d20cd040587",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.20.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>D:\\Desktop\\Università\\Magistrale AI\\Deep Learning Application\\Laboratories\\LAB_1\\wandb\\run-20250627_122943-wz1tujh1</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/leonardobiondi-universit-degli-studi-di-firenze/mnist-residual-mlp-comparison/runs/wz1tujh1' target=\"_blank\">sandy-cloud-1</a></strong> to <a href='https://wandb.ai/leonardobiondi-universit-degli-studi-di-firenze/mnist-residual-mlp-comparison' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/leonardobiondi-universit-degli-studi-di-firenze/mnist-residual-mlp-comparison' target=\"_blank\">https://wandb.ai/leonardobiondi-universit-degli-studi-di-firenze/mnist-residual-mlp-comparison</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/leonardobiondi-universit-degli-studi-di-firenze/mnist-residual-mlp-comparison/runs/wz1tujh1' target=\"_blank\">https://wandb.ai/leonardobiondi-universit-degli-studi-di-firenze/mnist-residual-mlp-comparison/runs/wz1tujh1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Dataset sizes - Train: 55000, Val: 5000, Test: 10000\n",
      "\n",
      "Creating models with depth 1...\n",
      "Standard MLP parameters: 134,794\n",
      "Residual MLP parameters: 134,794\n",
      "\n",
      "============================================================\n",
      "Training Networks with Depth: 1\n",
      "============================================================\n",
      "\n",
      "Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard MLP  - Train Loss: 0.2829, Train Acc: 91.35%, Val Loss: 0.1275, Val Acc: 96.02%\n",
      "Residual MLP  - Train Loss: 0.2493, Train Acc: 92.55%, Val Loss: 0.1393, Val Acc: 95.64%\n",
      "\n",
      "Epoch 2/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard MLP  - Train Loss: 0.1173, Train Acc: 96.38%, Val Loss: 0.0926, Val Acc: 97.32%\n",
      "Residual MLP  - Train Loss: 0.1056, Train Acc: 96.73%, Val Loss: 0.1003, Val Acc: 96.78%\n",
      "\n",
      "Epoch 3/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard MLP  - Train Loss: 0.0862, Train Acc: 97.33%, Val Loss: 0.0895, Val Acc: 97.22%\n",
      "Residual MLP  - Train Loss: 0.0749, Train Acc: 97.62%, Val Loss: 0.0888, Val Acc: 97.14%\n",
      "\n",
      "Epoch 4/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard MLP  - Train Loss: 0.0688, Train Acc: 97.83%, Val Loss: 0.0942, Val Acc: 97.22%\n",
      "Residual MLP  - Train Loss: 0.0571, Train Acc: 98.18%, Val Loss: 0.0943, Val Acc: 97.36%\n",
      "\n",
      "Epoch 5/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard MLP  - Train Loss: 0.0564, Train Acc: 98.18%, Val Loss: 0.0806, Val Acc: 97.70%\n",
      "Residual MLP  - Train Loss: 0.0495, Train Acc: 98.38%, Val Loss: 0.0882, Val Acc: 97.64%\n",
      "\n",
      "Epoch 6/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard MLP  - Train Loss: 0.0488, Train Acc: 98.39%, Val Loss: 0.0845, Val Acc: 97.74%\n",
      "Residual MLP  - Train Loss: 0.0392, Train Acc: 98.70%, Val Loss: 0.0850, Val Acc: 97.66%\n",
      "\n",
      "Epoch 7/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard MLP  - Train Loss: 0.0431, Train Acc: 98.58%, Val Loss: 0.1006, Val Acc: 97.26%\n",
      "Residual MLP  - Train Loss: 0.0351, Train Acc: 98.82%, Val Loss: 0.0758, Val Acc: 97.96%\n",
      "\n",
      "Epoch 8/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard MLP  - Train Loss: 0.0376, Train Acc: 98.74%, Val Loss: 0.1240, Val Acc: 96.74%\n",
      "Residual MLP  - Train Loss: 0.0305, Train Acc: 99.02%, Val Loss: 0.0853, Val Acc: 97.84%\n",
      "\n",
      "Epoch 9/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard MLP  - Train Loss: 0.0362, Train Acc: 98.82%, Val Loss: 0.0979, Val Acc: 97.72%\n",
      "Residual MLP  - Train Loss: 0.0265, Train Acc: 99.12%, Val Loss: 0.1063, Val Acc: 97.46%\n",
      "\n",
      "Epoch 10/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard MLP  - Train Loss: 0.0317, Train Acc: 98.94%, Val Loss: 0.1052, Val Acc: 97.62%\n",
      "Residual MLP  - Train Loss: 0.0253, Train Acc: 99.12%, Val Loss: 0.0954, Val Acc: 97.50%\n",
      "\n",
      "Epoch 11/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard MLP  - Train Loss: 0.0296, Train Acc: 99.07%, Val Loss: 0.1035, Val Acc: 97.68%\n",
      "Residual MLP  - Train Loss: 0.0201, Train Acc: 99.29%, Val Loss: 0.0834, Val Acc: 98.02%\n",
      "\n",
      "Epoch 12/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard MLP  - Train Loss: 0.0256, Train Acc: 99.14%, Val Loss: 0.1048, Val Acc: 97.62%\n",
      "Residual MLP  - Train Loss: 0.0205, Train Acc: 99.27%, Val Loss: 0.1150, Val Acc: 97.58%\n",
      "\n",
      "Epoch 13/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard MLP  - Train Loss: 0.0239, Train Acc: 99.19%, Val Loss: 0.1218, Val Acc: 97.66%\n",
      "Residual MLP  - Train Loss: 0.0219, Train Acc: 99.26%, Val Loss: 0.1105, Val Acc: 97.60%\n",
      "\n",
      "Epoch 14/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard MLP  - Train Loss: 0.0214, Train Acc: 99.30%, Val Loss: 0.1183, Val Acc: 97.62%\n",
      "Residual MLP  - Train Loss: 0.0171, Train Acc: 99.49%, Val Loss: 0.1034, Val Acc: 97.92%\n",
      "\n",
      "Epoch 15/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard MLP  - Train Loss: 0.0223, Train Acc: 99.28%, Val Loss: 0.1009, Val Acc: 97.90%\n",
      "Residual MLP  - Train Loss: 0.0138, Train Acc: 99.52%, Val Loss: 0.1392, Val Acc: 97.34%\n",
      "\n",
      "----------------------------------------\n",
      "GRADIENT ANALYSIS\n",
      "----------------------------------------\n",
      "\n",
      "Gradient Norms Comparison (Depth 1):\n",
      "Layer                          Standard MLP    Residual MLP    Ratio     \n",
      "----------------------------------------------------------------------\n",
      "input_layer.weight             0.006229        0.696967        111.90    \n",
      "blocks.0.fc1.weight            0.005742        0.561574        97.80     \n",
      "blocks.0.fc2.weight            0.003113        0.242486        77.90     \n",
      "output_layer.weight            0.005161        0.781906        151.51    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Test Accuracy - Depth 1:\n",
      "Standard MLP: 97.81%\n",
      "Residual MLP: 97.62%\n",
      "Improvement: -0.19%\n",
      "\n",
      "Creating models with depth 3...\n",
      "Standard MLP parameters: 200,842\n",
      "Residual MLP parameters: 200,842\n",
      "\n",
      "============================================================\n",
      "Training Networks with Depth: 3\n",
      "============================================================\n",
      "\n",
      "Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard MLP  - Train Loss: 0.4357, Train Acc: 85.95%, Val Loss: 0.1800, Val Acc: 95.02%\n",
      "Residual MLP  - Train Loss: 0.2411, Train Acc: 92.68%, Val Loss: 0.1496, Val Acc: 95.36%\n",
      "\n",
      "Epoch 2/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard MLP  - Train Loss: 0.1640, Train Acc: 95.46%, Val Loss: 0.1487, Val Acc: 95.92%\n",
      "Residual MLP  - Train Loss: 0.1090, Train Acc: 96.68%, Val Loss: 0.1343, Val Acc: 96.48%\n",
      "\n",
      "Epoch 3/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard MLP  - Train Loss: 0.1217, Train Acc: 96.60%, Val Loss: 0.1397, Val Acc: 96.20%\n",
      "Residual MLP  - Train Loss: 0.0786, Train Acc: 97.58%, Val Loss: 0.0850, Val Acc: 97.52%\n",
      "\n",
      "Epoch 4/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard MLP  - Train Loss: 0.0989, Train Acc: 97.31%, Val Loss: 0.1171, Val Acc: 96.70%\n",
      "Residual MLP  - Train Loss: 0.0635, Train Acc: 97.97%, Val Loss: 0.1175, Val Acc: 96.54%\n",
      "\n",
      "Epoch 5/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard MLP  - Train Loss: 0.0861, Train Acc: 97.61%, Val Loss: 0.1127, Val Acc: 97.18%\n",
      "Residual MLP  - Train Loss: 0.0522, Train Acc: 98.29%, Val Loss: 0.1026, Val Acc: 97.22%\n",
      "\n",
      "Epoch 6/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard MLP  - Train Loss: 0.0783, Train Acc: 97.77%, Val Loss: 0.1124, Val Acc: 97.38%\n",
      "Residual MLP  - Train Loss: 0.0428, Train Acc: 98.60%, Val Loss: 0.1056, Val Acc: 97.14%\n",
      "\n",
      "Epoch 7/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard MLP  - Train Loss: 0.0674, Train Acc: 98.21%, Val Loss: 0.1295, Val Acc: 96.86%\n",
      "Residual MLP  - Train Loss: 0.0379, Train Acc: 98.73%, Val Loss: 0.0909, Val Acc: 97.82%\n",
      "\n",
      "Epoch 8/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard MLP  - Train Loss: 0.0593, Train Acc: 98.36%, Val Loss: 0.1332, Val Acc: 96.86%\n",
      "Residual MLP  - Train Loss: 0.0304, Train Acc: 98.98%, Val Loss: 0.1060, Val Acc: 97.74%\n",
      "\n",
      "Epoch 9/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard MLP  - Train Loss: 0.0574, Train Acc: 98.52%, Val Loss: 0.1152, Val Acc: 97.44%\n",
      "Residual MLP  - Train Loss: 0.0289, Train Acc: 99.06%, Val Loss: 0.1146, Val Acc: 97.26%\n",
      "\n",
      "Epoch 10/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard MLP  - Train Loss: 0.0500, Train Acc: 98.62%, Val Loss: 0.1047, Val Acc: 97.60%\n",
      "Residual MLP  - Train Loss: 0.0241, Train Acc: 99.23%, Val Loss: 0.1089, Val Acc: 97.40%\n",
      "\n",
      "Epoch 11/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard MLP  - Train Loss: 0.0466, Train Acc: 98.73%, Val Loss: 0.1256, Val Acc: 97.24%\n",
      "Residual MLP  - Train Loss: 0.0214, Train Acc: 99.32%, Val Loss: 0.0991, Val Acc: 97.78%\n",
      "\n",
      "Epoch 12/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard MLP  - Train Loss: 0.0447, Train Acc: 98.79%, Val Loss: 0.1072, Val Acc: 97.50%\n",
      "Residual MLP  - Train Loss: 0.0258, Train Acc: 99.19%, Val Loss: 0.1087, Val Acc: 97.60%\n",
      "\n",
      "Epoch 13/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard MLP  - Train Loss: 0.0374, Train Acc: 98.95%, Val Loss: 0.1157, Val Acc: 97.54%\n",
      "Residual MLP  - Train Loss: 0.0199, Train Acc: 99.33%, Val Loss: 0.1047, Val Acc: 97.80%\n",
      "\n",
      "Epoch 14/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard MLP  - Train Loss: 0.0355, Train Acc: 99.06%, Val Loss: 0.1199, Val Acc: 97.66%\n",
      "Residual MLP  - Train Loss: 0.0164, Train Acc: 99.46%, Val Loss: 0.0976, Val Acc: 97.76%\n",
      "\n",
      "Epoch 15/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard MLP  - Train Loss: 0.0376, Train Acc: 98.98%, Val Loss: 0.1094, Val Acc: 97.78%\n",
      "Residual MLP  - Train Loss: 0.0190, Train Acc: 99.38%, Val Loss: 0.0973, Val Acc: 97.82%\n",
      "\n",
      "----------------------------------------\n",
      "GRADIENT ANALYSIS\n",
      "----------------------------------------\n",
      "\n",
      "Gradient Norms Comparison (Depth 3):\n",
      "Layer                          Standard MLP    Residual MLP    Ratio     \n",
      "----------------------------------------------------------------------\n",
      "input_layer.weight             0.271185        0.001908        0.01      \n",
      "blocks.0.fc1.weight            0.323914        0.000958        0.00      \n",
      "blocks.0.fc2.weight            0.175747        0.000545        0.00      \n",
      "blocks.1.fc1.weight            0.092680        0.000816        0.01      \n",
      "blocks.1.fc2.weight            0.053904        0.000476        0.01      \n",
      "blocks.2.fc1.weight            0.047610        0.000932        0.02      \n",
      "blocks.2.fc2.weight            0.044799        0.000611        0.01      \n",
      "output_layer.weight            0.146100        0.002954        0.02      \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Test Accuracy - Depth 3:\n",
      "Standard MLP: 97.74%\n",
      "Residual MLP: 97.79%\n",
      "Improvement: 0.05%\n",
      "\n",
      "Creating models with depth 5...\n",
      "Standard MLP parameters: 266,890\n",
      "Residual MLP parameters: 266,890\n",
      "\n",
      "============================================================\n",
      "Training Networks with Depth: 5\n",
      "============================================================\n",
      "\n",
      "Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard MLP  - Train Loss: 0.6944, Train Acc: 74.61%, Val Loss: 0.2802, Val Acc: 93.08%\n",
      "Residual MLP  - Train Loss: 0.2344, Train Acc: 92.78%, Val Loss: 0.1300, Val Acc: 96.00%\n",
      "\n",
      "Epoch 2/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard MLP  - Train Loss: 0.2426, Train Acc: 93.94%, Val Loss: 0.2227, Val Acc: 94.72%\n",
      "Residual MLP  - Train Loss: 0.1085, Train Acc: 96.67%, Val Loss: 0.0972, Val Acc: 97.16%\n",
      "\n",
      "Epoch 3/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard MLP  - Train Loss: 0.1950, Train Acc: 95.26%, Val Loss: 0.1586, Val Acc: 96.24%\n",
      "Residual MLP  - Train Loss: 0.0763, Train Acc: 97.57%, Val Loss: 0.0955, Val Acc: 97.22%\n",
      "\n",
      "Epoch 4/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard MLP  - Train Loss: 0.1644, Train Acc: 95.99%, Val Loss: 0.1506, Val Acc: 96.40%\n",
      "Residual MLP  - Train Loss: 0.0611, Train Acc: 98.05%, Val Loss: 0.0947, Val Acc: 97.34%\n",
      "\n",
      "Epoch 5/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard MLP  - Train Loss: 0.1507, Train Acc: 96.43%, Val Loss: 0.1742, Val Acc: 96.22%\n",
      "Residual MLP  - Train Loss: 0.0500, Train Acc: 98.42%, Val Loss: 0.0962, Val Acc: 97.56%\n",
      "\n",
      "Epoch 6/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard MLP  - Train Loss: 0.1337, Train Acc: 96.80%, Val Loss: 0.1411, Val Acc: 96.94%\n",
      "Residual MLP  - Train Loss: 0.0451, Train Acc: 98.61%, Val Loss: 0.0834, Val Acc: 97.78%\n",
      "\n",
      "Epoch 7/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard MLP  - Train Loss: 0.1173, Train Acc: 97.20%, Val Loss: 0.1416, Val Acc: 96.94%\n",
      "Residual MLP  - Train Loss: 0.0364, Train Acc: 98.81%, Val Loss: 0.0867, Val Acc: 97.80%\n",
      "\n",
      "Epoch 8/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard MLP  - Train Loss: 0.1126, Train Acc: 97.42%, Val Loss: 0.1325, Val Acc: 97.22%\n",
      "Residual MLP  - Train Loss: 0.0327, Train Acc: 98.95%, Val Loss: 0.0983, Val Acc: 97.46%\n",
      "\n",
      "Epoch 9/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard MLP  - Train Loss: 0.1029, Train Acc: 97.60%, Val Loss: 0.1587, Val Acc: 96.88%\n",
      "Residual MLP  - Train Loss: 0.0300, Train Acc: 99.01%, Val Loss: 0.0987, Val Acc: 97.60%\n",
      "\n",
      "Epoch 10/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard MLP  - Train Loss: 0.1020, Train Acc: 97.60%, Val Loss: 0.1262, Val Acc: 97.10%\n",
      "Residual MLP  - Train Loss: 0.0266, Train Acc: 99.10%, Val Loss: 0.0899, Val Acc: 97.84%\n",
      "\n",
      "Epoch 11/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard MLP  - Train Loss: 0.0926, Train Acc: 97.69%, Val Loss: 0.1474, Val Acc: 97.36%\n",
      "Residual MLP  - Train Loss: 0.0235, Train Acc: 99.25%, Val Loss: 0.1007, Val Acc: 97.88%\n",
      "\n",
      "Epoch 12/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard MLP  - Train Loss: 0.0828, Train Acc: 98.14%, Val Loss: 0.1272, Val Acc: 97.44%\n",
      "Residual MLP  - Train Loss: 0.0222, Train Acc: 99.26%, Val Loss: 0.1173, Val Acc: 97.58%\n",
      "\n",
      "Epoch 13/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard MLP  - Train Loss: 0.0854, Train Acc: 97.96%, Val Loss: 0.1293, Val Acc: 97.30%\n",
      "Residual MLP  - Train Loss: 0.0212, Train Acc: 99.34%, Val Loss: 0.1141, Val Acc: 97.70%\n",
      "\n",
      "Epoch 14/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard MLP  - Train Loss: 0.0757, Train Acc: 98.15%, Val Loss: 0.1305, Val Acc: 97.48%\n",
      "Residual MLP  - Train Loss: 0.0182, Train Acc: 99.45%, Val Loss: 0.0993, Val Acc: 97.90%\n",
      "\n",
      "Epoch 15/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard MLP  - Train Loss: 0.0725, Train Acc: 98.28%, Val Loss: 0.1410, Val Acc: 97.26%\n",
      "Residual MLP  - Train Loss: 0.0188, Train Acc: 99.39%, Val Loss: 0.1071, Val Acc: 97.64%\n",
      "\n",
      "----------------------------------------\n",
      "GRADIENT ANALYSIS\n",
      "----------------------------------------\n",
      "\n",
      "Gradient Norms Comparison (Depth 5):\n",
      "Layer                          Standard MLP    Residual MLP    Ratio     \n",
      "----------------------------------------------------------------------\n",
      "input_layer.weight             0.070593        0.010738        0.15      \n",
      "blocks.0.fc1.weight            0.058370        0.005103        0.09      \n",
      "blocks.0.fc2.weight            0.025526        0.003532        0.14      \n",
      "blocks.1.fc1.weight            0.011315        0.004429        0.39      \n",
      "blocks.1.fc2.weight            0.010423        0.002816        0.27      \n",
      "blocks.2.fc1.weight            0.011475        0.006636        0.58      \n",
      "blocks.2.fc2.weight            0.015336        0.005074        0.33      \n",
      "blocks.3.fc1.weight            0.016679        0.002148        0.13      \n",
      "blocks.3.fc2.weight            0.016911        0.000568        0.03      \n",
      "blocks.4.fc1.weight            0.018583        0.004306        0.23      \n",
      "blocks.4.fc2.weight            0.017644        0.001436        0.08      \n",
      "output_layer.weight            0.042029        0.013483        0.32      \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Test Accuracy - Depth 5:\n",
      "Standard MLP: 97.23%\n",
      "Residual MLP: 97.86%\n",
      "Improvement: 0.63%\n",
      "\n",
      "Creating models with depth 8...\n",
      "Standard MLP parameters: 365,962\n",
      "Residual MLP parameters: 365,962\n",
      "\n",
      "============================================================\n",
      "Training Networks with Depth: 8\n",
      "============================================================\n",
      "\n",
      "Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard MLP  - Train Loss: 1.3304, Train Acc: 46.73%, Val Loss: 0.7041, Val Acc: 80.42%\n",
      "Residual MLP  - Train Loss: 0.2377, Train Acc: 92.66%, Val Loss: 0.1280, Val Acc: 96.12%\n",
      "\n",
      "Epoch 2/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard MLP  - Train Loss: 0.7650, Train Acc: 76.78%, Val Loss: 0.7653, Val Acc: 79.68%\n",
      "Residual MLP  - Train Loss: 0.1131, Train Acc: 96.45%, Val Loss: 0.0966, Val Acc: 97.20%\n",
      "\n",
      "Epoch 3/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard MLP  - Train Loss: 0.7337, Train Acc: 77.68%, Val Loss: 0.7654, Val Acc: 77.18%\n",
      "Residual MLP  - Train Loss: 0.0801, Train Acc: 97.50%, Val Loss: 0.0963, Val Acc: 97.04%\n",
      "\n",
      "Epoch 4/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard MLP  - Train Loss: 0.9762, Train Acc: 66.08%, Val Loss: 0.9629, Val Acc: 61.50%\n",
      "Residual MLP  - Train Loss: 0.0667, Train Acc: 97.89%, Val Loss: 0.0985, Val Acc: 97.14%\n",
      "\n",
      "Epoch 5/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard MLP  - Train Loss: 0.7797, Train Acc: 74.90%, Val Loss: 0.6499, Val Acc: 80.74%\n",
      "Residual MLP  - Train Loss: 0.0543, Train Acc: 98.25%, Val Loss: 0.0889, Val Acc: 97.18%\n",
      "\n",
      "Epoch 6/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard MLP  - Train Loss: 0.6266, Train Acc: 82.14%, Val Loss: 0.6459, Val Acc: 83.20%\n",
      "Residual MLP  - Train Loss: 0.0442, Train Acc: 98.64%, Val Loss: 0.0888, Val Acc: 97.38%\n",
      "\n",
      "Epoch 7/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard MLP  - Train Loss: 0.6386, Train Acc: 82.23%, Val Loss: 0.8605, Val Acc: 75.86%\n",
      "Residual MLP  - Train Loss: 0.0394, Train Acc: 98.73%, Val Loss: 0.1140, Val Acc: 97.30%\n",
      "\n",
      "Epoch 8/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard MLP  - Train Loss: 0.6160, Train Acc: 83.58%, Val Loss: 0.4649, Val Acc: 88.02%\n",
      "Residual MLP  - Train Loss: 0.0343, Train Acc: 98.89%, Val Loss: 0.1337, Val Acc: 96.92%\n",
      "\n",
      "Epoch 9/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard MLP  - Train Loss: 0.7533, Train Acc: 77.38%, Val Loss: 1.3916, Val Acc: 48.30%\n",
      "Residual MLP  - Train Loss: 0.0345, Train Acc: 98.88%, Val Loss: 0.1158, Val Acc: 97.16%\n",
      "\n",
      "Epoch 10/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard MLP  - Train Loss: 0.7709, Train Acc: 76.28%, Val Loss: 0.8813, Val Acc: 73.48%\n",
      "Residual MLP  - Train Loss: 0.0289, Train Acc: 99.10%, Val Loss: 0.0922, Val Acc: 97.80%\n",
      "\n",
      "Epoch 11/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard MLP  - Train Loss: 0.9345, Train Acc: 69.73%, Val Loss: 0.7630, Val Acc: 76.22%\n",
      "Residual MLP  - Train Loss: 0.0265, Train Acc: 99.18%, Val Loss: 0.0951, Val Acc: 97.74%\n",
      "\n",
      "Epoch 12/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard MLP  - Train Loss: 0.9927, Train Acc: 65.35%, Val Loss: 0.8255, Val Acc: 76.60%\n",
      "Residual MLP  - Train Loss: 0.0237, Train Acc: 99.25%, Val Loss: 0.0986, Val Acc: 97.98%\n",
      "\n",
      "Epoch 13/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard MLP  - Train Loss: 0.7940, Train Acc: 76.48%, Val Loss: 0.5815, Val Acc: 86.06%\n",
      "Residual MLP  - Train Loss: 0.0224, Train Acc: 99.29%, Val Loss: 0.1013, Val Acc: 97.80%\n",
      "\n",
      "Epoch 14/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard MLP  - Train Loss: 0.7921, Train Acc: 76.49%, Val Loss: 0.7436, Val Acc: 76.46%\n",
      "Residual MLP  - Train Loss: 0.0232, Train Acc: 99.23%, Val Loss: 0.0939, Val Acc: 97.50%\n",
      "\n",
      "Epoch 15/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard MLP  - Train Loss: 0.8114, Train Acc: 73.11%, Val Loss: 0.6618, Val Acc: 83.40%\n",
      "Residual MLP  - Train Loss: 0.0174, Train Acc: 99.46%, Val Loss: 0.0923, Val Acc: 98.14%\n",
      "\n",
      "----------------------------------------\n",
      "GRADIENT ANALYSIS\n",
      "----------------------------------------\n",
      "\n",
      "Gradient Norms Comparison (Depth 8):\n",
      "Layer                          Standard MLP    Residual MLP    Ratio     \n",
      "----------------------------------------------------------------------\n",
      "input_layer.weight             0.261273        0.043286        0.17      \n",
      "blocks.0.fc1.weight            0.331847        0.027608        0.08      \n",
      "blocks.0.fc2.weight            0.167810        0.018926        0.11      \n",
      "blocks.1.fc1.weight            0.156891        0.022001        0.14      \n",
      "blocks.1.fc2.weight            0.264559        0.016629        0.06      \n",
      "blocks.2.fc1.weight            0.331135        0.023862        0.07      \n",
      "blocks.2.fc2.weight            0.373550        0.008972        0.02      \n",
      "blocks.3.fc1.weight            0.258659        0.016644        0.06      \n",
      "blocks.3.fc2.weight            0.236129        0.007894        0.03      \n",
      "blocks.4.fc1.weight            0.296816        0.018301        0.06      \n",
      "blocks.4.fc2.weight            0.572732        0.009520        0.02      \n",
      "blocks.5.fc1.weight            0.643610        0.011406        0.02      \n",
      "blocks.5.fc2.weight            0.382151        0.006643        0.02      \n",
      "blocks.6.fc1.weight            0.646368        0.012907        0.02      \n",
      "blocks.6.fc2.weight            1.090461        0.005480        0.01      \n",
      "blocks.7.fc1.weight            4.602203        0.014388        0.00      \n",
      "blocks.7.fc2.weight            3.074557        0.006683        0.00      \n",
      "output_layer.weight            1.621250        0.041485        0.03      \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Test Accuracy - Depth 8:\n",
      "Standard MLP: 84.77%\n",
      "Residual MLP: 98.07%\n",
      "Improvement: 13.30%\n",
      "\n",
      "============================================================\n",
      "EXPERIMENT SUMMARY\n",
      "============================================================\n",
      "Depth    Standard Acc    Residual Acc    Improvement \n",
      "------------------------------------------------------------\n",
      "1        97.90           97.34           -0.56       \n",
      "3        97.78           97.82           0.04        \n",
      "5        97.26           97.64           0.38        \n",
      "8        83.40           98.14           14.74       \n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>depth_1/residual_train_acc</td><td>▁▅▆▇▇▇▇▇███████</td></tr><tr><td>depth_1/residual_train_loss</td><td>█▄▃▂▂▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>depth_1/residual_val_acc</td><td>▁▄▅▆▇▇█▇▆▆█▇▇█▆</td></tr><tr><td>depth_1/residual_val_loss</td><td>█▄▂▃▂▂▁▂▄▃▂▅▅▄█</td></tr><tr><td>depth_1/standard_train_acc</td><td>▁▅▆▇▇▇▇████████</td></tr><tr><td>depth_1/standard_train_loss</td><td>█▄▃▂▂▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>depth_1/standard_val_acc</td><td>▁▆▅▅▇▇▆▄▇▇▇▇▇▇█</td></tr><tr><td>depth_1/standard_val_loss</td><td>█▃▂▃▁▂▄▇▄▅▄▅▇▇▄</td></tr><tr><td>depth_3/residual_train_acc</td><td>▁▅▆▆▇▇▇████████</td></tr><tr><td>depth_3/residual_train_loss</td><td>█▄▃▂▂▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>depth_3/residual_val_acc</td><td>▁▄▇▄▆▆██▆▇█▇███</td></tr><tr><td>depth_3/residual_val_loss</td><td>█▆▁▅▃▃▂▃▄▄▃▄▃▂▂</td></tr><tr><td>depth_3/standard_train_acc</td><td>▁▆▇▇▇▇█████████</td></tr><tr><td>depth_3/standard_train_loss</td><td>█▃▃▂▂▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>depth_3/standard_val_acc</td><td>▁▃▄▅▆▇▆▆▇█▇▇▇██</td></tr><tr><td>depth_3/standard_val_loss</td><td>█▅▄▂▂▂▃▄▂▁▃▁▂▂▁</td></tr><tr><td>depth_5/residual_train_acc</td><td>▁▅▆▇▇▇▇▇███████</td></tr><tr><td>depth_5/residual_train_loss</td><td>█▄▃▂▂▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>depth_5/residual_val_acc</td><td>▁▅▅▆▇██▆▇██▇▇█▇</td></tr><tr><td>depth_5/residual_val_loss</td><td>█▃▃▃▃▁▁▃▃▂▄▆▆▃▅</td></tr><tr><td>depth_5/standard_train_acc</td><td>▁▇▇▇▇██████████</td></tr><tr><td>depth_5/standard_train_loss</td><td>█▃▂▂▂▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>depth_5/standard_val_acc</td><td>▁▄▆▆▆▇▇█▇▇█████</td></tr><tr><td>depth_5/standard_val_loss</td><td>█▅▂▂▃▂▂▁▂▁▂▁▁▁▂</td></tr><tr><td>depth_8/residual_train_acc</td><td>▁▅▆▆▇▇▇▇▇██████</td></tr><tr><td>depth_8/residual_train_loss</td><td>█▄▃▃▂▂▂▂▂▁▁▁▁▁▁</td></tr><tr><td>depth_8/residual_val_acc</td><td>▁▅▄▅▅▅▅▄▅▇▇▇▇▆█</td></tr><tr><td>depth_8/residual_val_loss</td><td>▇▂▂▃▁▁▅█▅▂▂▃▃▂▂</td></tr><tr><td>depth_8/standard_train_acc</td><td>▁▇▇▅▆███▇▇▅▅▇▇▆</td></tr><tr><td>depth_8/standard_train_loss</td><td>█▂▂▅▃▁▁▁▂▃▄▅▃▃▃</td></tr><tr><td>depth_8/standard_val_acc</td><td>▇▇▆▃▇▇▆█▁▅▆▆█▆▇</td></tr><tr><td>depth_8/standard_val_loss</td><td>▃▃▃▅▂▂▄▁█▄▃▄▂▃▂</td></tr><tr><td>epoch</td><td>▁▁▂▃▃▅▅▅▆▇▇█▁▁▂▃▃▄▅▅▇▇█▁▁▄▅▅▇▇▁▁▂▃▃▄▅▅▆▇</td></tr><tr><td>final_test/depth_1_improvement</td><td>▁</td></tr><tr><td>final_test/depth_1_residual</td><td>▁</td></tr><tr><td>final_test/depth_1_standard</td><td>▁</td></tr><tr><td>final_test/depth_3_improvement</td><td>▁</td></tr><tr><td>final_test/depth_3_residual</td><td>▁</td></tr><tr><td>final_test/depth_3_standard</td><td>▁</td></tr><tr><td>final_test/depth_5_improvement</td><td>▁</td></tr><tr><td>final_test/depth_5_residual</td><td>▁</td></tr><tr><td>final_test/depth_5_standard</td><td>▁</td></tr><tr><td>final_test/depth_8_improvement</td><td>▁</td></tr><tr><td>final_test/depth_8_residual</td><td>▁</td></tr><tr><td>final_test/depth_8_standard</td><td>▁</td></tr><tr><td>gradients/depth_1/residual_blocks.0.fc1.weight</td><td>▁</td></tr><tr><td>gradients/depth_1/residual_blocks.0.fc2.weight</td><td>▁</td></tr><tr><td>gradients/depth_1/residual_input_layer.weight</td><td>▁</td></tr><tr><td>gradients/depth_1/residual_output_layer.weight</td><td>▁</td></tr><tr><td>gradients/depth_1/standard_blocks.0.fc1.weight</td><td>▁</td></tr><tr><td>gradients/depth_1/standard_blocks.0.fc2.weight</td><td>▁</td></tr><tr><td>gradients/depth_1/standard_input_layer.weight</td><td>▁</td></tr><tr><td>gradients/depth_1/standard_output_layer.weight</td><td>▁</td></tr><tr><td>gradients/depth_3/residual_blocks.0.fc1.weight</td><td>▁</td></tr><tr><td>gradients/depth_3/residual_blocks.0.fc2.weight</td><td>▁</td></tr><tr><td>gradients/depth_3/residual_blocks.1.fc1.weight</td><td>▁</td></tr><tr><td>gradients/depth_3/residual_blocks.1.fc2.weight</td><td>▁</td></tr><tr><td>gradients/depth_3/residual_blocks.2.fc1.weight</td><td>▁</td></tr><tr><td>gradients/depth_3/residual_blocks.2.fc2.weight</td><td>▁</td></tr><tr><td>gradients/depth_3/residual_input_layer.weight</td><td>▁</td></tr><tr><td>gradients/depth_3/residual_output_layer.weight</td><td>▁</td></tr><tr><td>gradients/depth_3/standard_blocks.0.fc1.weight</td><td>▁</td></tr><tr><td>gradients/depth_3/standard_blocks.0.fc2.weight</td><td>▁</td></tr><tr><td>gradients/depth_3/standard_blocks.1.fc1.weight</td><td>▁</td></tr><tr><td>gradients/depth_3/standard_blocks.1.fc2.weight</td><td>▁</td></tr><tr><td>gradients/depth_3/standard_blocks.2.fc1.weight</td><td>▁</td></tr><tr><td>gradients/depth_3/standard_blocks.2.fc2.weight</td><td>▁</td></tr><tr><td>gradients/depth_3/standard_input_layer.weight</td><td>▁</td></tr><tr><td>gradients/depth_3/standard_output_layer.weight</td><td>▁</td></tr><tr><td>gradients/depth_5/residual_blocks.0.fc1.weight</td><td>▁</td></tr><tr><td>gradients/depth_5/residual_blocks.0.fc2.weight</td><td>▁</td></tr><tr><td>gradients/depth_5/residual_blocks.1.fc1.weight</td><td>▁</td></tr><tr><td>gradients/depth_5/residual_blocks.1.fc2.weight</td><td>▁</td></tr><tr><td>gradients/depth_5/residual_blocks.2.fc1.weight</td><td>▁</td></tr><tr><td>gradients/depth_5/residual_blocks.2.fc2.weight</td><td>▁</td></tr><tr><td>gradients/depth_5/residual_blocks.3.fc1.weight</td><td>▁</td></tr><tr><td>gradients/depth_5/residual_blocks.3.fc2.weight</td><td>▁</td></tr><tr><td>gradients/depth_5/residual_blocks.4.fc1.weight</td><td>▁</td></tr><tr><td>gradients/depth_5/residual_blocks.4.fc2.weight</td><td>▁</td></tr><tr><td>gradients/depth_5/residual_input_layer.weight</td><td>▁</td></tr><tr><td>gradients/depth_5/residual_output_layer.weight</td><td>▁</td></tr><tr><td>gradients/depth_5/standard_blocks.0.fc1.weight</td><td>▁</td></tr><tr><td>gradients/depth_5/standard_blocks.0.fc2.weight</td><td>▁</td></tr><tr><td>gradients/depth_5/standard_blocks.1.fc1.weight</td><td>▁</td></tr><tr><td>gradients/depth_5/standard_blocks.1.fc2.weight</td><td>▁</td></tr><tr><td>gradients/depth_5/standard_blocks.2.fc1.weight</td><td>▁</td></tr><tr><td>gradients/depth_5/standard_blocks.2.fc2.weight</td><td>▁</td></tr><tr><td>gradients/depth_5/standard_blocks.3.fc1.weight</td><td>▁</td></tr><tr><td>gradients/depth_5/standard_blocks.3.fc2.weight</td><td>▁</td></tr><tr><td>gradients/depth_5/standard_blocks.4.fc1.weight</td><td>▁</td></tr><tr><td>gradients/depth_5/standard_blocks.4.fc2.weight</td><td>▁</td></tr><tr><td>gradients/depth_5/standard_input_layer.weight</td><td>▁</td></tr><tr><td>gradients/depth_5/standard_output_layer.weight</td><td>▁</td></tr><tr><td>gradients/depth_8/residual_blocks.0.fc1.weight</td><td>▁</td></tr><tr><td>gradients/depth_8/residual_blocks.0.fc2.weight</td><td>▁</td></tr><tr><td>gradients/depth_8/residual_blocks.1.fc1.weight</td><td>▁</td></tr><tr><td>gradients/depth_8/residual_blocks.1.fc2.weight</td><td>▁</td></tr><tr><td>gradients/depth_8/residual_blocks.2.fc1.weight</td><td>▁</td></tr><tr><td>gradients/depth_8/residual_blocks.2.fc2.weight</td><td>▁</td></tr><tr><td>gradients/depth_8/residual_blocks.3.fc1.weight</td><td>▁</td></tr><tr><td>gradients/depth_8/residual_blocks.3.fc2.weight</td><td>▁</td></tr><tr><td>gradients/depth_8/residual_blocks.4.fc1.weight</td><td>▁</td></tr><tr><td>gradients/depth_8/residual_blocks.4.fc2.weight</td><td>▁</td></tr><tr><td>gradients/depth_8/residual_blocks.5.fc1.weight</td><td>▁</td></tr><tr><td>gradients/depth_8/residual_blocks.5.fc2.weight</td><td>▁</td></tr><tr><td>gradients/depth_8/residual_blocks.6.fc1.weight</td><td>▁</td></tr><tr><td>gradients/depth_8/residual_blocks.6.fc2.weight</td><td>▁</td></tr><tr><td>gradients/depth_8/residual_blocks.7.fc1.weight</td><td>▁</td></tr><tr><td>gradients/depth_8/residual_blocks.7.fc2.weight</td><td>▁</td></tr><tr><td>gradients/depth_8/residual_input_layer.weight</td><td>▁</td></tr><tr><td>gradients/depth_8/residual_output_layer.weight</td><td>▁</td></tr><tr><td>gradients/depth_8/standard_blocks.0.fc1.weight</td><td>▁</td></tr><tr><td>gradients/depth_8/standard_blocks.0.fc2.weight</td><td>▁</td></tr><tr><td>gradients/depth_8/standard_blocks.1.fc1.weight</td><td>▁</td></tr><tr><td>gradients/depth_8/standard_blocks.1.fc2.weight</td><td>▁</td></tr><tr><td>gradients/depth_8/standard_blocks.2.fc1.weight</td><td>▁</td></tr><tr><td>gradients/depth_8/standard_blocks.2.fc2.weight</td><td>▁</td></tr><tr><td>gradients/depth_8/standard_blocks.3.fc1.weight</td><td>▁</td></tr><tr><td>gradients/depth_8/standard_blocks.3.fc2.weight</td><td>▁</td></tr><tr><td>gradients/depth_8/standard_blocks.4.fc1.weight</td><td>▁</td></tr><tr><td>gradients/depth_8/standard_blocks.4.fc2.weight</td><td>▁</td></tr><tr><td>gradients/depth_8/standard_blocks.5.fc1.weight</td><td>▁</td></tr><tr><td>gradients/depth_8/standard_blocks.5.fc2.weight</td><td>▁</td></tr><tr><td>gradients/depth_8/standard_blocks.6.fc1.weight</td><td>▁</td></tr><tr><td>gradients/depth_8/standard_blocks.6.fc2.weight</td><td>▁</td></tr><tr><td>gradients/depth_8/standard_blocks.7.fc1.weight</td><td>▁</td></tr><tr><td>gradients/depth_8/standard_blocks.7.fc2.weight</td><td>▁</td></tr><tr><td>gradients/depth_8/standard_input_layer.weight</td><td>▁</td></tr><tr><td>gradients/depth_8/standard_output_layer.weight</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>depth_1/residual_train_acc</td><td>99.52364</td></tr><tr><td>depth_1/residual_train_loss</td><td>0.01382</td></tr><tr><td>depth_1/residual_val_acc</td><td>97.34</td></tr><tr><td>depth_1/residual_val_loss</td><td>0.13924</td></tr><tr><td>depth_1/standard_train_acc</td><td>99.27636</td></tr><tr><td>depth_1/standard_train_loss</td><td>0.02229</td></tr><tr><td>depth_1/standard_val_acc</td><td>97.9</td></tr><tr><td>depth_1/standard_val_loss</td><td>0.10094</td></tr><tr><td>depth_3/residual_train_acc</td><td>99.37636</td></tr><tr><td>depth_3/residual_train_loss</td><td>0.019</td></tr><tr><td>depth_3/residual_val_acc</td><td>97.82</td></tr><tr><td>depth_3/residual_val_loss</td><td>0.09735</td></tr><tr><td>depth_3/standard_train_acc</td><td>98.98182</td></tr><tr><td>depth_3/standard_train_loss</td><td>0.03759</td></tr><tr><td>depth_3/standard_val_acc</td><td>97.78</td></tr><tr><td>depth_3/standard_val_loss</td><td>0.10939</td></tr><tr><td>depth_5/residual_train_acc</td><td>99.39091</td></tr><tr><td>depth_5/residual_train_loss</td><td>0.0188</td></tr><tr><td>depth_5/residual_val_acc</td><td>97.64</td></tr><tr><td>depth_5/residual_val_loss</td><td>0.10706</td></tr><tr><td>depth_5/standard_train_acc</td><td>98.27818</td></tr><tr><td>depth_5/standard_train_loss</td><td>0.0725</td></tr><tr><td>depth_5/standard_val_acc</td><td>97.26</td></tr><tr><td>depth_5/standard_val_loss</td><td>0.14101</td></tr><tr><td>depth_8/residual_train_acc</td><td>99.46</td></tr><tr><td>depth_8/residual_train_loss</td><td>0.01742</td></tr><tr><td>depth_8/residual_val_acc</td><td>98.14</td></tr><tr><td>depth_8/residual_val_loss</td><td>0.09227</td></tr><tr><td>depth_8/standard_train_acc</td><td>73.10909</td></tr><tr><td>depth_8/standard_train_loss</td><td>0.81137</td></tr><tr><td>depth_8/standard_val_acc</td><td>83.4</td></tr><tr><td>depth_8/standard_val_loss</td><td>0.66181</td></tr><tr><td>epoch</td><td>15</td></tr><tr><td>final_test/depth_1_improvement</td><td>-0.19</td></tr><tr><td>final_test/depth_1_residual</td><td>97.62</td></tr><tr><td>final_test/depth_1_standard</td><td>97.81</td></tr><tr><td>final_test/depth_3_improvement</td><td>0.05</td></tr><tr><td>final_test/depth_3_residual</td><td>97.79</td></tr><tr><td>final_test/depth_3_standard</td><td>97.74</td></tr><tr><td>final_test/depth_5_improvement</td><td>0.63</td></tr><tr><td>final_test/depth_5_residual</td><td>97.86</td></tr><tr><td>final_test/depth_5_standard</td><td>97.23</td></tr><tr><td>final_test/depth_8_improvement</td><td>13.3</td></tr><tr><td>final_test/depth_8_residual</td><td>98.07</td></tr><tr><td>final_test/depth_8_standard</td><td>84.77</td></tr><tr><td>gradients/depth_1/residual_blocks.0.fc1.weight</td><td>0.56157</td></tr><tr><td>gradients/depth_1/residual_blocks.0.fc2.weight</td><td>0.24249</td></tr><tr><td>gradients/depth_1/residual_input_layer.weight</td><td>0.69697</td></tr><tr><td>gradients/depth_1/residual_output_layer.weight</td><td>0.78191</td></tr><tr><td>gradients/depth_1/standard_blocks.0.fc1.weight</td><td>0.00574</td></tr><tr><td>gradients/depth_1/standard_blocks.0.fc2.weight</td><td>0.00311</td></tr><tr><td>gradients/depth_1/standard_input_layer.weight</td><td>0.00623</td></tr><tr><td>gradients/depth_1/standard_output_layer.weight</td><td>0.00516</td></tr><tr><td>gradients/depth_3/residual_blocks.0.fc1.weight</td><td>0.00096</td></tr><tr><td>gradients/depth_3/residual_blocks.0.fc2.weight</td><td>0.00054</td></tr><tr><td>gradients/depth_3/residual_blocks.1.fc1.weight</td><td>0.00082</td></tr><tr><td>gradients/depth_3/residual_blocks.1.fc2.weight</td><td>0.00048</td></tr><tr><td>gradients/depth_3/residual_blocks.2.fc1.weight</td><td>0.00093</td></tr><tr><td>gradients/depth_3/residual_blocks.2.fc2.weight</td><td>0.00061</td></tr><tr><td>gradients/depth_3/residual_input_layer.weight</td><td>0.00191</td></tr><tr><td>gradients/depth_3/residual_output_layer.weight</td><td>0.00295</td></tr><tr><td>gradients/depth_3/standard_blocks.0.fc1.weight</td><td>0.32391</td></tr><tr><td>gradients/depth_3/standard_blocks.0.fc2.weight</td><td>0.17575</td></tr><tr><td>gradients/depth_3/standard_blocks.1.fc1.weight</td><td>0.09268</td></tr><tr><td>gradients/depth_3/standard_blocks.1.fc2.weight</td><td>0.0539</td></tr><tr><td>gradients/depth_3/standard_blocks.2.fc1.weight</td><td>0.04761</td></tr><tr><td>gradients/depth_3/standard_blocks.2.fc2.weight</td><td>0.0448</td></tr><tr><td>gradients/depth_3/standard_input_layer.weight</td><td>0.27118</td></tr><tr><td>gradients/depth_3/standard_output_layer.weight</td><td>0.1461</td></tr><tr><td>gradients/depth_5/residual_blocks.0.fc1.weight</td><td>0.0051</td></tr><tr><td>gradients/depth_5/residual_blocks.0.fc2.weight</td><td>0.00353</td></tr><tr><td>gradients/depth_5/residual_blocks.1.fc1.weight</td><td>0.00443</td></tr><tr><td>gradients/depth_5/residual_blocks.1.fc2.weight</td><td>0.00282</td></tr><tr><td>gradients/depth_5/residual_blocks.2.fc1.weight</td><td>0.00664</td></tr><tr><td>gradients/depth_5/residual_blocks.2.fc2.weight</td><td>0.00507</td></tr><tr><td>gradients/depth_5/residual_blocks.3.fc1.weight</td><td>0.00215</td></tr><tr><td>gradients/depth_5/residual_blocks.3.fc2.weight</td><td>0.00057</td></tr><tr><td>gradients/depth_5/residual_blocks.4.fc1.weight</td><td>0.00431</td></tr><tr><td>gradients/depth_5/residual_blocks.4.fc2.weight</td><td>0.00144</td></tr><tr><td>gradients/depth_5/residual_input_layer.weight</td><td>0.01074</td></tr><tr><td>gradients/depth_5/residual_output_layer.weight</td><td>0.01348</td></tr><tr><td>gradients/depth_5/standard_blocks.0.fc1.weight</td><td>0.05837</td></tr><tr><td>gradients/depth_5/standard_blocks.0.fc2.weight</td><td>0.02553</td></tr><tr><td>gradients/depth_5/standard_blocks.1.fc1.weight</td><td>0.01131</td></tr><tr><td>gradients/depth_5/standard_blocks.1.fc2.weight</td><td>0.01042</td></tr><tr><td>gradients/depth_5/standard_blocks.2.fc1.weight</td><td>0.01147</td></tr><tr><td>gradients/depth_5/standard_blocks.2.fc2.weight</td><td>0.01534</td></tr><tr><td>gradients/depth_5/standard_blocks.3.fc1.weight</td><td>0.01668</td></tr><tr><td>gradients/depth_5/standard_blocks.3.fc2.weight</td><td>0.01691</td></tr><tr><td>gradients/depth_5/standard_blocks.4.fc1.weight</td><td>0.01858</td></tr><tr><td>gradients/depth_5/standard_blocks.4.fc2.weight</td><td>0.01764</td></tr><tr><td>gradients/depth_5/standard_input_layer.weight</td><td>0.07059</td></tr><tr><td>gradients/depth_5/standard_output_layer.weight</td><td>0.04203</td></tr><tr><td>gradients/depth_8/residual_blocks.0.fc1.weight</td><td>0.02761</td></tr><tr><td>gradients/depth_8/residual_blocks.0.fc2.weight</td><td>0.01893</td></tr><tr><td>gradients/depth_8/residual_blocks.1.fc1.weight</td><td>0.022</td></tr><tr><td>gradients/depth_8/residual_blocks.1.fc2.weight</td><td>0.01663</td></tr><tr><td>gradients/depth_8/residual_blocks.2.fc1.weight</td><td>0.02386</td></tr><tr><td>gradients/depth_8/residual_blocks.2.fc2.weight</td><td>0.00897</td></tr><tr><td>gradients/depth_8/residual_blocks.3.fc1.weight</td><td>0.01664</td></tr><tr><td>gradients/depth_8/residual_blocks.3.fc2.weight</td><td>0.00789</td></tr><tr><td>gradients/depth_8/residual_blocks.4.fc1.weight</td><td>0.0183</td></tr><tr><td>gradients/depth_8/residual_blocks.4.fc2.weight</td><td>0.00952</td></tr><tr><td>gradients/depth_8/residual_blocks.5.fc1.weight</td><td>0.01141</td></tr><tr><td>gradients/depth_8/residual_blocks.5.fc2.weight</td><td>0.00664</td></tr><tr><td>gradients/depth_8/residual_blocks.6.fc1.weight</td><td>0.01291</td></tr><tr><td>gradients/depth_8/residual_blocks.6.fc2.weight</td><td>0.00548</td></tr><tr><td>gradients/depth_8/residual_blocks.7.fc1.weight</td><td>0.01439</td></tr><tr><td>gradients/depth_8/residual_blocks.7.fc2.weight</td><td>0.00668</td></tr><tr><td>gradients/depth_8/residual_input_layer.weight</td><td>0.04329</td></tr><tr><td>gradients/depth_8/residual_output_layer.weight</td><td>0.04149</td></tr><tr><td>gradients/depth_8/standard_blocks.0.fc1.weight</td><td>0.33185</td></tr><tr><td>gradients/depth_8/standard_blocks.0.fc2.weight</td><td>0.16781</td></tr><tr><td>gradients/depth_8/standard_blocks.1.fc1.weight</td><td>0.15689</td></tr><tr><td>gradients/depth_8/standard_blocks.1.fc2.weight</td><td>0.26456</td></tr><tr><td>gradients/depth_8/standard_blocks.2.fc1.weight</td><td>0.33114</td></tr><tr><td>gradients/depth_8/standard_blocks.2.fc2.weight</td><td>0.37355</td></tr><tr><td>gradients/depth_8/standard_blocks.3.fc1.weight</td><td>0.25866</td></tr><tr><td>gradients/depth_8/standard_blocks.3.fc2.weight</td><td>0.23613</td></tr><tr><td>gradients/depth_8/standard_blocks.4.fc1.weight</td><td>0.29682</td></tr><tr><td>gradients/depth_8/standard_blocks.4.fc2.weight</td><td>0.57273</td></tr><tr><td>gradients/depth_8/standard_blocks.5.fc1.weight</td><td>0.64361</td></tr><tr><td>gradients/depth_8/standard_blocks.5.fc2.weight</td><td>0.38215</td></tr><tr><td>gradients/depth_8/standard_blocks.6.fc1.weight</td><td>0.64637</td></tr><tr><td>gradients/depth_8/standard_blocks.6.fc2.weight</td><td>1.09046</td></tr><tr><td>gradients/depth_8/standard_blocks.7.fc1.weight</td><td>4.6022</td></tr><tr><td>gradients/depth_8/standard_blocks.7.fc2.weight</td><td>3.07456</td></tr><tr><td>gradients/depth_8/standard_input_layer.weight</td><td>0.26127</td></tr><tr><td>gradients/depth_8/standard_output_layer.weight</td><td>1.62125</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">sandy-cloud-1</strong> at: <a href='https://wandb.ai/leonardobiondi-universit-degli-studi-di-firenze/mnist-residual-mlp-comparison/runs/wz1tujh1' target=\"_blank\">https://wandb.ai/leonardobiondi-universit-degli-studi-di-firenze/mnist-residual-mlp-comparison/runs/wz1tujh1</a><br> View project at: <a href='https://wandb.ai/leonardobiondi-universit-degli-studi-di-firenze/mnist-residual-mlp-comparison' target=\"_blank\">https://wandb.ai/leonardobiondi-universit-degli-studi-di-firenze/mnist-residual-mlp-comparison</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250627_122943-wz1tujh1\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Experiment completed! Check W&B for detailed comparisons.\n"
     ]
    }
   ],
   "source": [
    "# Exercise 1.2: Adding Residual Connections\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision.datasets import MNIST\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import torchvision.transforms as transforms\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import classification_report\n",
    "import wandb\n",
    "\n",
    "# Standard MLP Block\n",
    "class MLPBlock(nn.Module):\n",
    "    def __init__(self, hidden_size, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.fc1(x))\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc2(out)\n",
    "        return F.relu(out)\n",
    "\n",
    "# Residual MLP Block with Skip Connection\n",
    "class ResidualMLPBlock(nn.Module):\n",
    "    def __init__(self, hidden_size, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        identity = x  # Skip connection\n",
    "        out = F.relu(self.fc1(x))\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc2(out)\n",
    "        out = out + identity  # Add skip connection\n",
    "        return F.relu(out)\n",
    "\n",
    "# Standard MLP Network\n",
    "class StandardMLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes, depth, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.input_layer = nn.Linear(input_size, hidden_size)\n",
    "        self.blocks = nn.ModuleList([\n",
    "            MLPBlock(hidden_size, dropout_rate) for _ in range(depth)\n",
    "        ])\n",
    "        self.output_layer = nn.Linear(hidden_size, num_classes)\n",
    "        self.depth = depth\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = F.relu(self.input_layer(x))\n",
    "        \n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "            \n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "\n",
    "# Residual MLP Network\n",
    "class ResidualMLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes, depth, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.input_layer = nn.Linear(input_size, hidden_size)\n",
    "        self.blocks = nn.ModuleList([\n",
    "            ResidualMLPBlock(hidden_size, dropout_rate) for _ in range(depth)\n",
    "        ])\n",
    "        self.output_layer = nn.Linear(hidden_size, num_classes)\n",
    "        self.depth = depth\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = F.relu(self.input_layer(x))\n",
    "        \n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "            \n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "\n",
    "# Training function\n",
    "def train_one_epoch(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for data, target in tqdm(dataloader, desc=\"Training\", leave=False):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        pred = output.argmax(dim=1)\n",
    "        correct += pred.eq(target).sum().item()\n",
    "        total += target.size(0)\n",
    "\n",
    "    return total_loss / len(dataloader), 100.0 * correct / total\n",
    "\n",
    "# Validation function\n",
    "def validate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target in tqdm(dataloader, desc=\"Validating\", leave=False):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            pred = output.argmax(dim=1)\n",
    "            correct += pred.eq(target).sum().item()\n",
    "            total += target.size(0)\n",
    "\n",
    "    return total_loss / len(dataloader), 100.0 * correct / total\n",
    "\n",
    "# Gradient analysis function\n",
    "def analyze_gradients(model, sample_batch, criterion, device, model_name):\n",
    "    model.train()\n",
    "    data, target = sample_batch\n",
    "    data, target = data.to(device), target.to(device)\n",
    "    \n",
    "    # Forward pass\n",
    "    output = model(data)\n",
    "    loss = criterion(output, target)\n",
    "    \n",
    "    # Backward pass\n",
    "    model.zero_grad()\n",
    "    loss.backward()\n",
    "    \n",
    "    # Collect gradient norms\n",
    "    grad_norms = {}\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.grad is not None and 'weight' in name:\n",
    "            grad_norm = param.grad.norm().item()\n",
    "            grad_norms[name] = grad_norm\n",
    "    \n",
    "    return grad_norms\n",
    "\n",
    "# Training pipeline for comparison\n",
    "def train_and_compare_models(standard_mlp, residual_mlp, train_loader, val_loader, \n",
    "                           config, device, depth):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Training Networks with Depth: {depth}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Setup optimizers and criterion\n",
    "    optimizer_std = optim.Adam(standard_mlp.parameters(), lr=config.learning_rate)\n",
    "    optimizer_res = optim.Adam(residual_mlp.parameters(), lr=config.learning_rate)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Storage for results\n",
    "    results = {\n",
    "        'standard': {'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': []},\n",
    "        'residual': {'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': []}\n",
    "    }\n",
    "    \n",
    "    # Sample batch for gradient analysis\n",
    "    sample_batch = next(iter(train_loader))\n",
    "    \n",
    "    for epoch in range(config.epochs):\n",
    "        print(f\"\\nEpoch {epoch + 1}/{config.epochs}\")\n",
    "        \n",
    "        # Train Standard MLP\n",
    "        train_loss_std, train_acc_std = train_one_epoch(standard_mlp, train_loader, \n",
    "                                                      optimizer_std, criterion, device)\n",
    "        val_loss_std, val_acc_std = validate(standard_mlp, val_loader, criterion, device)\n",
    "        \n",
    "        # Train Residual MLP\n",
    "        train_loss_res, train_acc_res = train_one_epoch(residual_mlp, train_loader, \n",
    "                                                      optimizer_res, criterion, device)\n",
    "        val_loss_res, val_acc_res = validate(residual_mlp, val_loader, criterion, device)\n",
    "        \n",
    "        # Store results\n",
    "        results['standard']['train_loss'].append(train_loss_std)\n",
    "        results['standard']['val_loss'].append(val_loss_std)\n",
    "        results['standard']['train_acc'].append(train_acc_std)\n",
    "        results['standard']['val_acc'].append(val_acc_std)\n",
    "        \n",
    "        results['residual']['train_loss'].append(train_loss_res)\n",
    "        results['residual']['val_loss'].append(val_loss_res)\n",
    "        results['residual']['train_acc'].append(train_acc_res)\n",
    "        results['residual']['val_acc'].append(val_acc_res)\n",
    "        \n",
    "        # Log to W&B\n",
    "        wandb.log({\n",
    "            f\"depth_{depth}/standard_train_loss\": train_loss_std,\n",
    "            f\"depth_{depth}/standard_val_loss\": val_loss_std,\n",
    "            f\"depth_{depth}/standard_train_acc\": train_acc_std,\n",
    "            f\"depth_{depth}/standard_val_acc\": val_acc_std,\n",
    "            f\"depth_{depth}/residual_train_loss\": train_loss_res,\n",
    "            f\"depth_{depth}/residual_val_loss\": val_loss_res,\n",
    "            f\"depth_{depth}/residual_train_acc\": train_acc_res,\n",
    "            f\"depth_{depth}/residual_val_acc\": val_acc_res,\n",
    "            \"epoch\": epoch + 1\n",
    "        })\n",
    "        \n",
    "        # Print results\n",
    "        print(f\"Standard MLP  - Train Loss: {train_loss_std:.4f}, Train Acc: {train_acc_std:.2f}%, \"\n",
    "              f\"Val Loss: {val_loss_std:.4f}, Val Acc: {val_acc_std:.2f}%\")\n",
    "        print(f\"Residual MLP  - Train Loss: {train_loss_res:.4f}, Train Acc: {train_acc_res:.2f}%, \"\n",
    "              f\"Val Loss: {val_loss_res:.4f}, Val Acc: {val_acc_res:.2f}%\")\n",
    "    \n",
    "    # Gradient analysis at the end\n",
    "    print(f\"\\n{'-'*40}\")\n",
    "    print(\"GRADIENT ANALYSIS\")\n",
    "    print(f\"{'-'*40}\")\n",
    "    \n",
    "    grad_norms_std = analyze_gradients(standard_mlp, sample_batch, criterion, device, \"Standard\")\n",
    "    grad_norms_res = analyze_gradients(residual_mlp, sample_batch, criterion, device, \"Residual\")\n",
    "    \n",
    "    print(f\"\\nGradient Norms Comparison (Depth {depth}):\")\n",
    "    print(f\"{'Layer':<30} {'Standard MLP':<15} {'Residual MLP':<15} {'Ratio':<10}\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    for layer_std in grad_norms_std:\n",
    "        layer_res = layer_std.replace('blocks', 'blocks')  # Same naming convention\n",
    "        if layer_res in grad_norms_res:\n",
    "            ratio = grad_norms_res[layer_res] / max(grad_norms_std[layer_std], 1e-8)\n",
    "            print(f\"{layer_std:<30} {grad_norms_std[layer_std]:<15.6f} \"\n",
    "                  f\"{grad_norms_res[layer_res]:<15.6f} {ratio:<10.2f}\")\n",
    "    \n",
    "    # Log gradient analysis\n",
    "    for layer_name, norm in grad_norms_std.items():\n",
    "        wandb.log({f\"gradients/depth_{depth}/standard_{layer_name}\": norm})\n",
    "    for layer_name, norm in grad_norms_res.items():\n",
    "        wandb.log({f\"gradients/depth_{depth}/residual_{layer_name}\": norm})\n",
    "    \n",
    "    return results\n",
    "\n",
    "def main():\n",
    "    # Model architecture parameters\n",
    "    input_size = 28 * 28 * 1\n",
    "    hidden_size = 128\n",
    "    num_classes = 10\n",
    "    \n",
    "    # Training hyperparameters\n",
    "    epochs = 15\n",
    "    batch_size = 64\n",
    "    learning_rate = 0.001\n",
    "    val_size = 5000\n",
    "    dropout_rate = 0.1\n",
    "    \n",
    "    # Depths to compare\n",
    "    depths = [1, 3, 5, 8]\n",
    "    \n",
    "    # Initialize W&B\n",
    "    wandb.init(\n",
    "        project=\"mnist-residual-mlp-comparison\",\n",
    "        config={\n",
    "            \"epochs\": epochs,\n",
    "            \"batch_size\": batch_size,\n",
    "            \"learning_rate\": learning_rate,\n",
    "            \"hidden_size\": hidden_size,\n",
    "            \"dropout_rate\": dropout_rate,\n",
    "            \"depths\": depths,\n",
    "            \"architecture\": \"Standard vs Residual MLP\"\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    config = wandb.config\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Data preparation\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "    \n",
    "    # Load datasets\n",
    "    train_dataset = MNIST('./data', train=True, download=True, transform=transform)\n",
    "    test_dataset = MNIST('./data', train=False, transform=transform)\n",
    "    \n",
    "    # Split training set\n",
    "    train_size = len(train_dataset) - val_size\n",
    "    train_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])\n",
    "    \n",
    "    # Data loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "    \n",
    "    print(f\"Dataset sizes - Train: {len(train_dataset)}, Val: {len(val_dataset)}, Test: {len(test_dataset)}\")\n",
    "    \n",
    "    # Compare different depths\n",
    "    all_results = {}\n",
    "    \n",
    "    for depth in depths:\n",
    "        print(f\"\\nCreating models with depth {depth}...\")\n",
    "        \n",
    "        # Create models\n",
    "        standard_mlp = StandardMLP(input_size, hidden_size, num_classes, depth, dropout_rate).to(device)\n",
    "        residual_mlp = ResidualMLP(input_size, hidden_size, num_classes, depth, dropout_rate).to(device)\n",
    "        \n",
    "        print(f\"Standard MLP parameters: {sum(p.numel() for p in standard_mlp.parameters()):,}\")\n",
    "        print(f\"Residual MLP parameters: {sum(p.numel() for p in residual_mlp.parameters()):,}\")\n",
    "        \n",
    "        # Train and compare\n",
    "        results = train_and_compare_models(standard_mlp, residual_mlp, train_loader, \n",
    "                                         val_loader, config, device, depth)\n",
    "        all_results[depth] = results\n",
    "        \n",
    "        # Test final performance\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        _, test_acc_std = validate(standard_mlp, test_loader, criterion, device)\n",
    "        _, test_acc_res = validate(residual_mlp, test_loader, criterion, device)\n",
    "        \n",
    "        print(f\"\\nFinal Test Accuracy - Depth {depth}:\")\n",
    "        print(f\"Standard MLP: {test_acc_std:.2f}%\")\n",
    "        print(f\"Residual MLP: {test_acc_res:.2f}%\")\n",
    "        print(f\"Improvement: {test_acc_res - test_acc_std:.2f}%\")\n",
    "        \n",
    "        # Log final test results\n",
    "        wandb.log({\n",
    "            f\"final_test/depth_{depth}_standard\": test_acc_std,\n",
    "            f\"final_test/depth_{depth}_residual\": test_acc_res,\n",
    "            f\"final_test/depth_{depth}_improvement\": test_acc_res - test_acc_std\n",
    "        })\n",
    "    \n",
    "    # Summary\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"EXPERIMENT SUMMARY\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"{'Depth':<8} {'Standard Acc':<15} {'Residual Acc':<15} {'Improvement':<12}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for depth in depths:\n",
    "        std_acc = all_results[depth]['standard']['val_acc'][-1]\n",
    "        res_acc = all_results[depth]['residual']['val_acc'][-1]\n",
    "        improvement = res_acc - std_acc\n",
    "        print(f\"{depth:<8} {std_acc:<15.2f} {res_acc:<15.2f} {improvement:<12.2f}\")\n",
    "    \n",
    "    wandb.finish()\n",
    "    print(f\"\\nExperiment completed! Check W&B for detailed comparisons.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ace0403-9a9f-43fa-aed3-fca368f9cb34",
   "metadata": {},
   "source": [
    "## Considerazioni Finali \n",
    "\n",
    "### Risultati Sperimentali\n",
    "\n",
    "| Profondità | Standard MLP Acc (%) | Residual MLP Acc (%) | Miglioramento (%) |\n",
    "|------------|----------------------|------------------------|-------------------|\n",
    "| 1          | 97.81                | 97.62                  | -0.19             |\n",
    "| 3          | 97.74                | 97.79                  | +0.05             |\n",
    "| 5          | 97.23                | 97.86                  | +0.63             |\n",
    "| 8          | 84.77                | 98.07                  | +13.30            |\n",
    "\n",
    "---\n",
    "\n",
    "### Evidenza del Vanishing Gradient Problem\n",
    "\n",
    "L'analisi dei gradienti rivela chiaramente il **vanishing gradient problem** negli MLP standard:\n",
    "\n",
    "**Depth 8 - Standard MLP:**\n",
    "- Primo layer: gradiente norm = 0.261  \n",
    "- Ultimo layer: gradiente norm = 4.602 (**esplosione**)  \n",
    "- Layers intermedi: gradienti molto piccoli (~0.02–0.1)\n",
    "\n",
    "**Depth 8 - Residual MLP:**\n",
    "- Gradienti **uniformi**: tutti i layer mantengono norme tra 0.006–0.043  \n",
    "- **Stabilità**: nessuna esplosione o vanishing eccessivo  \n",
    "- **Training efficace**: validation accuracy del **98.07%**\n",
    "\n",
    "---\n",
    "\n",
    "### Analisi del Collasso nella Rete Profonda\n",
    "\n",
    "Il **crollo drammatico** dell'MLP standard a depth 8 (84.77% vs 98.07%) dimostra concretamente:\n",
    "\n",
    "- **Vanishing Gradients**: i layer iniziali ricevono gradienti troppo piccoli per apprendere efficacemente  \n",
    "- **Exploding Gradients**: i layer finali hanno gradienti eccessivi (4.602 vs ~0.01 nei residual)  \n",
    "- **Mancanza di Convergenza**: training accuracy solo **73.11%** vs **99.46%** nei residual\n",
    "\n",
    "---\n",
    "\n",
    "### Skip Connections\n",
    "\n",
    "Le **residual connections** risolvono il problema attraverso:\n",
    "\n",
    "**Flusso diretto del gradiente:**  \n",
    "∇loss → output → skip connection → input\n",
    "\n",
    "- I gradienti possono \"saltare\" i layer intermedi  \n",
    "- Ogni layer riceve un **segnale di gradient robusto**  \n",
    "- **Training stabile** anche in reti profonde\n",
    "\n",
    "**Confronto training accuracy (depth 8):**\n",
    "- Standard MLP: **73.11%** (collasso del training)  \n",
    "- Residual MLP: **99.46%** (convergenza ottimale)\n",
    "\n",
    "---\n",
    "\n",
    "### Implicazioni\n",
    "Questi risultati confermano empiricamente perché le **residual connections** sono diventate **standard** nell'architettura di reti profonde:\n",
    "\n",
    "- Stabilizzano il training prevenendo **vanishing/exploding gradients**  \n",
    "- Abilitano reti più profonde **senza degradazione** delle performance  \n",
    "- Mantengono la **qualità del gradient flow** attraverso tutti i layer\n",
    "\n",
    "**Shallow Networks (depth 1–3):**\n",
    "- Beneficio limitato delle residual connections  \n",
    "- Standard MLP già capace di training efficace  \n",
    "- Overhead delle skip connections non sempre vantaggioso\n",
    "\n",
    "**Medium Networks (depth 5):**\n",
    "- Miglioramento moderato (+0.63%)  \n",
    "- Prime evidenze di vanishing gradients nell'MLP standard  \n",
    "- Residual connections iniziano a mostrare valore\n",
    "\n",
    "**Deep Networks (depth 8):**\n",
    "- **Beneficio drammatico** (+13.30%)  \n",
    "- MLP standard completamente incapace di training  \n",
    "- Residual MLP mantiene performance eccellenti\n",
    "\n",
    "---\n",
    "\n",
    "### Conclusione\n",
    "\n",
    "L'esperimento dimostra chiaramente che le **residual connections** non sono solo un'**ottimizzazione tecnica**, ma una **necessità fondamentale** per il training di reti profonde.\n",
    "\n",
    "La differenza di **13.3 punti percentuali** a depth 8 evidenzia come, senza skip connections, l’aumento della profondità diventi **controproducente**, mentre con esse si mantengano **performance ottimali** indipendentemente dalla profondità della rete.\n",
    "\n",
    "Questo risultato pone le basi **teoriche e pratiche** per comprendere l’importanza delle architetture moderne come **ResNet**, dove le skip connections permettono di trainare reti con **centinaia di layer** mantenendo un **gradient flow efficace**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c59bdd8-3377-4311-b45f-511c2fb0b53e",
   "metadata": {},
   "source": [
    "### Exercise 1.3: Rinse and Repeat (but with a CNN)\n",
    "\n",
    "Repeat the verification you did above, but with **Convolutional** Neural Networks. If you were careful about abstracting your model and training code, this should be a simple exercise. Show that **deeper** CNNs *without* residual connections do not always work better and **even deeper** ones *with* residual connections.\n",
    "\n",
    "**Hint**: You probably should do this exercise using CIFAR-10, since MNIST is *very* easy (at least up to about 99% accuracy).\n",
    "\n",
    "**Tip**: Feel free to reuse the ResNet building blocks defined in `torchvision.models.resnet` (e.g. [BasicBlock](https://github.com/pytorch/vision/blob/main/torchvision/models/resnet.py#L59) which handles the cascade of 3x3 convolutions, skip connections, and optional downsampling). This is an excellent exercise in code diving. \n",
    "\n",
    "**Spoiler**: Depending on the optional exercises you plan to do below, you should think *very* carefully about the architectures of your CNNs here (so you can reuse them!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c8baa0e-b17f-4a77-8a88-dadfdc6763ea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: NVIDIA GeForce RTX 3060\n",
      "Starting CNN comparison on CIFAR-10...\n",
      "Configuration: 10 epochs, batch size 64\n",
      "Dataset sizes - Train: 45000, Val: 5000, Test: 10000\n",
      "\n",
      "==================================================\n",
      "TRAINING SIMPLE CNN (BASELINE)\n",
      "==================================================\n",
      "Simple CNN parameters: 8,475,530\n",
      "\n",
      "Training SimpleCNN for 10 epochs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|████████████████████████████| 704/704 [00:11<00:00, 59.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Acc: 45.98%, Val Acc: 59.14%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|████████████████████████████| 704/704 [00:12<00:00, 58.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Train Acc: 60.41%, Val Acc: 62.60%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|████████████████████████████| 704/704 [00:12<00:00, 57.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Train Acc: 65.99%, Val Acc: 65.88%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|████████████████████████████| 704/704 [00:12<00:00, 58.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Train Acc: 69.33%, Val Acc: 67.98%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|████████████████████████████| 704/704 [00:12<00:00, 57.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train Acc: 71.84%, Val Acc: 72.08%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|████████████████████████████| 704/704 [00:11<00:00, 58.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Train Acc: 74.29%, Val Acc: 73.34%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|████████████████████████████| 704/704 [00:12<00:00, 56.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: Train Acc: 76.01%, Val Acc: 73.08%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|████████████████████████████| 704/704 [00:12<00:00, 57.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: Train Acc: 78.00%, Val Acc: 72.32%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|████████████████████████████| 704/704 [00:12<00:00, 55.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: Train Acc: 79.41%, Val Acc: 74.36%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|███████████████████████████| 704/704 [00:12<00:00, 58.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: Train Acc: 80.75%, Val Acc: 75.02%\n",
      "\n",
      "Testing SimpleCNN...\n",
      "SimpleCNN Test Accuracy: 74.85%\n",
      "\n",
      "==================================================\n",
      "TRAINING DEEP CNN (NO RESIDUAL)\n",
      "==================================================\n",
      "Deep CNN parameters: 4,570,762\n",
      "\n",
      "Training DeepCNN for 10 epochs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|████████████████████████████| 704/704 [00:15<00:00, 46.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Acc: 41.32%, Val Acc: 56.98%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|████████████████████████████| 704/704 [00:14<00:00, 47.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Train Acc: 60.74%, Val Acc: 67.24%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|████████████████████████████| 704/704 [00:15<00:00, 45.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Train Acc: 68.07%, Val Acc: 69.68%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|████████████████████████████| 704/704 [00:15<00:00, 46.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Train Acc: 71.90%, Val Acc: 75.58%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|████████████████████████████| 704/704 [00:14<00:00, 47.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train Acc: 75.18%, Val Acc: 76.64%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|████████████████████████████| 704/704 [00:15<00:00, 46.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Train Acc: 77.55%, Val Acc: 75.52%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|████████████████████████████| 704/704 [00:15<00:00, 45.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: Train Acc: 79.21%, Val Acc: 78.10%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|████████████████████████████| 704/704 [00:15<00:00, 46.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: Train Acc: 80.71%, Val Acc: 79.22%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|████████████████████████████| 704/704 [00:15<00:00, 46.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: Train Acc: 82.08%, Val Acc: 79.44%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|███████████████████████████| 704/704 [00:15<00:00, 46.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: Train Acc: 83.42%, Val Acc: 80.10%\n",
      "\n",
      "Testing DeepCNN...\n",
      "DeepCNN Test Accuracy: 79.35%\n",
      "\n",
      "==================================================\n",
      "TRAINING RESNET CNN (WITH RESIDUAL)\n",
      "==================================================\n",
      "ResNet CNN parameters: 11,173,962\n",
      "\n",
      "Training ResNetCNN for 10 epochs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|████████████████████████████| 704/704 [00:33<00:00, 21.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Acc: 55.54%, Val Acc: 66.54%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|████████████████████████████| 704/704 [00:33<00:00, 20.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Train Acc: 73.42%, Val Acc: 71.52%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|████████████████████████████| 704/704 [00:33<00:00, 20.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Train Acc: 79.29%, Val Acc: 79.36%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|████████████████████████████| 704/704 [00:33<00:00, 20.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Train Acc: 83.03%, Val Acc: 81.40%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|████████████████████████████| 704/704 [00:33<00:00, 21.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train Acc: 85.67%, Val Acc: 83.56%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|████████████████████████████| 704/704 [00:33<00:00, 21.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Train Acc: 87.86%, Val Acc: 82.98%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|████████████████████████████| 704/704 [00:33<00:00, 21.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: Train Acc: 89.63%, Val Acc: 85.28%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|████████████████████████████| 704/704 [00:33<00:00, 21.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: Train Acc: 91.25%, Val Acc: 86.18%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|████████████████████████████| 704/704 [00:33<00:00, 21.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: Train Acc: 92.43%, Val Acc: 85.80%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|███████████████████████████| 704/704 [00:33<00:00, 21.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: Train Acc: 93.88%, Val Acc: 86.36%\n",
      "\n",
      "Testing ResNetCNN...\n",
      "ResNetCNN Test Accuracy: 84.94%\n",
      "\n",
      "======================================================================\n",
      "EXPERIMENT SUMMARY - CNN PROGRESSION ON CIFAR-10\n",
      "======================================================================\n",
      "Model        Val Acc    Test Acc   Parameters  \n",
      "----------------------------------------------------------------------\n",
      "SimpleCNN    75.02      74.85      8,475,530   \n",
      "DeepCNN      80.10      79.35      4,570,762   \n",
      "ResNetCNN    86.36      84.94      11,173,962  \n",
      "\n",
      "======================================================================\n",
      "ANALYSIS - EFFECT OF RESIDUAL CONNECTIONS:\n",
      "======================================================================\n",
      "RESULTS:\n",
      "• Simple CNN:  74.85% (baseline with 2 conv layers)\n",
      "• Deep CNN:    79.35% (4 conv layers, no residual connections)\n",
      "• ResNet CNN:  84.94% (ResNet-18 architecture with residual connections)\n",
      "\n",
      "======================================================================\n",
      "Experiment completed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Exercise 1.3: Rinse and Repeat (but with a CNN)\n",
    "# Academic version with essential improvements\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "\n",
    "# Setup device\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    torch.cuda.empty_cache()\n",
    "    print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"Using CPU\")\n",
    "\n",
    "# 1. Simple CNN (Baseline)\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(128 * 8 * 8, 1024)\n",
    "        self.fc2 = nn.Linear(1024, num_classes)\n",
    "        self.dropout = nn.Dropout(0.3)  # For consistency with MLP approach\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2(x), 2))\n",
    "        x = x.view(-1, 128 * 8 * 8)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        return self.fc2(x)\n",
    "\n",
    "# 2. Deep CNN (No residual connections)\n",
    "class DeepCNN(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(128, 128, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 128, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128 * 8 * 8, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4),  # For consistency with MLP approach\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        return self.classifier(x)\n",
    "\n",
    "# 3. ResNet CNN (With residual connections - from scratch)\n",
    "class ResNetCNN(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super().__init__()\n",
    "        # Use ResNet18 architecture but train from scratch for fair comparison\n",
    "        self.resnet = models.resnet18(weights=False)\n",
    "        \n",
    "        # Modify for CIFAR-10 (32x32 instead of ImageNet 224x224)\n",
    "        self.resnet.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.resnet.maxpool = nn.Identity()  # Remove maxpool for smaller images\n",
    "        \n",
    "        # Modify final layer for CIFAR-10 (10 classes)\n",
    "        self.resnet.fc = nn.Linear(self.resnet.fc.in_features, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.resnet(x)\n",
    "\n",
    "# Training function\n",
    "def train_model(model, train_loader, val_loader, epochs, device, model_name):\n",
    "    print(f\"\\nTraining {model_name} for {epochs} epochs...\")\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    best_val_acc = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for inputs, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        train_acc = 100 * correct / total\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += (predicted == labels).sum().item()\n",
    "\n",
    "        val_loss = val_loss / len(val_loader)\n",
    "        val_acc = 100 * val_correct / val_total\n",
    "        \n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "\n",
    "        print(f\"Epoch {epoch+1}: Train Acc: {train_acc:.2f}%, Val Acc: {val_acc:.2f}%\")\n",
    "    \n",
    "    return best_val_acc\n",
    "\n",
    "# Test function\n",
    "def test_model(model, test_loader, device, model_name):\n",
    "    print(f\"\\nTesting {model_name}...\")\n",
    "    \n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    test_acc = 100 * correct / total\n",
    "    print(f\"{model_name} Test Accuracy: {test_acc:.2f}%\")\n",
    "    return test_acc\n",
    "\n",
    "# Main experiment\n",
    "def main():\n",
    "    print(\"Starting CNN comparison on CIFAR-10...\")\n",
    "    \n",
    "    # Standard academic parameters\n",
    "    epochs = 10\n",
    "    batch_size = 64\n",
    "    val_size = 5000\n",
    "    \n",
    "    print(f\"Configuration: {epochs} epochs, batch size {batch_size}\")\n",
    "    \n",
    "    # Minimal data augmentation - academic standard\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "    test_transform = transforms.Compose([\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "    # Load CIFAR-10 dataset\n",
    "    train_dataset = CIFAR10('./data', train=True, download=True, transform=train_transform)\n",
    "    test_dataset = CIFAR10('./data', train=False, transform=test_transform)\n",
    "    \n",
    "    # Split training set into train/validation\n",
    "    train_size = len(train_dataset) - val_size\n",
    "    train_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "    \n",
    "    print(f\"Dataset sizes - Train: {len(train_dataset)}, Val: {len(val_dataset)}, Test: {len(test_dataset)}\")\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # ===== MODEL 1: Simple CNN =====\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(\"TRAINING SIMPLE CNN (BASELINE)\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    simple_cnn = SimpleCNN().to(device)\n",
    "    params = sum(p.numel() for p in simple_cnn.parameters())\n",
    "    print(f\"Simple CNN parameters: {params:,}\")\n",
    "    \n",
    "    best_val_acc = train_model(simple_cnn, train_loader, val_loader, epochs, device, \"SimpleCNN\")\n",
    "    test_acc = test_model(simple_cnn, test_loader, device, \"SimpleCNN\")\n",
    "    \n",
    "    results['SimpleCNN'] = {\n",
    "        'val_acc': best_val_acc,\n",
    "        'test_acc': test_acc,\n",
    "        'params': params\n",
    "    }\n",
    "    \n",
    "    # Cleanup memory\n",
    "    del simple_cnn\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    # ===== MODEL 2: Deep CNN =====\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(\"TRAINING DEEP CNN (NO RESIDUAL)\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    deep_cnn = DeepCNN().to(device)\n",
    "    params = sum(p.numel() for p in deep_cnn.parameters())\n",
    "    print(f\"Deep CNN parameters: {params:,}\")\n",
    "    \n",
    "    best_val_acc = train_model(deep_cnn, train_loader, val_loader, epochs, device, \"DeepCNN\")\n",
    "    test_acc = test_model(deep_cnn, test_loader, device, \"DeepCNN\")\n",
    "    \n",
    "    results['DeepCNN'] = {\n",
    "        'val_acc': best_val_acc,\n",
    "        'test_acc': test_acc,\n",
    "        'params': params\n",
    "    }\n",
    "    \n",
    "    # Cleanup memory\n",
    "    del deep_cnn\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    # ===== MODEL 3: ResNet CNN =====\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(\"TRAINING RESNET CNN (WITH RESIDUAL)\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    resnet_cnn = ResNetCNN().to(device)\n",
    "    params = sum(p.numel() for p in resnet_cnn.parameters())\n",
    "    print(f\"ResNet CNN parameters: {params:,}\")\n",
    "    \n",
    "    best_val_acc = train_model(resnet_cnn, train_loader, val_loader, epochs, device, \"ResNetCNN\")\n",
    "    test_acc = test_model(resnet_cnn, test_loader, device, \"ResNetCNN\")\n",
    "    \n",
    "    results['ResNetCNN'] = {\n",
    "        'val_acc': best_val_acc,\n",
    "        'test_acc': test_acc,\n",
    "        'params': params\n",
    "    }\n",
    "    \n",
    "    # Cleanup memory\n",
    "    del resnet_cnn\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    # ===== DETAILED ANALYSIS =====\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"EXPERIMENT SUMMARY - CNN PROGRESSION ON CIFAR-10\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"{'Model':<12} {'Val Acc':<10} {'Test Acc':<10} {'Parameters':<12}\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    for model_name, result in results.items():\n",
    "        print(f\"{model_name:<12} {result['val_acc']:<10.2f} {result['test_acc']:<10.2f} {result['params']:<12,}\")\n",
    "    \n",
    "    # Detailed analysis\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"ANALYSIS - EFFECT OF RESIDUAL CONNECTIONS:\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    simple_acc = results.get('SimpleCNN', {}).get('test_acc', 0)\n",
    "    deep_acc = results.get('DeepCNN', {}).get('test_acc', 0)\n",
    "    resnet_acc = results.get('ResNetCNN', {}).get('test_acc', 0)\n",
    "    \n",
    "    print(f\"RESULTS:\")\n",
    "    print(f\"• Simple CNN:  {simple_acc:.2f}% (baseline with 2 conv layers)\")\n",
    "    print(f\"• Deep CNN:    {deep_acc:.2f}% (4 conv layers, no residual connections)\")\n",
    "    print(f\"• ResNet CNN:  {resnet_acc:.2f}% (ResNet-18 architecture with residual connections)\")\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"Experiment completed successfully!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f71b1b-b523-44bd-b049-280d1c281b0a",
   "metadata": {},
   "source": [
    "## Considerazioni Finali \n",
    "\n",
    "### Risultati Sperimentali\n",
    "\n",
    "| Modello     | Test Accuracy | Parametri | Architettura                      |\n",
    "|-------------|----------------|-----------|-----------------------------------|\n",
    "| SimpleCNN   | 74.85%         | 8.5M      | 2 conv layers (baseline)          |\n",
    "| DeepCNN     | 79.35%         | 4.6M      | 4 conv layers, no residual        |\n",
    "| ResNetCNN   | 84.94%         | 11.2M     | ResNet-18 with residual connections |\n",
    "\n",
    "---\n",
    "\n",
    "### Analisi delle Performance\n",
    "\n",
    "**Progressione delle accuratezze:**\n",
    "\n",
    "- SimpleCNN → DeepCNN: **+4.5%** di miglioramento  \n",
    "- DeepCNN → ResNetCNN: **+5.59%** di miglioramento  \n",
    "- **Miglioramento totale:** +10.1% (SimpleCNN → ResNetCNN)\n",
    "\n",
    "---\n",
    "\n",
    "### Conclusioni\n",
    "\n",
    "**1. Efficacia della Profondità Standard**  \n",
    "Il **DeepCNN** dimostra che l'aumento di profondità (da 2 a 4 conv layers) porta a **miglioramenti significativi**, contrariamente al problema del vanishing gradient che spesso si manifesta in reti molto più profonde.\n",
    "\n",
    "**2. Potenza delle Residual Connections**  \n",
    "Il **ResNetCNN** ottiene le performance migliori (**84.94%**), dimostrando come le **skip connections** permettano di sfruttare efficacemente **architetture più profonde** (ResNet-18 vs 4 layer CNN).\n",
    "L’esperimento dimostra che **non è solo la profondità** a determinare le performance, ma **come** la profondità viene implementata.  \n",
    "Le **residual connections** permettono **gradient flow efficace** anche in reti profonde.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4de2f2-abc5-4f98-9eaf-3497f734a022",
   "metadata": {},
   "source": [
    "-----\n",
    "## Exercise 2: Choose at Least One\n",
    "\n",
    "Below are **three** exercises that ask you to deepen your understanding of Deep Networks for visual recognition. You must choose **at least one** of the below for your final submission -- feel free to do **more**, but at least **ONE** you must submit. Each exercise is designed to require you to dig your hands **deep** into the guts of your models in order to do new and interesting things.\n",
    "\n",
    "**Note**: These exercises are designed to use your small, custom CNNs and small datasets. This is to keep training times reasonable. If you have a decent GPU, feel free to use pretrained ResNets and larger datasets (e.g. the [Imagenette](https://pytorch.org/vision/0.20/generated/torchvision.datasets.Imagenette.html#torchvision.datasets.Imagenette) dataset at 160px)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07978e8e-9f2e-4949-9699-495af6cb6349",
   "metadata": {},
   "source": [
    "### Exercise 2.1: *Fine-tune* a pre-trained model\n",
    "Train one of your residual CNN models from Exercise 1.3 on CIFAR-10. Then:\n",
    "1. Use the pre-trained model as a **feature extractor** (i.e. to extract the feature activations of the layer input into the classifier) on CIFAR-100. Use a **classical** approach (e.g. Linear SVM, K-Nearest Neighbor, or Bayesian Generative Classifier) from scikit-learn to establish a **stable baseline** performance on CIFAR-100 using the features extracted using your CNN.\n",
    "2. Fine-tune your CNN on the CIFAR-100 training set and compare with your stable baseline. Experiment with different strategies:\n",
    "    - Unfreeze some of the earlier layers for fine-tuning.\n",
    "    - Test different optimizers (Adam, SGD, etc.).\n",
    "\n",
    "Each of these steps will require you to modify your model definition in some way. For 1, you will need to return the activations of the last fully-connected layer (or the global average pooling layer). For 2, you will need to replace the original, 10-class classifier with a new, randomly-initialized 100-class classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469e81a3-08ca-4549-a2f8-f47cf5a0308b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440a3a7b-2ed6-4f58-a1b7-5ab1fc432893",
   "metadata": {},
   "source": [
    "### Exercise 2.2: *Distill* the knowledge from a large model into a smaller one\n",
    "In this exercise you will see if you can derive a *small* model that performs comparably to a larger one on CIFAR-10. To do this, you will use [Knowledge Distillation](https://arxiv.org/abs/1503.02531):\n",
    "\n",
    "> Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the Knowledge in a Neural Network, NeurIPS 2015.\n",
    "\n",
    "To do this:\n",
    "1. Train one of your best-performing CNNs on CIFAR-10 from Exercise 1.3 above. This will be your **teacher** model.\n",
    "2. Define a *smaller* variant with about half the number of parameters (change the width and/or depth of the network). Train it on CIFAR-10 and verify that it performs *worse* than your **teacher**. This small network will be your **student** model.\n",
    "3. Train the **student** using a combination of **hard labels** from the CIFAR-10 training set (cross entropy loss) and **soft labels** from predictions of the **teacher** (Kulback-Leibler loss between teacher and student).\n",
    "\n",
    "Try to optimize training parameters in order to maximize the performance of the student. It should at least outperform the student trained only on hard labels in Setp 2.\n",
    "\n",
    "**Tip**: You can save the predictions of the trained teacher network on the training set and adapt your dataloader to provide them together with hard labels. This will **greatly** speed up training compared to performing a forward pass through the teacher for each batch of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e33c912-0716-44ef-a91b-47ca19a2b2cd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: NVIDIA GeForce RTX 3060\n",
      "Starting Knowledge Distillation Experiment on CIFAR-10...\n",
      "Dataset sizes - Train: 45000, Val: 5000, Test: 10000\n",
      "\n",
      "============================================================\n",
      "STEP 1: TRAINING TEACHER MODEL (ResNetCNN)\n",
      "============================================================\n",
      "Teacher parameters: 11,173,962\n",
      "\n",
      "Training Teacher for 10 epochs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|████████████████████████████| 704/704 [00:37<00:00, 19.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Acc: 54.85%, Val Acc: 58.98%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|████████████████████████████| 704/704 [00:36<00:00, 19.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Train Acc: 72.94%, Val Acc: 74.16%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|████████████████████████████| 704/704 [00:36<00:00, 19.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Train Acc: 79.28%, Val Acc: 76.48%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|████████████████████████████| 704/704 [00:33<00:00, 20.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Train Acc: 83.13%, Val Acc: 77.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|████████████████████████████| 704/704 [00:33<00:00, 20.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train Acc: 85.73%, Val Acc: 81.36%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|████████████████████████████| 704/704 [00:33<00:00, 20.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Train Acc: 87.86%, Val Acc: 83.34%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|████████████████████████████| 704/704 [00:34<00:00, 20.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: Train Acc: 89.73%, Val Acc: 84.36%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|████████████████████████████| 704/704 [00:34<00:00, 20.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: Train Acc: 91.26%, Val Acc: 85.28%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|████████████████████████████| 704/704 [00:34<00:00, 20.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: Train Acc: 92.79%, Val Acc: 85.92%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|███████████████████████████| 704/704 [00:34<00:00, 20.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: Train Acc: 93.69%, Val Acc: 84.64%\n",
      "\n",
      "Testing Teacher...\n",
      "Teacher Test Accuracy: 84.63%\n",
      "\n",
      "============================================================\n",
      "STEP 2: TRAINING STUDENT BASELINE (HARD LABELS ONLY)\n",
      "============================================================\n",
      "Student parameters: 2,159,114\n",
      "Parameter reduction: 80.7%\n",
      "\n",
      "Training Student_Baseline for 15 epochs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/15: 100%|████████████████████████████| 704/704 [00:11<00:00, 63.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Acc: 43.42%, Val Acc: 53.24%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/15: 100%|████████████████████████████| 704/704 [00:12<00:00, 57.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Train Acc: 58.46%, Val Acc: 63.54%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/15: 100%|████████████████████████████| 704/704 [00:12<00:00, 58.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Train Acc: 64.42%, Val Acc: 66.36%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/15: 100%|████████████████████████████| 704/704 [00:12<00:00, 56.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Train Acc: 68.67%, Val Acc: 69.54%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/15: 100%|████████████████████████████| 704/704 [00:12<00:00, 57.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train Acc: 71.19%, Val Acc: 71.86%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/15: 100%|████████████████████████████| 704/704 [00:12<00:00, 58.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Train Acc: 73.77%, Val Acc: 72.72%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/15: 100%|████████████████████████████| 704/704 [00:12<00:00, 56.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: Train Acc: 75.81%, Val Acc: 73.42%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/15: 100%|████████████████████████████| 704/704 [00:12<00:00, 56.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: Train Acc: 77.39%, Val Acc: 73.80%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/15: 100%|████████████████████████████| 704/704 [00:12<00:00, 57.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: Train Acc: 78.95%, Val Acc: 74.72%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/15: 100%|███████████████████████████| 704/704 [00:12<00:00, 58.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: Train Acc: 80.32%, Val Acc: 75.22%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/15: 100%|███████████████████████████| 704/704 [00:12<00:00, 56.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: Train Acc: 81.57%, Val Acc: 75.88%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/15: 100%|███████████████████████████| 704/704 [00:12<00:00, 57.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: Train Acc: 82.80%, Val Acc: 75.60%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/15: 100%|███████████████████████████| 704/704 [00:12<00:00, 57.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: Train Acc: 84.25%, Val Acc: 74.94%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/15: 100%|███████████████████████████| 704/704 [00:11<00:00, 59.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: Train Acc: 85.01%, Val Acc: 75.26%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/15: 100%|███████████████████████████| 704/704 [00:11<00:00, 58.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15: Train Acc: 85.96%, Val Acc: 74.88%\n",
      "\n",
      "Testing Student_Baseline...\n",
      "Student_Baseline Test Accuracy: 74.93%\n",
      "\n",
      "============================================================\n",
      "STEP 3: KNOWLEDGE DISTILLATION\n",
      "============================================================\n",
      "Saving teacher predictions for distillation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting teacher knowledge: 100%|██████████| 704/704 [00:15<00:00, 45.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Testing hyperparameters 1/3: T=4.0, α=0.7 ---\n",
      "\n",
      "Training Student with Knowledge Distillation (T=4.0, α=0.7)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "KD Epoch 1/15: 100%|████████████████████████| 704/704 [00:03<00:00, 231.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Acc: 42.36%, Val Acc: 54.56%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "KD Epoch 2/15: 100%|████████████████████████| 704/704 [00:03<00:00, 231.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Train Acc: 56.87%, Val Acc: 61.32%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "KD Epoch 3/15: 100%|████████████████████████| 704/704 [00:03<00:00, 219.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Train Acc: 63.29%, Val Acc: 64.88%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "KD Epoch 4/15: 100%|████████████████████████| 704/704 [00:03<00:00, 230.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Train Acc: 68.51%, Val Acc: 67.78%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "KD Epoch 5/15: 100%|████████████████████████| 704/704 [00:03<00:00, 226.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train Acc: 71.69%, Val Acc: 71.26%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "KD Epoch 6/15: 100%|████████████████████████| 704/704 [00:03<00:00, 219.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Train Acc: 74.20%, Val Acc: 72.68%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "KD Epoch 7/15: 100%|████████████████████████| 704/704 [00:03<00:00, 229.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: Train Acc: 76.35%, Val Acc: 73.34%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "KD Epoch 8/15: 100%|████████████████████████| 704/704 [00:03<00:00, 226.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: Train Acc: 78.55%, Val Acc: 74.34%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "KD Epoch 9/15: 100%|████████████████████████| 704/704 [00:03<00:00, 224.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: Train Acc: 79.85%, Val Acc: 75.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "KD Epoch 10/15: 100%|███████████████████████| 704/704 [00:03<00:00, 217.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: Train Acc: 81.34%, Val Acc: 75.26%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "KD Epoch 11/15: 100%|███████████████████████| 704/704 [00:03<00:00, 216.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: Train Acc: 82.68%, Val Acc: 76.30%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "KD Epoch 12/15: 100%|███████████████████████| 704/704 [00:03<00:00, 213.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: Train Acc: 83.44%, Val Acc: 77.26%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "KD Epoch 13/15: 100%|███████████████████████| 704/704 [00:03<00:00, 198.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: Train Acc: 84.72%, Val Acc: 77.28%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "KD Epoch 14/15: 100%|███████████████████████| 704/704 [00:03<00:00, 209.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: Train Acc: 85.60%, Val Acc: 77.16%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "KD Epoch 15/15: 100%|███████████████████████| 704/704 [00:03<00:00, 218.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15: Train Acc: 86.65%, Val Acc: 77.30%\n",
      "\n",
      "Testing Student_KD_T4.0_A0.7...\n",
      "Student_KD_T4.0_A0.7 Test Accuracy: 76.82%\n",
      "\n",
      "--- Testing hyperparameters 2/3: T=6.0, α=0.8 ---\n",
      "\n",
      "Training Student with Knowledge Distillation (T=6.0, α=0.8)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "KD Epoch 1/15: 100%|████████████████████████| 704/704 [00:03<00:00, 199.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Acc: 40.84%, Val Acc: 42.10%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "KD Epoch 2/15: 100%|████████████████████████| 704/704 [00:03<00:00, 224.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Train Acc: 56.05%, Val Acc: 61.42%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "KD Epoch 3/15: 100%|████████████████████████| 704/704 [00:03<00:00, 227.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Train Acc: 63.10%, Val Acc: 64.62%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "KD Epoch 4/15: 100%|████████████████████████| 704/704 [00:03<00:00, 215.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Train Acc: 66.98%, Val Acc: 68.54%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "KD Epoch 5/15: 100%|████████████████████████| 704/704 [00:03<00:00, 223.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train Acc: 70.19%, Val Acc: 69.84%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "KD Epoch 6/15: 100%|████████████████████████| 704/704 [00:03<00:00, 199.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Train Acc: 72.59%, Val Acc: 71.60%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "KD Epoch 7/15: 100%|████████████████████████| 704/704 [00:03<00:00, 191.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: Train Acc: 74.75%, Val Acc: 72.52%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "KD Epoch 8/15: 100%|████████████████████████| 704/704 [00:03<00:00, 222.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: Train Acc: 76.39%, Val Acc: 74.50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "KD Epoch 9/15: 100%|████████████████████████| 704/704 [00:03<00:00, 218.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: Train Acc: 77.72%, Val Acc: 74.30%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "KD Epoch 10/15: 100%|███████████████████████| 704/704 [00:03<00:00, 220.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: Train Acc: 79.02%, Val Acc: 75.24%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "KD Epoch 11/15: 100%|███████████████████████| 704/704 [00:03<00:00, 205.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: Train Acc: 80.25%, Val Acc: 73.96%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "KD Epoch 12/15: 100%|███████████████████████| 704/704 [00:03<00:00, 223.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: Train Acc: 81.27%, Val Acc: 73.68%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "KD Epoch 13/15: 100%|███████████████████████| 704/704 [00:03<00:00, 199.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: Train Acc: 82.05%, Val Acc: 75.64%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "KD Epoch 14/15: 100%|███████████████████████| 704/704 [00:03<00:00, 200.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: Train Acc: 83.10%, Val Acc: 75.44%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "KD Epoch 15/15: 100%|███████████████████████| 704/704 [00:03<00:00, 207.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15: Train Acc: 83.78%, Val Acc: 76.66%\n",
      "\n",
      "Testing Student_KD_T6.0_A0.8...\n",
      "Student_KD_T6.0_A0.8 Test Accuracy: 76.48%\n",
      "\n",
      "--- Testing hyperparameters 3/3: T=3.0, α=0.6 ---\n",
      "\n",
      "Training Student with Knowledge Distillation (T=3.0, α=0.6)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "KD Epoch 1/15: 100%|████████████████████████| 704/704 [00:03<00:00, 180.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Acc: 43.79%, Val Acc: 53.60%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "KD Epoch 2/15: 100%|████████████████████████| 704/704 [00:03<00:00, 218.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Train Acc: 58.30%, Val Acc: 62.94%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "KD Epoch 3/15: 100%|████████████████████████| 704/704 [00:03<00:00, 193.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Train Acc: 65.18%, Val Acc: 66.10%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "KD Epoch 4/15: 100%|████████████████████████| 704/704 [00:03<00:00, 225.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Train Acc: 69.53%, Val Acc: 70.32%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "KD Epoch 5/15: 100%|████████████████████████| 704/704 [00:03<00:00, 209.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train Acc: 72.94%, Val Acc: 70.42%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "KD Epoch 6/15: 100%|████████████████████████| 704/704 [00:03<00:00, 218.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Train Acc: 75.78%, Val Acc: 71.14%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "KD Epoch 7/15: 100%|████████████████████████| 704/704 [00:03<00:00, 219.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: Train Acc: 78.07%, Val Acc: 73.06%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "KD Epoch 8/15: 100%|████████████████████████| 704/704 [00:03<00:00, 224.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: Train Acc: 79.56%, Val Acc: 75.64%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "KD Epoch 9/15: 100%|████████████████████████| 704/704 [00:03<00:00, 232.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: Train Acc: 81.69%, Val Acc: 75.74%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "KD Epoch 10/15: 100%|███████████████████████| 704/704 [00:03<00:00, 219.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: Train Acc: 83.07%, Val Acc: 76.32%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "KD Epoch 11/15: 100%|███████████████████████| 704/704 [00:03<00:00, 230.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: Train Acc: 84.72%, Val Acc: 77.30%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "KD Epoch 12/15: 100%|███████████████████████| 704/704 [00:03<00:00, 220.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: Train Acc: 86.28%, Val Acc: 75.12%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "KD Epoch 13/15: 100%|███████████████████████| 704/704 [00:03<00:00, 229.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: Train Acc: 87.28%, Val Acc: 75.86%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "KD Epoch 14/15: 100%|███████████████████████| 704/704 [00:03<00:00, 231.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: Train Acc: 88.31%, Val Acc: 76.74%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "KD Epoch 15/15: 100%|███████████████████████| 704/704 [00:03<00:00, 232.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15: Train Acc: 89.21%, Val Acc: 76.16%\n",
      "\n",
      "Testing Student_KD_T3.0_A0.6...\n",
      "Student_KD_T3.0_A0.6 Test Accuracy: 76.64%\n",
      "\n",
      "================================================================================\n",
      "KNOWLEDGE DISTILLATION EXPERIMENT SUMMARY\n",
      "================================================================================\n",
      "Model           Train Acc   Val Acc    Test Acc   Parameters   Improvement \n",
      "-----------------------------------------------------------------------------------------------\n",
      "Teacher         93.69       85.92      84.63      11,173,962   Teacher     \n",
      "Student_Baseline 85.96       75.88      74.93      2,159,114    Baseline    \n",
      "Student_KD      86.65       77.30      76.82      2,159,114    +1.89%      \n",
      "\n",
      "================================================================================\n",
      "DETAILED ANALYSIS:\n",
      "================================================================================\n",
      "RESULTS:\n",
      "• Teacher (ResNet):     Train: 93.69%, Test: 84.63% with 11,173,962 parameters\n",
      "• Student Baseline:     Train: 85.96%, Test: 74.93% with 2,159,114 parameters\n",
      "• Student + KD:         Train: 86.65%, Test: 76.82% with 2,159,114 parameters\n",
      "• Best KD parameters:   T=4.0, α=0.7\n",
      "\n",
      "KNOWLEDGE DISTILLATION EFFECTIVENESS:\n",
      "• Improvement over baseline: +1.89%\n",
      "\n",
      "OVERFITTING ANALYSIS:\n",
      "• Teacher train-test gap:       +9.06%\n",
      "• Student baseline gap:         +11.03%\n",
      "• Student KD gap:               +9.83%\n",
      "SUCCESS: Knowledge distillation improved student performance!\n",
      "\n",
      "MODEL EFFICIENCY:\n",
      "• Teacher-Student gap: 7.81%\n",
      "• Parameter efficiency: 0.87 accuracy loss per 1M parameters saved\n",
      "\n",
      "================================================================================\n",
      "Experiment completed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Exercise 2.2: Knowledge Distillation\n",
    "# Distill knowledge from ResNetCNN (teacher) to a smaller model (student)\n",
    "# Based on Exercise 1.3 code structure\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torch.utils.data import DataLoader, random_split, TensorDataset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "\n",
    "# Setup device (same as Exercise 1.3)\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    torch.cuda.empty_cache()\n",
    "    print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"Using CPU\")\n",
    "\n",
    "# ===== TEACHER MODEL: ResNetCNN from Exercise 1.3 =====\n",
    "class ResNetTeacher(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super().__init__()\n",
    "        # Same as your ResNetCNN from Exercise 1.3\n",
    "        self.resnet = models.resnet18(weights=None)\n",
    "        \n",
    "        # Modify for CIFAR-10 (32x32 instead of ImageNet 224x224)\n",
    "        self.resnet.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.resnet.maxpool = nn.Identity()  # Remove maxpool for smaller images\n",
    "        \n",
    "        # Modify final layer for CIFAR-10 (10 classes)\n",
    "        self.resnet.fc = nn.Linear(self.resnet.fc.in_features, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.resnet(x)\n",
    "\n",
    "# ===== STUDENT MODEL: Smaller CNN (about half parameters) =====\n",
    "class SmallStudent(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super().__init__()\n",
    "        # Smaller version: ~5M parameters vs 11.2M of teacher\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "        \n",
    "        self.fc1 = nn.Linear(64 * 8 * 8, 512)\n",
    "        self.fc2 = nn.Linear(512, num_classes)\n",
    "        self.dropout = nn.Dropout(0.3)  # Same as your models\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2(x), 2))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = x.view(-1, 64 * 8 * 8)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        return self.fc2(x)\n",
    "\n",
    "# ===== KNOWLEDGE DISTILLATION LOSS FUNCTION =====\n",
    "def distillation_loss(student_logits, teacher_logits, labels, temperature=4.0, alpha=0.7):\n",
    "    \"\"\"\n",
    "    Combine soft targets (from teacher) and hard targets (true labels)\n",
    "    \n",
    "    Args:\n",
    "        student_logits: Raw outputs from student model\n",
    "        teacher_logits: Raw outputs from teacher model\n",
    "        labels: True labels\n",
    "        temperature: Softmax temperature (higher = softer distributions)\n",
    "        alpha: Weight for distillation vs hard loss\n",
    "    \n",
    "    Returns:\n",
    "        Combined loss\n",
    "    \"\"\"\n",
    "    # Soft targets from teacher (with temperature)\n",
    "    soft_targets = F.softmax(teacher_logits / temperature, dim=1)\n",
    "    soft_student = F.log_softmax(student_logits / temperature, dim=1)\n",
    "    \n",
    "    # Distillation loss (KL divergence)\n",
    "    kl_loss = F.kl_div(soft_student, soft_targets, reduction='batchmean') * (temperature ** 2)\n",
    "    \n",
    "    # Hard loss (standard cross-entropy)\n",
    "    ce_loss = F.cross_entropy(student_logits, labels)\n",
    "    \n",
    "    # Combined loss\n",
    "    total_loss = alpha * kl_loss + (1 - alpha) * ce_loss\n",
    "    \n",
    "    return total_loss\n",
    "\n",
    "# ===== TRAINING FUNCTIONS =====\n",
    "def train_model(model, train_loader, val_loader, epochs, device, model_name):\n",
    "    \"\"\"Standard training function (same structure as Exercise 1.3)\"\"\"\n",
    "    print(f\"\\nTraining {model_name} for {epochs} epochs...\")\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    best_val_acc = 0\n",
    "    final_train_acc = 0  # Track final train accuracy\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for inputs, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        train_acc = 100 * correct / total\n",
    "        final_train_acc = train_acc  # Update final train accuracy\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += (predicted == labels).sum().item()\n",
    "\n",
    "        val_loss = val_loss / len(val_loader)\n",
    "        val_acc = 100 * val_correct / val_total\n",
    "        \n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "\n",
    "        print(f\"Epoch {epoch+1}: Train Acc: {train_acc:.2f}%, Val Acc: {val_acc:.2f}%\")\n",
    "    \n",
    "    return best_val_acc, final_train_acc\n",
    "\n",
    "def save_teacher_predictions(teacher_model, train_loader, device):\n",
    "    \"\"\"Save teacher predictions to speed up distillation training\"\"\"\n",
    "    print(\"Saving teacher predictions for distillation...\")\n",
    "    \n",
    "    teacher_model.eval()\n",
    "    all_inputs = []\n",
    "    all_labels = []\n",
    "    all_teacher_outputs = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(train_loader, desc=\"Extracting teacher knowledge\"):\n",
    "            inputs = inputs.to(device)\n",
    "            teacher_outputs = teacher_model(inputs)\n",
    "            \n",
    "            all_inputs.append(inputs.cpu())\n",
    "            all_labels.append(labels)\n",
    "            all_teacher_outputs.append(teacher_outputs.cpu())\n",
    "    \n",
    "    # Create new dataset with teacher predictions\n",
    "    inputs_tensor = torch.cat(all_inputs, dim=0)\n",
    "    labels_tensor = torch.cat(all_labels, dim=0)\n",
    "    teacher_outputs_tensor = torch.cat(all_teacher_outputs, dim=0)\n",
    "    \n",
    "    return TensorDataset(inputs_tensor, labels_tensor, teacher_outputs_tensor)\n",
    "\n",
    "def train_student_with_distillation(model, distillation_loader, val_loader, epochs, device, temperature=4.0, alpha=0.7):\n",
    "    \"\"\"Train student using knowledge distillation\"\"\"\n",
    "    print(f\"\\nTraining Student with Knowledge Distillation (T={temperature}, α={alpha})...\")\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    best_val_acc = 0\n",
    "    final_train_acc = 0  # Track final train accuracy\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for inputs, labels, teacher_outputs in tqdm(distillation_loader, desc=f\"KD Epoch {epoch+1}/{epochs}\"):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            teacher_outputs = teacher_outputs.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            student_outputs = model(inputs)\n",
    "            \n",
    "            # Knowledge distillation loss\n",
    "            loss = distillation_loss(student_outputs, teacher_outputs, labels, temperature, alpha)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(student_outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        train_loss = running_loss / len(distillation_loader)\n",
    "        train_acc = 100 * correct / total\n",
    "        final_train_acc = train_acc  # Update final train accuracy\n",
    "\n",
    "        # Validation phase (same as standard training)\n",
    "        model.eval()\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += (predicted == labels).sum().item()\n",
    "\n",
    "        val_acc = 100 * val_correct / val_total\n",
    "        \n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "\n",
    "        print(f\"Epoch {epoch+1}: Train Acc: {train_acc:.2f}%, Val Acc: {val_acc:.2f}%\")\n",
    "    \n",
    "    return best_val_acc, final_train_acc\n",
    "\n",
    "def test_model(model, test_loader, device, model_name):\n",
    "    \"\"\"Test function (same as Exercise 1.3)\"\"\"\n",
    "    print(f\"\\nTesting {model_name}...\")\n",
    "    \n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    test_acc = 100 * correct / total\n",
    "    print(f\"{model_name} Test Accuracy: {test_acc:.2f}%\")\n",
    "    return test_acc\n",
    "\n",
    "# ===== MAIN EXPERIMENT =====\n",
    "def main():\n",
    "    print(\"Starting Knowledge Distillation Experiment on CIFAR-10...\")\n",
    "    \n",
    "    # Same configuration as Exercise 1.3\n",
    "    epochs_teacher = 10  # Teacher training epochs\n",
    "    epochs_student = 15  # Student training epochs (more time to learn)\n",
    "    batch_size = 64\n",
    "    val_size = 5000\n",
    "    \n",
    "    # Same data transforms as Exercise 1.3\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "    test_transform = transforms.Compose([\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "    # Load CIFAR-10 dataset (same as Exercise 1.3)\n",
    "    train_dataset = CIFAR10('./data', train=True, download=True, transform=train_transform)\n",
    "    test_dataset = CIFAR10('./data', train=False, transform=test_transform)\n",
    "    \n",
    "    # Split training set into train/validation\n",
    "    train_size = len(train_dataset) - val_size\n",
    "    train_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "    \n",
    "    print(f\"Dataset sizes - Train: {len(train_dataset)}, Val: {len(val_dataset)}, Test: {len(test_dataset)}\")\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # ===== STEP 1: TRAIN TEACHER MODEL =====\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"STEP 1: TRAINING TEACHER MODEL (ResNetCNN)\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    teacher = ResNetTeacher().to(device)\n",
    "    teacher_params = sum(p.numel() for p in teacher.parameters())\n",
    "    print(f\"Teacher parameters: {teacher_params:,}\")\n",
    "    \n",
    "    teacher_val_acc, teacher_train_acc = train_model(teacher, train_loader, val_loader, epochs_teacher, device, \"Teacher\")\n",
    "    teacher_test_acc = test_model(teacher, test_loader, device, \"Teacher\")\n",
    "    \n",
    "    results['Teacher'] = {\n",
    "        'train_acc': teacher_train_acc,\n",
    "        'val_acc': teacher_val_acc,\n",
    "        'test_acc': teacher_test_acc,\n",
    "        'params': teacher_params\n",
    "    }\n",
    "    \n",
    "    # ===== STEP 2: TRAIN STUDENT BASELINE (HARD LABELS ONLY) =====\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"STEP 2: TRAINING STUDENT BASELINE (HARD LABELS ONLY)\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    student_baseline = SmallStudent().to(device)\n",
    "    student_params = sum(p.numel() for p in student_baseline.parameters())\n",
    "    print(f\"Student parameters: {student_params:,}\")\n",
    "    print(f\"Parameter reduction: {((teacher_params - student_params) / teacher_params * 100):.1f}%\")\n",
    "    \n",
    "    student_baseline_val_acc, student_baseline_train_acc = train_model(student_baseline, train_loader, val_loader, epochs_student, device, \"Student_Baseline\")\n",
    "    student_baseline_test_acc = test_model(student_baseline, test_loader, device, \"Student_Baseline\")\n",
    "    \n",
    "    results['Student_Baseline'] = {\n",
    "        'train_acc': student_baseline_train_acc,\n",
    "        'val_acc': student_baseline_val_acc,\n",
    "        'test_acc': student_baseline_test_acc,\n",
    "        'params': student_params\n",
    "    }\n",
    "    \n",
    "    # Cleanup memory\n",
    "    del student_baseline\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    # ===== STEP 3: KNOWLEDGE DISTILLATION =====\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"STEP 3: KNOWLEDGE DISTILLATION\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Save teacher predictions for efficient training\n",
    "    distillation_dataset = save_teacher_predictions(teacher, train_loader, device)\n",
    "    distillation_loader = DataLoader(distillation_dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    # Try different hyperparameter combinations\n",
    "    hyperparams_to_try = [\n",
    "        {'temperature': 4.0, 'alpha': 0.7},\n",
    "        {'temperature': 6.0, 'alpha': 0.8},\n",
    "        {'temperature': 3.0, 'alpha': 0.6}\n",
    "    ]\n",
    "    \n",
    "    best_kd_test_acc = 0\n",
    "    best_params = None\n",
    "    best_kd_train_acc = 0  # Track best train accuracy for KD\n",
    "    \n",
    "    for i, params in enumerate(hyperparams_to_try):\n",
    "        print(f\"\\n--- Testing hyperparameters {i+1}/3: T={params['temperature']}, α={params['alpha']} ---\")\n",
    "        \n",
    "        student_kd = SmallStudent().to(device)\n",
    "        \n",
    "        kd_val_acc, kd_train_acc = train_student_with_distillation(\n",
    "            student_kd, distillation_loader, val_loader, epochs_student, device,\n",
    "            temperature=params['temperature'], alpha=params['alpha']\n",
    "        )\n",
    "        kd_test_acc = test_model(student_kd, test_loader, device, f\"Student_KD_T{params['temperature']}_A{params['alpha']}\")\n",
    "        \n",
    "        if kd_test_acc > best_kd_test_acc:\n",
    "            best_kd_test_acc = kd_test_acc\n",
    "            best_kd_train_acc = kd_train_acc\n",
    "            best_params = params\n",
    "        \n",
    "        # Cleanup\n",
    "        del student_kd\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "    \n",
    "    results['Student_KD'] = {\n",
    "        'train_acc': best_kd_train_acc,\n",
    "        'val_acc': kd_val_acc,  # Last val_acc from best run\n",
    "        'test_acc': best_kd_test_acc,\n",
    "        'params': student_params\n",
    "    }\n",
    "    \n",
    "    # ===== FINAL RESULTS SUMMARY =====\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"KNOWLEDGE DISTILLATION EXPERIMENT SUMMARY\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"{'Model':<15} {'Train Acc':<11} {'Val Acc':<10} {'Test Acc':<10} {'Parameters':<12} {'Improvement':<12}\")\n",
    "    print(\"-\" * 95)\n",
    "    \n",
    "    baseline_test_acc = results['Student_Baseline']['test_acc']\n",
    "    \n",
    "    for model_name, result in results.items():\n",
    "        if model_name == 'Student_Baseline':\n",
    "            improvement = \"Baseline\"\n",
    "        elif model_name == 'Student_KD':\n",
    "            improvement = f\"+{result['test_acc'] - baseline_test_acc:.2f}%\"\n",
    "        else:\n",
    "            improvement = \"Teacher\"\n",
    "            \n",
    "        print(f\"{model_name:<15} {result['train_acc']:<11.2f} {result['val_acc']:<10.2f} {result['test_acc']:<10.2f} {result['params']:<12,} {improvement:<12}\")\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"DETAILED ANALYSIS:\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    print(f\"RESULTS:\")\n",
    "    print(f\"• Teacher (ResNet):     Train: {results['Teacher']['train_acc']:.2f}%, Test: {results['Teacher']['test_acc']:.2f}% with {results['Teacher']['params']:,} parameters\")\n",
    "    print(f\"• Student Baseline:     Train: {results['Student_Baseline']['train_acc']:.2f}%, Test: {results['Student_Baseline']['test_acc']:.2f}% with {results['Student_Baseline']['params']:,} parameters\")\n",
    "    print(f\"• Student + KD:         Train: {results['Student_KD']['train_acc']:.2f}%, Test: {results['Student_KD']['test_acc']:.2f}% with {results['Student_KD']['params']:,} parameters\")\n",
    "    print(f\"• Best KD parameters:   T={best_params['temperature']}, α={best_params['alpha']}\")\n",
    "    \n",
    "    improvement = best_kd_test_acc - baseline_test_acc\n",
    "    print(f\"\\nKNOWLEDGE DISTILLATION EFFECTIVENESS:\")\n",
    "    print(f\"• Improvement over baseline: {improvement:+.2f}%\")\n",
    "    \n",
    "    # Add overfitting analysis\n",
    "    print(f\"\\nOVERFITTING ANALYSIS:\")\n",
    "    teacher_gap = results['Teacher']['train_acc'] - results['Teacher']['test_acc']\n",
    "    baseline_gap = results['Student_Baseline']['train_acc'] - results['Student_Baseline']['test_acc']\n",
    "    kd_gap = results['Student_KD']['train_acc'] - results['Student_KD']['test_acc']\n",
    "    \n",
    "    print(f\"• Teacher train-test gap:       {teacher_gap:+.2f}%\")\n",
    "    print(f\"• Student baseline gap:         {baseline_gap:+.2f}%\")\n",
    "    print(f\"• Student KD gap:               {kd_gap:+.2f}%\")\n",
    "    \n",
    "    if improvement > 0:\n",
    "        print(f\"SUCCESS: Knowledge distillation improved student performance!\")\n",
    "    else:\n",
    "        print(f\"Knowledge distillation did not improve performance\")\n",
    "    \n",
    "    # Teacher vs Student comparison\n",
    "    teacher_gap = results['Teacher']['test_acc'] - results['Student_KD']['test_acc']\n",
    "    param_efficiency = teacher_gap / (results['Teacher']['params'] - results['Student_KD']['params']) * 1000000\n",
    "    \n",
    "    print(f\"\\nMODEL EFFICIENCY:\")\n",
    "    print(f\"• Teacher-Student gap: {teacher_gap:.2f}%\")\n",
    "    print(f\"• Parameter efficiency: {param_efficiency:.2f} accuracy loss per 1M parameters saved\")\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"Experiment completed successfully!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdda8122-b882-483d-80f3-768c09faa682",
   "metadata": {},
   "source": [
    "### Architetture Sperimentate\n",
    "\n",
    "**Teacher Model: ResNetCNN**\n",
    "- Architettura: ResNet-18 modificata per CIFAR-10  \n",
    "- Parametri: 11,173,962 (~11.2M)  \n",
    "- Performance: **83.96%** test accuracy  \n",
    "- Ruolo: Fornire soft labels ricche di informazione per guidare l’apprendimento dello student\n",
    "\n",
    "**Student Model: SmallStudent**\n",
    "- Architettura: CNN compatta con 3 layer convoluzionali + 2 fully connected  \n",
    "- Parametri: 2,159,114 (~2.2M) → riduzione dell’**80%** rispetto al teacher  \n",
    "- Design: Struttura simile al teacher ma più compatta\n",
    "\n",
    "---\n",
    "\n",
    "### Metodologia Sperimentale\n",
    "\n",
    "**Step 1: Training del Teacher**  \n",
    "- Allenato per 10 epochs con cross-entropy standard  \n",
    "- Test accuracy: **84.63%**\n",
    "\n",
    "**Step 2: Student Baseline**  \n",
    "- Allenato per 15 epochs con **sole hard labels**  \n",
    "- Test accuracy: **74.93%**\n",
    "\n",
    "**Step 3: Knowledge Distillation**  \n",
    "- Combinazione di due loss functions:\n",
    "  - **Soft Loss**: KL divergence tra le predizioni dello student e le soft labels del teacher (temperatura T)\n",
    "  - **Hard Loss**: Cross-entropy tra predizioni dello student e true labels  \n",
    "- Formula: `total_loss = α × soft_loss + (1−α) × hard_loss`\n",
    "\n",
    "---\n",
    "\n",
    "### Ottimizzazione Hyperparameters\n",
    "\n",
    "Combinazioni testate:\n",
    "- `T=4.0`, `α=0.7` (configurazione standard)  \n",
    "- `T=6.0`, `α=0.8` (più soft, maggiore fiducia nel teacher)  \n",
    "- `T=3.0`, `α=0.6` (più conservativa)\n",
    "\n",
    "---\n",
    "\n",
    "### Risultati Ottenuti\n",
    "\n",
    "| Modello           | Train Accuracy | Val Accuracy | Test Accuracy | Parametri | Miglioramento | Train-Test Gap |\n",
    "|-------------------|----------------|--------------|----------------|-----------|---------------|----------------|\n",
    "| Teacher (ResNet)  | 93.69%         | 85.92%       | 84.63%         | 11.2M     | —             | +9.06%         |\n",
    "| Student Baseline  | 85.96%         | 75.88%       | 74.93%         | 2.2M      | baseline      | +11.03%        |\n",
    "| Student + KD      | 86.65%         | 77.30%       | 76.82%         | 2.2M      | **+1.89%**     | +9.83%         |\n",
    "\n",
    "\n",
    "**Performance Ottimali:**\n",
    "- Migliori hyperparameters: `T=4.0`, `α=0.7`\n",
    "- Miglioramento ottenuto: **+1.89%** rispetto al baseline student  \n",
    "- Gap teacher-student: **7.23%**  \n",
    "- Riduzione parametri: **80.7%** (da 11.2M a 2.2M)\n",
    "\n",
    "---\n",
    "\n",
    "## Conclusioni\n",
    "L'esperimento di **knowledge distillation** ha **superato gli obiettivi prefissati**, dimostrando con **eccellenti risultati** che è possibile **trasferire efficacemente la conoscenza** da un modello complesso a uno compatto.\n",
    "\n",
    "Il miglioramento di **+1.89%** in test accuracy, combinato con la **riduzione dell’overfitting** (il train-test gap dello student migliora da **+11.03%** (baseline) a **+9.83%** (con KD), avvicinandosi al comportamento del teacher (**+9.06%**)), conferma la **validità della tecnica** proposta da Hinton et al. e la sua **applicabilità pratica**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8243f811-8227-4c6f-b07f-56e8cd91643a",
   "metadata": {},
   "source": [
    "### Exercise 2.3: *Explain* the predictions of a CNN\n",
    "\n",
    "Use the CNN model you trained in Exercise 1.3 and implement [*Class Activation Maps*](http://cnnlocalization.csail.mit.edu/#:~:text=A%20class%20activation%20map%20for,decision%20made%20by%20the%20CNN.):\n",
    "\n",
    "> B. Zhou, A. Khosla, A. Lapedriza, A. Oliva, and A. Torralba. Learning Deep Features for Discriminative Localization. CVPR'16 (arXiv:1512.04150, 2015).\n",
    "\n",
    "Use your CNN implementation to demonstrate how your trained CNN *attends* to specific image features to recognize *specific* classes. Try your implementation out using a pre-trained ResNet-18 model and some images from the [Imagenette](https://pytorch.org/vision/0.20/generated/torchvision.datasets.Imagenette.html#torchvision.datasets.Imagenette) dataset -- I suggest you start with the low resolution version of images at 160px."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d634a700-56c2-48fd-96e0-4c94d1bd0cfe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Your code here."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
