{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08e91cdb-4693-4632-b7aa-f37eec027131",
   "metadata": {},
   "source": [
    "## Working with Transformers in the HuggingFace Ecosystem\n",
    "\n",
    "In this laboratory exercise we will learn how to work with the HuggingFace ecosystem to adapt models to new tasks. As you will see, much of what is required is *investigation* into the inner-workings of the HuggingFace abstractions. With a little work, a little trial-and-error, it is fairly easy to get a working adaptation pipeline up and running."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e556105-269f-43e3-8933-227269afb9ea",
   "metadata": {},
   "source": [
    "### Exercise 1: Sentiment Analysis (warm up)\n",
    "\n",
    "In this first exercise we will start from a pre-trained BERT transformer and build up a model able to perform text sentiment analysis. Transformers are complex beasts, so we will build up our pipeline in several explorative and incremental steps.\n",
    "\n",
    "#### Exercise 1.1: Dataset Splits and Pre-trained model\n",
    "There are a many sentiment analysis datasets, but we will use one of the smallest ones available: the [Cornell Rotten Tomatoes movie review dataset](cornell-movie-review-data/rotten_tomatoes), which consists of 5,331 positive and 5,331 negative processed sentences from the Rotten Tomatoes movie reviews.\n",
    "\n",
    "**Your first task**: Load the dataset and figure out what splits are available and how to get them. Spend some time exploring the dataset to see how it is organized. Note that we will be using the [HuggingFace Datasets](https://huggingface.co/docs/datasets/en/index) library for downloading, accessing, splitting, and batching data for training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15819af4-e850-412b-ab67-61b9b98e3a90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splits disponibili: ['train', 'validation', 'test']\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, get_dataset_split_names\n",
    "\n",
    "# Vediamo che splits sono disponibili\n",
    "dataset_name = \"rotten_tomatoes\"\n",
    "print(\"Splits disponibili:\", get_dataset_split_names(dataset_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aebef659-bf3e-4d02-8176-b880ac992464",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e916ce852edb4355a021648557735f8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train.parquet:   0%|          | 0.00/699k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fd53a5ecc524096b519179c8740b388",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation.parquet:   0%|          | 0.00/90.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff88f446ecf34348807b9c1e3060a49d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test.parquet:   0%|          | 0.00/92.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "357ad784760f4bac9f656a4a3e10fa5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/8530 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "492836b1a61a45d3b61bc4605b178c9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/1066 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d2c80b4116040d19561925defff7ca4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/1066 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Struttura del dataset:\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 8530\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 1066\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 1066\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Carichiamo il dataset completo\n",
    "dataset = load_dataset(dataset_name)\n",
    "print(\"\\nStruttura del dataset:\")\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0d6e9e4-83c1-471f-81b4-8d946f80e503",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Split: train ---\n",
      "Numero di esempi: 8530\n",
      "Caratteristiche: {'text': Value(dtype='string', id=None), 'label': ClassLabel(names=['neg', 'pos'], id=None)}\n",
      "\n",
      "Primi 3 esempi del train:\n",
      "Esempio 1:\n",
      "  Testo: the rock is destined to be the 21st century's new \" conan \" and that he's going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .\n",
      "  Label: 1\n",
      "\n",
      "Esempio 2:\n",
      "  Testo: the gorgeously elaborate continuation of \" the lord of the rings \" trilogy is so huge that a column of words cannot adequately describe co-writer/director peter jackson's expanded vision of j . r . r . tolkien's middle-earth .\n",
      "  Label: 1\n",
      "\n",
      "Esempio 3:\n",
      "  Testo: effective but too-tepid biopic\n",
      "  Label: 1\n",
      "\n",
      "\n",
      "--- Split: validation ---\n",
      "Numero di esempi: 1066\n",
      "Caratteristiche: {'text': Value(dtype='string', id=None), 'label': ClassLabel(names=['neg', 'pos'], id=None)}\n",
      "\n",
      "Primi 3 esempi del validation:\n",
      "Esempio 1:\n",
      "  Testo: compassionately explores the seemingly irreconcilable situation between conservative christian parents and their estranged gay and lesbian children .\n",
      "  Label: 1\n",
      "\n",
      "Esempio 2:\n",
      "  Testo: the soundtrack alone is worth the price of admission .\n",
      "  Label: 1\n",
      "\n",
      "Esempio 3:\n",
      "  Testo: rodriguez does a splendid job of racial profiling hollywood style--casting excellent latin actors of all ages--a trend long overdue .\n",
      "  Label: 1\n",
      "\n",
      "\n",
      "--- Split: test ---\n",
      "Numero di esempi: 1066\n",
      "Caratteristiche: {'text': Value(dtype='string', id=None), 'label': ClassLabel(names=['neg', 'pos'], id=None)}\n",
      "\n",
      "Primi 3 esempi del test:\n",
      "Esempio 1:\n",
      "  Testo: lovingly photographed in the manner of a golden book sprung to life , stuart little 2 manages sweetness largely without stickiness .\n",
      "  Label: 1\n",
      "\n",
      "Esempio 2:\n",
      "  Testo: consistently clever and suspenseful .\n",
      "  Label: 1\n",
      "\n",
      "Esempio 3:\n",
      "  Testo: it's like a \" big chill \" reunion of the baader-meinhof gang , only these guys are more harmless pranksters than political activists .\n",
      "  Label: 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Esploriamo ogni split\n",
    "for split_name in dataset.keys():\n",
    "    split_data = dataset[split_name]\n",
    "    print(f\"\\n--- Split: {split_name} ---\")\n",
    "    print(f\"Numero di esempi: {len(split_data)}\")\n",
    "    print(f\"Caratteristiche: {split_data.features}\")\n",
    "    \n",
    "    # Mostriamo alcuni esempi\n",
    "    print(f\"\\nPrimi 3 esempi del {split_name}:\")\n",
    "    for i in range(min(3, len(split_data))):\n",
    "        example = split_data[i]\n",
    "        print(f\"Esempio {i+1}:\")\n",
    "        print(f\"  Testo: {example['text']}\")\n",
    "        print(f\"  Label: {example['label']}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d264fca-d764-4dd6-b41f-9a9317fcd8ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classi uniche: {0, 1}\n",
      "Label 0: 4265 esempi\n",
      "Label 1: 4265 esempi\n"
     ]
    }
   ],
   "source": [
    "# Controlliamo la distribuzione delle classi\n",
    "if 'train' in dataset:\n",
    "    train_labels = dataset['train']['label']\n",
    "    unique_labels = set(train_labels)\n",
    "    print(f\"\\nClassi uniche: {unique_labels}\")\n",
    "    \n",
    "    for label in unique_labels:\n",
    "        count = train_labels.count(label)\n",
    "        print(f\"Label {label}: {count} esempi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae2b79f-433e-444d-8a58-4c8d6b2c2505",
   "metadata": {},
   "source": [
    "## Recap Esplorazione Dataset Rotten Tomatoes\n",
    "Il dataset **Cornell Rotten Tomatoes** è perfettamente organizzato per un task di **sentiment analysis**.\n",
    "\n",
    "#### Splits Disponibili:\n",
    "- **Train**: 8,530 esempi (per l'addestramento)  \n",
    "- **Validation**: 1,066 esempi (per la validazione durante il training)  \n",
    "- **Test**: 1,066 esempi (per la valutazione finale)\n",
    "\n",
    "#### Caratteristiche dei Dati:\n",
    "- **Text**: recensioni di film come stringhe di testo  \n",
    "- **Label**: etichette di sentimento con encoding:\n",
    "  - `0 = neg` (negativo)  \n",
    "  - `1 = pos` (positivo)\n",
    "\n",
    "### Osservazioni \n",
    "\n",
    "#### Bilanciamento Perfetto\n",
    "Il training set è **perfettamente bilanciato**:\n",
    "- 4,265 esempi **negativi** (`label 0`)  \n",
    "- 4,265 esempi **positivi** (`label 1`)\n",
    "\n",
    "> Questo è **ideale per evitare bias** verso una classe specifica.\n",
    "\n",
    "#### Qualità del Testo\n",
    "Le recensioni sono:\n",
    "- Di **lunghezza variabile** (da brevi a articolate)  \n",
    "- Espresse in **linguaggio naturale e descrittivo**  \n",
    "- Riferite a **diversi generi cinematografici**\n",
    "\n",
    "#### Pronto per BERT\n",
    "Il dataset è **già pronto** per essere processato da:\n",
    "- **Tokenizer di BERT** (per convertire testo in token)  \n",
    "- **Pipeline di classificazione** (binary classification)  \n",
    "- **Fine-tuning** di un modello pre-addestrato\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a27f50a0-0af4-4bdd-9a60-bab26f126c14",
   "metadata": {},
   "source": [
    "#### Exercise 1.2: A Pre-trained BERT and Tokenizer\n",
    "\n",
    "The model we will use is a *very* small BERT transformer called [Distilbert](https://huggingface.co/distilbert/distilbert-base-uncased) this model was trained (using self-supervised learning) on the same corpus as BERT but using the full BERT base model as a *teacher*.\n",
    "\n",
    "**Your next task**: Load the Distilbert model and corresponding tokenizer. Use the tokenizer on a few samples from the dataset and pass the tokens through the model to see what outputs are provided. I suggest you use the [`AutoModel`](https://huggingface.co/transformers/v3.0.2/model_doc/auto.html) class (and the `from_pretrained()` method) to load the model and `AutoTokenizer` to load the tokenizer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3eefc651-8551-44c2-8340-37d64660fc58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer: <class 'transformers.models.distilbert.tokenization_distilbert_fast.DistilBertTokenizerFast'>\n",
      "Modello: <class 'transformers.models.distilbert.modeling_distilbert.DistilBertModel'>\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# Carico il modello DistilBERT e il tokenizer\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "print(f\"Tokenizer: {type(tokenizer)}\")\n",
    "print(f\"Modello: {type(model)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "38193a94-d9e5-4caf-8917-239ddee9c8a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample texts:\n",
      "1. the rock is destined to be the 21st century's new \" conan \" and that he's going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .\n",
      "2. the gorgeously elaborate continuation of \" the lord of the rings \" trilogy is so huge that a column of words cannot adequately describe co-writer/director peter jackson's expanded vision of j . r . r . tolkien's middle-earth .\n",
      "3. chicago is sophisticated , brash , sardonic , completely joyful in its execution .\n"
     ]
    }
   ],
   "source": [
    "# Prendiamo alcune frasi di esempio dal dataset\n",
    "sample_texts = [\n",
    "    dataset[\"train\"][0][\"text\"],  \n",
    "    dataset[\"train\"][1][\"text\"],  \n",
    "    dataset[\"train\"][100][\"text\"] \n",
    "]\n",
    "\n",
    "# Stampa degli esempi\n",
    "print(\"Sample texts:\")\n",
    "for i, text in enumerate(sample_texts):\n",
    "    print(f\"{i+1}. {text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1f71301c-ee40-423e-b3cf-f3af1790b71a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TOKENIZER + MODEL OUTPUT SU CAMPIONI DEL DATASET\n",
      "================================================================================\n",
      "\n",
      "--- ESEMPIO 1 ---\n",
      "Testo: the rock is destined to be the 21st century's new \" conan \" and that he's going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .\n",
      "Label: 1\n",
      "Token (45): ['the', 'rock', 'is', 'destined', 'to', 'be', 'the', '21st', 'century', \"'\"]...\n",
      "Input shape: torch.Size([1, 47])\n",
      "Output keys: ['last_hidden_state']\n",
      "Last hidden state shape: torch.Size([1, 47, 768])\n",
      "Hidden size (dimensioni features): 768\n",
      "[CLS] representation shape: torch.Size([768])\n",
      "[CLS] primi 5 valori: tensor([-0.0332, -0.0168,  0.0194, -0.0257, -0.1380])\n",
      "\n",
      "--- ESEMPIO 2 ---\n",
      "Testo: the gorgeously elaborate continuation of \" the lord of the rings \" trilogy is so huge that a column of words cannot adequately describe co-writer/director peter jackson's expanded vision of j . r . r . tolkien's middle-earth .\n",
      "Label: 1\n",
      "Token (50): ['the', 'gorgeous', '##ly', 'elaborate', 'continuation', 'of', '\"', 'the', 'lord', 'of']...\n",
      "Input shape: torch.Size([1, 52])\n",
      "Output keys: ['last_hidden_state']\n",
      "Last hidden state shape: torch.Size([1, 52, 768])\n",
      "Hidden size (dimensioni features): 768\n",
      "[CLS] representation shape: torch.Size([768])\n",
      "[CLS] primi 5 valori: tensor([-0.2062, -0.0490, -0.4036, -0.3747, -0.2676])\n",
      "\n",
      "--- ESEMPIO 3 ---\n",
      "Testo: chicago is sophisticated , brash , sardonic , completely joyful in its execution .\n",
      "Label: 1\n",
      "Token (18): ['chicago', 'is', 'sophisticated', ',', 'bra', '##sh', ',', 'sar', '##don', '##ic']...\n",
      "Input shape: torch.Size([1, 20])\n",
      "Output keys: ['last_hidden_state']\n",
      "Last hidden state shape: torch.Size([1, 20, 768])\n",
      "Hidden size (dimensioni features): 768\n",
      "[CLS] representation shape: torch.Size([768])\n",
      "[CLS] primi 5 valori: tensor([-0.0839, -0.1955, -0.0903, -0.1041, -0.0582])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TOKENIZER + MODEL OUTPUT SU CAMPIONI DEL DATASET\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# 3. Per ogni esempio: tokenizza e passa attraverso il modello\n",
    "for i, text in enumerate(sample_texts):\n",
    "    print(f\"\\n--- ESEMPIO {i+1} ---\")\n",
    "    print(f\"Testo: {text}\")\n",
    "    print(f\"Label: {dataset['train'][i if i < 2 else 100]['label']}\")\n",
    "    \n",
    "    # Tokenizzazione\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    print(f\"Token ({len(tokens)}): {tokens[:10]}...\")  # Primi 10 token\n",
    "    \n",
    "    # Preparazione input per il modello\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    print(f\"Input shape: {inputs['input_ids'].shape}\")\n",
    "    \n",
    "    # Passaggio attraverso il modello\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    print(f\"Output keys: {list(outputs.keys())}\")\n",
    "    print(f\"Last hidden state shape: {outputs.last_hidden_state.shape}\")\n",
    "    \n",
    "    # Il last_hidden_state contiene la rappresentazione di ogni token\n",
    "    # Shape: (batch_size, sequence_length, hidden_size)\n",
    "    print(f\"Hidden size (dimensioni features): {outputs.last_hidden_state.shape[-1]}\")\n",
    "    \n",
    "    # Rappresentazione del token [CLS] (primo token, usato per classificazione)\n",
    "    cls_representation = outputs.last_hidden_state[0, 0, :]  # [0,0,:] = primo batch, primo token\n",
    "    print(f\"[CLS] representation shape: {cls_representation.shape}\")\n",
    "    print(f\"[CLS] primi 5 valori: {cls_representation[:5]}\")\n",
    "    \n",
    "    if hasattr(outputs, 'pooler_output') and outputs.pooler_output is not None:\n",
    "        print(f\"Pooler output shape: {outputs.pooler_output.shape}\")\n",
    "        print(f\"Pooler primi 5 valori: {outputs.pooler_output[0, :5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2147c52d-503a-41f9-93fe-22a33094199c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "INFORMAZIONI SUL MODELLO\n",
      "================================================================================\n",
      "Parametri totali: 66,362,880\n",
      "Vocab size: 30,522\n",
      "Hidden size: 768\n",
      "Num layers: 6\n",
      "Num attention heads: 12\n",
      "Max length: 512\n"
     ]
    }
   ],
   "source": [
    "# 4. Informazioni sul modello\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"INFORMAZIONI SUL MODELLO\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Parametri totali: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"Vocab size: {tokenizer.vocab_size:,}\")\n",
    "print(f\"Hidden size: {model.config.hidden_size}\")\n",
    "print(f\"Num layers: {model.config.n_layers}\")\n",
    "print(f\"Num attention heads: {model.config.n_heads}\")\n",
    "print(f\"Max length: {tokenizer.model_max_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7aebc6d0-8432-40dd-9b40-d8bf1793d50a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "CONFRONTO LUNGHEZZE DIVERSE\n",
      "================================================================================\n",
      "CORTO: 3 token → output shape torch.Size([1, 5, 768])\n",
      "LUNGO: 45 token → output shape torch.Size([1, 47, 768])\n"
     ]
    }
   ],
   "source": [
    "# 5. Confronto lunghezze diverse\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CONFRONTO LUNGHEZZE DIVERSE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "short_text = \"Great movie!\"\n",
    "long_text = sample_texts[0]  # Testo più lungo\n",
    "\n",
    "for label, text in [(\"CORTO\", short_text), (\"LUNGO\", long_text)]:\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    print(f\"{label}: {len(tokenizer.tokenize(text))} token → output shape {outputs.last_hidden_state.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38ce182-b785-42e7-9ae3-a0bca84849c4",
   "metadata": {},
   "source": [
    "### Considerazioni su DistilBERT\n",
    "- **DistilBERT**: carica 66M parametri, 6 layer, vocabolario di 30k token.\n",
    "- **Tokenizzazione intelligente**: spezza parole complesse in sub-token (es. \"gorgeously\" → \"gorgeous\" + \"##ly\") e aggiunge automaticamente token speciali [CLS] e [SEP].\n",
    "- **Output del modello**: produce rappresentazioni di 768 dimensioni per ogni token. Il token [CLS] (primo della sequenza) cattura il significato globale della frase ed è quello che verrà usato per classificare il sentiment.\n",
    "- **Gestione dinamica**: il modello si adatta automaticamente a frasi di lunghezza diversa senza bisogno di padding.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec0f6fd4-b80b-466d-b4ff-9aac0492c062",
   "metadata": {},
   "source": [
    "#### Exercise 1.3: A Stable Baseline\n",
    "\n",
    "In this exercise I want you to:\n",
    "1. Use Distilbert as a *feature extractor* to extract representations of the text strings from the dataset splits;\n",
    "2. Train a classifier (your choice, by an SVM from Scikit-learn is an easy choice).\n",
    "3. Evaluate performance on the validation and test splits.\n",
    "\n",
    "These results are our *stable baseline* -- the **starting** point on which we will (hopefully) improve in the next exercise.\n",
    "\n",
    "**Hint**: There are a number of ways to implement the feature extractor, but probably the best is to use a [feature extraction `pipeline`](https://huggingface.co/tasks/feature-extraction). You will need to interpret the output of the pipeline and extract only the `[CLS]` token from the *last* transformer layer. *How can you figure out which output that is?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ebb961ab-aa24-4d5c-86a1-c9b4237b40fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Creazione della pipeline di feature extraction\n",
    "feature_extractor = pipeline(\n",
    "    \"feature-extraction\",\n",
    "    model=\"distilbert-base-uncased\", \n",
    "    tokenizer=\"distilbert-base-uncased\",\n",
    "    return_tensors=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5368bed7-34c6-4c01-ba4e-010491bb4fb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: torch.Size([1, 47, 768])\n",
      "CLS token shape: torch.Size([768])\n",
      "CLS token first 5 values: tensor([-0.0332, -0.0168,  0.0194, -0.0257, -0.1380])\n"
     ]
    }
   ],
   "source": [
    "# Vedimao cosa restituisce la pipeline\n",
    "text = dataset[\"train\"][0][\"text\"]\n",
    "features = feature_extractor(text)\n",
    "print(f\"Shape: {features.shape}\")  # [1, 47, 768]\n",
    "print(f\"CLS token shape: {features[0, 0, :].shape}\")  # [768]\n",
    "print(f\"CLS token first 5 values: {features[0, 0, :5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd799c1-fc0c-49a5-8479-7a5ff205d567",
   "metadata": {},
   "source": [
    "torch.Size([1, 47, 768]) --> 1 = batch size, 47 = sequence length (numero di token), 768 = hidden size\n",
    "Il token [CLS] è il primo token della sequenza."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e0b88a5e-e66d-48ae-bb6b-5ba7aec51646",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type: <class 'list'>\n",
      "Length: 2\n",
      "First element shape: torch.Size([1, 4, 768])\n",
      "Second element shape: torch.Size([1, 4, 768])\n"
     ]
    }
   ],
   "source": [
    "batch_texts = [\"Hello world\", \"Another text\"]\n",
    "batch_features = feature_extractor(batch_texts)\n",
    "print(f\"Type: {type(batch_features)}\")\n",
    "print(f\"Length: {len(batch_features)}\")\n",
    "print(f\"First element shape: {batch_features[0].shape}\")\n",
    "print(f\"Second element shape: {batch_features[1].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8a41cf75-8fc2-4f4e-9db6-5941b78015a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Funzione per estrarre features [CLS] - con progress bar\n",
    "def extract_cls(texts, batch_size):\n",
    "    all_features = []\n",
    "    total_batches = (len(texts) + batch_size - 1) // batch_size\n",
    "    \n",
    "    print(f\"Processing {len(texts)} texts in {total_batches} batches\")\n",
    "    \n",
    "    with tqdm(total=total_batches, desc=\"Extracting features\", unit=\"batch\") as pbar:\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch_texts = texts[i:i+batch_size]\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                batch_features = feature_extractor(batch_texts)\n",
    "            \n",
    "            for text_features in batch_features:\n",
    "                cls_token = text_features[0, 0, :]\n",
    "                all_features.append(cls_token.cpu().numpy())\n",
    "            \n",
    "            pbar.update(1)\n",
    "    \n",
    "    features_array = np.vstack(all_features)\n",
    "    print(f\"Features extracted: {features_array.shape}\")\n",
    "    return features_array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b1d001a2-7c59-4311-a7b5-dd80a109059f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estrazione features dal training set...\n",
      "Processing 8530 texts in 134 batches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 134/134 [00:35<00:00,  3.83batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features extracted: (8530, 768)\n",
      "Estrazione features dal validation set...\n",
      "Processing 1066 texts in 17 batches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:04<00:00,  3.78batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features extracted: (1066, 768)\n",
      "Estrazione features dal test set...\n",
      "Processing 1066 texts in 17 batches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:04<00:00,  3.92batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features extracted: (1066, 768)\n",
      "Training SVM classifier...\n",
      "Valutazione su validation set:\n",
      "Validation Accuracy: 0.8143\n",
      "Valutazione su test set:\n",
      "Test Accuracy: 0.7946\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.77      0.84      0.80       533\n",
      "    Positive       0.82      0.75      0.79       533\n",
      "\n",
      "    accuracy                           0.79      1066\n",
      "   macro avg       0.80      0.79      0.79      1066\n",
      "weighted avg       0.80      0.79      0.79      1066\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "batch_size=64\n",
    "\n",
    "# Estrazione features dai dataset splits\n",
    "print(\"Estrazione features dal training set...\")\n",
    "train_features = extract_cls(train_texts, batch_size)\n",
    "\n",
    "print(\"Estrazione features dal validation set...\")\n",
    "val_features = extract_cls(val_texts, batch_size)\n",
    "\n",
    "print(\"Estrazione features dal test set...\")\n",
    "test_features = extract_cls(test_texts, batch_size)\n",
    "\n",
    "# Ora procedi con SVM\n",
    "print(\"Training SVM classifier...\")\n",
    "svm_classifier = SVC(kernel='rbf', C=1.0, random_state=42)\n",
    "svm_classifier.fit(train_features, train_labels)\n",
    "\n",
    "print(\"Valutazione su validation set:\")\n",
    "val_predictions = svm_classifier.predict(val_features)\n",
    "val_accuracy = accuracy_score(val_labels, val_predictions)\n",
    "print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "print(\"Valutazione su test set:\")\n",
    "test_predictions = svm_classifier.predict(test_features)\n",
    "test_accuracy = accuracy_score(test_labels, test_predictions)\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "print(classification_report(test_labels, test_predictions, target_names=['Negative', 'Positive']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ec19c1-0d43-40b8-841b-32e26e3891ec",
   "metadata": {},
   "source": [
    "### Risultati\n",
    "\n",
    "- **Accuratezza in validazione**: **81.43%**  \n",
    "- **Accuratezza sul test set**: **79.46%**  \n",
    "- **Performance bilanciate**: precision e recall simili per entrambe le classi (*negative* / *positive*)\n",
    "\n",
    "#### DistilBERT come Feature Extractor\n",
    "- L’utilizzo del token **[CLS]** di **DistilBERT** (768 dimensioni) fornisce **rappresentazioni semantiche ricche**\n",
    "- Non è necessario il fine-tuning del modello transformer\n",
    "\n",
    "#### Considerazioni\n",
    "- L’approccio **frozen features + SVM** è **significativamente più veloce e leggero**\n",
    "- Richiede solo **l’estrazione una tantum** delle features → ideale per risorse limitate\n",
    "- Un’**accuratezza del 79.5%** sul test set rappresenta un punto di partenza **robusto**\n",
    "- Conferma l’efficacia delle **rappresentazioni pre-addestrate** per il sentiment analysis\n",
    "- Si dimostra che è possibile ottenere **performance competitive** con **risorse computazionali contenute**\n",
    "- Questi risultati costituiscono un punto di riferimento su cui confrontare eventuali miglioramenti ottenuti tramite fine-tuning o con architetture diverse."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37141d1b-935b-425c-804c-b9b487853791",
   "metadata": {},
   "source": [
    "-----\n",
    "### Exercise 2: Fine-tuning Distilbert"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a53f64-0238-42f2-bc20-86e51c77d2e5",
   "metadata": {},
   "source": [
    "In this exercise we will fine-tune the Distilbert model to (hopefully) improve sentiment analysis performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3392b1ed-597b-4a92-90fc-10eb11eac515",
   "metadata": {},
   "source": [
    "#### Exercise 2.1: Token Preprocessing\n",
    "\n",
    "The first thing we need to do is *tokenize* our dataset splits. Our current datasets return a dictionary with *strings*, but we want *input token ids* (i.e. the output of the tokenizer). This is easy enough to do my hand, but the HugginFace `Dataset` class provides convenient, efficient, and *lazy* methods. See the documentation for [`Dataset.map`](https://huggingface.co/docs/datasets/v3.5.0/en/package_reference/main_classes#datasets.Dataset.map).\n",
    "\n",
    "**Tip**: Verify that your new datasets are returning for every element: `text`, `label`, `intput_ids`, and `attention_mask`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9e6e2a95-5b08-4f81-b824-59a0fb3404e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funzione di tokenizzazione - Tokenizza i testi e restituisce input_ids e attention_mask\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,        # Tronca se troppo lungo\n",
    "        padding=True,           # Padding per uniformare lunghezze\n",
    "        max_length=512,         # Lunghezza massima\n",
    "        return_tensors=None     # Restituisce liste, non tensori (liste piu compatibili con HugginFace Datasets)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d320f1ba-bffa-4bfc-b223-bcda078f5ab5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizzazione del training set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30cc61475fb34f0cba05deb4890596ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train:   0%|          | 0/8530 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizzazione del validation set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1aec6fc55cbb49129ddf03c203fbb63d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing validation:   0%|          | 0/1066 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizzazione del test set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fdb5095673f4c4e87c370cccaf71741",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing test:   0%|          | 0/1066 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tokenizzazione ai dataset\n",
    "print(\"Tokenizzazione del training set...\")\n",
    "train_dataset_tokenized = dataset[\"train\"].map(\n",
    "    tokenize_function,\n",
    "    batched=True,              # Processa in batch per efficienza\n",
    "    desc=\"Tokenizing train\"\n",
    ")\n",
    "\n",
    "print(\"Tokenizzazione del validation set...\")\n",
    "val_dataset_tokenized = dataset[\"validation\"].map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    desc=\"Tokenizing validation\"\n",
    ")\n",
    "\n",
    "print(\"Tokenizzazione del test set...\")\n",
    "test_dataset_tokenized = dataset[\"test\"].map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    desc=\"Tokenizing test\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "651ed364-5c85-4e2d-803a-0be5c045130c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "VERIFICA TOKENIZZAZIONE\n",
      "==================================================\n",
      "Chiavi disponibili: ['text', 'label', 'input_ids', 'attention_mask']\n",
      "text: the rock is destined to be the 21st century's new \" conan \" and that he's going to make a splash eve...\n",
      "label: 1\n",
      "input_ids length: 70\n",
      "attention_mask length: 70\n",
      "input_ids (primi 10): [101, 1996, 2600, 2003, 16036, 2000, 2022, 1996, 7398, 2301]\n",
      "attention_mask (primi 10): [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "# Verifica dei risultati\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"VERIFICA TOKENIZZAZIONE\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Controllo un esempio del training set\n",
    "sample = train_dataset_tokenized[0]\n",
    "print(\"Chiavi disponibili:\", list(sample.keys()))\n",
    "print(f\"text: {sample['text'][:100]}...\")\n",
    "print(f\"label: {sample['label']}\")\n",
    "print(f\"input_ids length: {len(sample['input_ids'])}\")\n",
    "print(f\"attention_mask length: {len(sample['attention_mask'])}\")\n",
    "print(f\"input_ids (primi 10): {sample['input_ids'][:10]}\")\n",
    "print(f\"attention_mask (primi 10): {sample['attention_mask'][:10]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "13145321-b237-4064-a8d0-3149c8fbac51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train dataset:\n",
      "  Columns: ['text', 'label', 'input_ids', 'attention_mask']\n",
      "  Size: 8530\n",
      "All required columns present!\n",
      "\n",
      "validation dataset:\n",
      "  Columns: ['text', 'label', 'input_ids', 'attention_mask']\n",
      "  Size: 1066\n",
      "All required columns present!\n",
      "\n",
      "test dataset:\n",
      "  Columns: ['text', 'label', 'input_ids', 'attention_mask']\n",
      "  Size: 1066\n",
      "All required columns present!\n"
     ]
    }
   ],
   "source": [
    "# Verifica che tutti i dataset abbiano le colonne giuste\n",
    "required_columns = ['text', 'label', 'input_ids', 'attention_mask']\n",
    "\n",
    "for split_name, split_data in [\n",
    "    (\"train\", train_dataset_tokenized),\n",
    "    (\"validation\", val_dataset_tokenized), \n",
    "    (\"test\", test_dataset_tokenized)\n",
    "]:\n",
    "    print(f\"\\n{split_name} dataset:\")\n",
    "    print(f\"  Columns: {split_data.column_names}\")\n",
    "    print(f\"  Size: {len(split_data)}\")\n",
    "    \n",
    "    # Verifica che abbia tutte le colonne necessarie\n",
    "    missing_cols = [col for col in required_columns if col not in split_data.column_names]\n",
    "    if missing_cols:\n",
    "        print(f\"Missing columns: {missing_cols}\")\n",
    "    else:\n",
    "        print(f\"All required columns present!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "172025a3-94b1-4b3f-9b19-47b5e351a12e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "STATISTICHE TOKENIZZAZIONE\n",
      "==================================================\n",
      "\n",
      "Training set:\n",
      "  Min length: 63\n",
      "  Max length: 78\n",
      "  Mean length: 67.3\n",
      "  Examples with max length (512): 0\n",
      "\n",
      "Validation set:\n",
      "  Min length: 54\n",
      "  Max length: 72\n",
      "  Mean length: 70.9\n",
      "  Examples with max length (512): 0\n",
      "\n",
      "Test set:\n",
      "  Min length: 52\n",
      "  Max length: 67\n",
      "  Mean length: 66.1\n",
      "  Examples with max length (512): 0\n"
     ]
    }
   ],
   "source": [
    "# Stampa delle statistiche sui token\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"STATISTICHE TOKENIZZAZIONE\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "def compute_token_stats(dataset_split, split_name):\n",
    "    token_lengths = [len(example['input_ids']) for example in dataset_split]\n",
    "    \n",
    "    print(f\"\\n{split_name} set:\")\n",
    "    print(f\"  Min length: {min(token_lengths)}\")\n",
    "    print(f\"  Max length: {max(token_lengths)}\")\n",
    "    print(f\"  Mean length: {sum(token_lengths) / len(token_lengths):.1f}\")\n",
    "    print(f\"  Examples with max length (512): {sum(1 for l in token_lengths if l == 512)}\")\n",
    "\n",
    "compute_token_stats(train_dataset_tokenized, \"Training\")\n",
    "compute_token_stats(val_dataset_tokenized, \"Validation\")\n",
    "compute_token_stats(test_dataset_tokenized, \"Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f1155083-44cc-443a-b134-cb4c74c8d57b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "TEST DECODIFICA\n",
      "==================================================\n",
      "Original: the rock is destined to be the 21st century's new \" conan \" and that he's going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .\n",
      "--------\n",
      "Decoded:  the rock is destined to be the 21st century ' s new \" conan \" and that he ' s going to make a splash even greater than arnold schwarzenegger, jean - claud van damme or steven segal.\n"
     ]
    }
   ],
   "source": [
    "# Test decodifica (verifica reversibilità)\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"TEST DECODIFICA\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "sample_tokens = train_dataset_tokenized[0]['input_ids']\n",
    "decoded_text = tokenizer.decode(sample_tokens, skip_special_tokens=True)\n",
    "original_text = train_dataset_tokenized[0]['text']\n",
    "\n",
    "print(f\"Original: {original_text}\")\n",
    "print(\"--------\")\n",
    "print(f\"Decoded:  {decoded_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a80de2-83c9-4c12-af4e-34babe23ffd1",
   "metadata": {},
   "source": [
    "#### Exercise 2.2: Setting up the Model to be Fine-tuned\n",
    "\n",
    "In this exercise we need to prepare the base Distilbert model for fine-tuning for a *sequence classification task*. This means, at the very least, appending a new, randomly-initialized classification head connected to the `[CLS]` token of the last transformer layer. Luckily, HuggingFace already provides an `AutoModel` for just this type of instantiation: [`AutoModelForSequenceClassification`](https://huggingface.co/transformers/v3.0.2/model_doc/auto.html#automodelforsequenceclassification). You will want you instantiate one of these for fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "a6327d73-3b71-478a-9932-bb062a650c57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "GPU: NVIDIA GeForce RTX 3060\n",
      "GPU Memory: 12.9GB\n",
      "\n",
      "Caricamento modello per sequence classification...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modello caricato: DistilBertForSequenceClassification\n",
      "Numero di parametri: 66,955,010\n",
      "Device del modello: cuda:0\n",
      "\n",
      "============================================================\n",
      "ARCHITETTURA DEL MODELLO\n",
      "============================================================\n",
      "DistilBertForSequenceClassification(\n",
      "  (distilbert): DistilBertModel(\n",
      "    (embeddings): Embeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (transformer): Transformer(\n",
      "      (layer): ModuleList(\n",
      "        (0-5): 6 x TransformerBlock(\n",
      "          (attention): DistilBertSdpaAttention(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (ffn): FFN(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (activation): GELUActivation()\n",
      "          )\n",
      "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n",
      "\n",
      "============================================================\n",
      "DETTAGLI ARCHITETTURA\n",
      "============================================================\n",
      "distilbert: DistilBertModel\n",
      "  Hidden size: 768\n",
      "  Layers: 6\n",
      "  Attention heads: 12\n",
      "pre_classifier: Linear\n",
      "classifier: Linear\n",
      "dropout: Dropout\n",
      "\n",
      "Classifier head:\n",
      "  Input features: 768\n",
      "  Output features: 2\n",
      "  Parametri classifier: 1,538\n",
      "\n",
      "Parametri trainable: 66,955,010\n",
      "Parametri totali: 66,955,010\n",
      "Percentuale trainable: 100.0%\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    AutoModelForSequenceClassification, \n",
    "    AutoTokenizer,\n",
    "    DataCollatorWithPadding,\n",
    "    Trainer, \n",
    "    TrainingArguments\n",
    ")\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import numpy as np\n",
    "\n",
    "# 1. Device setup\n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f}GB\")\n",
    "\n",
    "# 2. Carica il modello per sequence classification\n",
    "print(\"\\nCaricamento modello per sequence classification...\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\",\n",
    "    num_labels=2,  # Binary classification (positive/negative)\n",
    "    id2label={0: \"NEGATIVE\", 1: \"POSITIVE\"},\n",
    "    label2id={\"NEGATIVE\": 0, \"POSITIVE\": 1}\n",
    ")\n",
    "\n",
    "# Sposta il modello su GPU\n",
    "model = model.to(device)\n",
    "\n",
    "# Tokenizer (se non già caricato)\n",
    "if 'tokenizer' not in globals():\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "print(f\"Modello caricato: {model.__class__.__name__}\")\n",
    "print(f\"Numero di parametri: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"Device del modello: {next(model.parameters()).device}\")\n",
    "\n",
    "# 3. Stampa architettura del modello\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ARCHITETTURA DEL MODELLO\")\n",
    "print(\"=\"*60)\n",
    "print(model)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DETTAGLI ARCHITETTURA\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Mostra i layer principali\n",
    "for name, module in model.named_children():\n",
    "    print(f\"{name}: {module.__class__.__name__}\")\n",
    "    if hasattr(module, 'config'):\n",
    "        if hasattr(module.config, 'hidden_size'):\n",
    "            print(f\"  Hidden size: {module.config.hidden_size}\")\n",
    "        if hasattr(module.config, 'n_layers'):\n",
    "            print(f\"  Layers: {module.config.n_layers}\")\n",
    "        if hasattr(module.config, 'n_heads'):\n",
    "            print(f\"  Attention heads: {module.config.n_heads}\")\n",
    "\n",
    "# Dettagli del classifier head\n",
    "if hasattr(model, 'classifier'):\n",
    "    print(f\"\\nClassifier head:\")\n",
    "    print(f\"  Input features: {model.classifier.in_features}\")\n",
    "    print(f\"  Output features: {model.classifier.out_features}\")\n",
    "    print(f\"  Parametri classifier: {sum(p.numel() for p in model.classifier.parameters()):,}\")\n",
    "\n",
    "# Parametri trainable vs frozen\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"\\nParametri trainable: {trainable_params:,}\")\n",
    "print(f\"Parametri totali: {total_params:,}\")\n",
    "print(f\"Percentuale trainable: {100 * trainable_params / total_params:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01eaf86b-d560-4664-b2cb-0e76303a9840",
   "metadata": {},
   "source": [
    "### Considerazioni\n",
    "- DistilBERT base (66.3M parametri) + classification head (1,538 parametri aggiuntivi)\n",
    "- 6 transformer layers, 12 attention heads, 768 dimensioni hidden\n",
    "- Classification head: pre_classifier (768→768) + classifier (768→2) + dropout\n",
    "- \n",
    "Differenza dal baseline: invece di usare features fisse + SVM, ora tutti i 67M parametri si adatteranno specificamente al sentiment analysis task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d109f7bd-955f-4fc4-bc3e-75bd99a17adf",
   "metadata": {},
   "source": [
    "#### Exercise 2.3: Fine-tuning Distilbert\n",
    "\n",
    "Finally. In this exercise you should use a HuggingFace [`Trainer`](https://huggingface.co/docs/transformers/main/en/trainer) to fine-tune your model on the Rotten Tomatoes training split. Setting up the trainer will involve (at least):\n",
    "\n",
    "\n",
    "1. Instantiating a [`DataCollatorWithPadding`](https://huggingface.co/docs/transformers/en/main_classes/data_collator) object which is what *actually* does your batch construction (by padding all sequences to the same length).\n",
    "2. Writing an *evaluation function* that will measure the classification accuracy. This function takes a single argument which is a tuple containing `(logits, labels)` which you should use to compute classification accuracy (and maybe other metrics like F1 score, precision, recall) and return a `dict` with these metrics.  \n",
    "3. Instantiating a [`TrainingArguments`](https://huggingface.co/docs/transformers/v4.51.1/en/main_classes/trainer#transformers.TrainingArguments) object using some reasonable defaults.\n",
    "4. Instantiating a `Trainer` object using your train and validation splits, you data collator, and function to compute performance metrics.\n",
    "5. Calling `trainer.train()`, waiting, waiting some more, and then calling `trainer.evaluate()` to see how it did.\n",
    "\n",
    "**Tip**: When prototyping this laboratory I discovered the HuggingFace [Evaluate library](https://huggingface.co/docs/evaluate/en/index) which provides evaluation metrics. However I found it to have insufferable layers of abstraction and getting actual metrics computed. I suggest just using the Scikit-learn metrics..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ab17178d-5028-47e3-af97-953e8de5aae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training arguments configurati:\n",
      "  Epochs: 3\n",
      "  Train batch size: 16\n",
      "  Eval batch size: 64\n",
      "Configurazione del Trainer...\n",
      "Trainer configurato con successo!\n",
      "\n",
      "============================================================\n",
      "INIZIO FINE-TUNING\n",
      "============================================================\n",
      "Training examples: 8530\n",
      "Validation examples: 1066\n",
      "Steps per epoch: 533\n",
      "Total training steps: 1599\n",
      "\n",
      "Avvio training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Temp\\ipykernel_18496\\2091315759.py:60: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "D:\\Programmi\\anaconda3\\envs\\'transformers'\\Lib\\site-packages\\torch\\utils\\data\\_utils\\pin_memory.py:21: UserWarning: Cannot set number of intraop threads after parallel work has started or after set_num_threads call when using native parallel backend (Triggered internally at C:\\bld\\libtorch_1744233537601\\work\\aten\\src\\ATen\\ParallelNative.cpp:228.)\n",
      "  torch.set_num_threads(1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1602' max='1602' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1602/1602 02:59, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.398200</td>\n",
       "      <td>0.371052</td>\n",
       "      <td>0.842402</td>\n",
       "      <td>0.842346</td>\n",
       "      <td>0.842884</td>\n",
       "      <td>0.842402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.269100</td>\n",
       "      <td>0.413695</td>\n",
       "      <td>0.837711</td>\n",
       "      <td>0.837700</td>\n",
       "      <td>0.837807</td>\n",
       "      <td>0.837711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.091200</td>\n",
       "      <td>0.617461</td>\n",
       "      <td>0.841463</td>\n",
       "      <td>0.841413</td>\n",
       "      <td>0.841898</td>\n",
       "      <td>0.841463</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Programmi\\anaconda3\\envs\\'transformers'\\Lib\\site-packages\\torch\\utils\\data\\_utils\\pin_memory.py:21: UserWarning: Cannot set number of intraop threads after parallel work has started or after set_num_threads call when using native parallel backend (Triggered internally at C:\\bld\\libtorch_1744233537601\\work\\aten\\src\\ATen\\ParallelNative.cpp:228.)\n",
      "  torch.set_num_threads(1)\n",
      "D:\\Programmi\\anaconda3\\envs\\'transformers'\\Lib\\site-packages\\torch\\utils\\data\\_utils\\pin_memory.py:21: UserWarning: Cannot set number of intraop threads after parallel work has started or after set_num_threads call when using native parallel backend (Triggered internally at C:\\bld\\libtorch_1744233537601\\work\\aten\\src\\ATen\\ParallelNative.cpp:228.)\n",
      "  torch.set_num_threads(1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "TRAINING COMPLETATO\n",
      "============================================================\n",
      "Valutazione finale sul validation set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Programmi\\anaconda3\\envs\\'transformers'\\Lib\\site-packages\\torch\\utils\\data\\_utils\\pin_memory.py:21: UserWarning: Cannot set number of intraop threads after parallel work has started or after set_num_threads call when using native parallel backend (Triggered internally at C:\\bld\\libtorch_1744233537601\\work\\aten\\src\\ATen\\ParallelNative.cpp:228.)\n",
      "  torch.set_num_threads(1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='34' max='17' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [17/17 00:10]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Risultati validation:\n",
      "  eval_loss: 0.3711\n",
      "  eval_accuracy: 0.8424\n",
      "  eval_f1: 0.8423\n",
      "  eval_precision: 0.8429\n",
      "  eval_recall: 0.8424\n",
      "  eval_runtime: 9.5888\n",
      "  eval_samples_per_second: 111.1720\n",
      "  eval_steps_per_second: 1.7730\n",
      "  epoch: 3.0000\n",
      "\n",
      "Valutazione sul test set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Programmi\\anaconda3\\envs\\'transformers'\\Lib\\site-packages\\torch\\utils\\data\\_utils\\pin_memory.py:21: UserWarning: Cannot set number of intraop threads after parallel work has started or after set_num_threads call when using native parallel backend (Triggered internally at C:\\bld\\libtorch_1744233537601\\work\\aten\\src\\ATen\\ParallelNative.cpp:228.)\n",
      "  torch.set_num_threads(1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Risultati test:\n",
      "  eval_loss: 0.3988\n",
      "  eval_accuracy: 0.8358\n",
      "  eval_f1: 0.8358\n",
      "  eval_precision: 0.8359\n",
      "  eval_recall: 0.8358\n",
      "  eval_runtime: 9.3502\n",
      "  eval_samples_per_second: 114.0080\n",
      "  eval_steps_per_second: 1.8180\n",
      "  epoch: 3.0000\n",
      "\n",
      "============================================================\n",
      "CONFRONTO CON BASELINE\n",
      "============================================================\n",
      "Baseline (DistilBERT + SVM):\n",
      "  Test Accuracy: 0.7946\n",
      "\n",
      "Fine-tuned DistilBERT:\n",
      "  Test Accuracy: 0.8358\n",
      "\n",
      "Miglioramento: +0.0412 (+4.12%)\n",
      "\n",
      "Salvataggio del modello fine-tuned...\n",
      "Modello salvato in ./distilbert-finetuned-final\n"
     ]
    }
   ],
   "source": [
    "# 2. Data Collator per batch construction con padding\n",
    "data_collator = DataCollatorWithPadding(\n",
    "    tokenizer=tokenizer,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "# 3. Funzione di valutazione\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"\n",
    "    Calcola metriche di classificazione\n",
    "    eval_pred è una tupla (predictions, labels)\n",
    "    \"\"\"\n",
    "    predictions, labels = eval_pred\n",
    "    \n",
    "    # I predictions sono logits, prendiamo la classe con probabilità maggiore\n",
    "    predicted_labels = np.argmax(predictions, axis=1)\n",
    "    \n",
    "    # Calcola metriche usando scikit-learn\n",
    "    accuracy = accuracy_score(labels, predicted_labels)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        labels, predicted_labels, average='weighted'\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }\n",
    "\n",
    "# 4. Training Arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./distilbert-finetuned-sentiment',\n",
    "    num_train_epochs=3,                    # Numero di epoche\n",
    "    per_device_train_batch_size=16,        # Batch size per training\n",
    "    per_device_eval_batch_size=64,         # Batch size per evaluation\n",
    "    warmup_steps=500,                      # Warmup steps per learning rate\n",
    "    weight_decay=0.01,                     # Weight decay per regolarizzazione\n",
    "    logging_dir='./logs',                  # Directory per i log\n",
    "    logging_steps=100,                     # Log ogni 100 steps\n",
    "    eval_strategy=\"epoch\",                 # Valuta alla fine di ogni epoca (renamed from evaluation_strategy)\n",
    "    save_strategy=\"epoch\",                 # Salva alla fine di ogni epoca\n",
    "    load_best_model_at_end=True,          # Carica il miglior modello alla fine\n",
    "    metric_for_best_model=\"accuracy\",      # Metrica per scegliere il miglior modello\n",
    "    greater_is_better=True,                # accuracy maggiore = migliore\n",
    "    save_total_limit=2,                    # Mantieni solo i 2 migliori checkpoint\n",
    "    seed=42,                               # Seed per riproducibilità\n",
    "    dataloader_num_workers=2,              # Worker per caricamento dati\n",
    "    report_to=[],                          # Disabilita reporting (wandb, etc.) - usa lista vuota invece di None\n",
    ")\n",
    "\n",
    "print(f\"Training arguments configurati:\")\n",
    "print(f\"  Epochs: {training_args.num_train_epochs}\")\n",
    "print(f\"  Train batch size: {training_args.per_device_train_batch_size}\")\n",
    "print(f\"  Eval batch size: {training_args.per_device_eval_batch_size}\")\n",
    "\n",
    "# 5. Trainer setup\n",
    "print(\"Configurazione del Trainer...\")\n",
    "trainer = Trainer(\n",
    "    model=model,                           # Il modello da fine-tunare\n",
    "    args=training_args,                    # Training arguments\n",
    "    train_dataset=train_dataset_tokenized, # Dataset di training tokenizzato\n",
    "    eval_dataset=val_dataset_tokenized,    # Dataset di validation tokenizzato\n",
    "    tokenizer=tokenizer,                   # Tokenizer\n",
    "    data_collator=data_collator,           # Data collator per padding\n",
    "    compute_metrics=compute_metrics,       # Funzione per calcolare metriche\n",
    ")\n",
    "\n",
    "print(\"Trainer configurato con successo!\")\n",
    "\n",
    "# 6. Training\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"INIZIO FINE-TUNING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Informazioni prima del training\n",
    "print(f\"Training examples: {len(train_dataset_tokenized)}\")\n",
    "print(f\"Validation examples: {len(val_dataset_tokenized)}\")\n",
    "print(f\"Steps per epoch: {len(train_dataset_tokenized) // training_args.per_device_train_batch_size}\")\n",
    "print(f\"Total training steps: {training_args.num_train_epochs * (len(train_dataset_tokenized) // training_args.per_device_train_batch_size)}\")\n",
    "\n",
    "# Avvia il training\n",
    "print(\"\\nAvvio training...\")\n",
    "trainer.train()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING COMPLETATO\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 7. Evaluation\n",
    "print(\"Valutazione finale sul validation set...\")\n",
    "eval_results = trainer.evaluate()\n",
    "\n",
    "print(\"Risultati validation:\")\n",
    "for key, value in eval_results.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"  {key}: {value:.4f}\")\n",
    "    else:\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "# 8. Test set evaluation\n",
    "print(\"\\nValutazione sul test set...\")\n",
    "test_results = trainer.evaluate(eval_dataset=test_dataset_tokenized)\n",
    "\n",
    "print(\"Risultati test:\")\n",
    "for key, value in test_results.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"  {key}: {value:.4f}\")\n",
    "    else:\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "# 9. Confronto con baseline\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CONFRONTO CON BASELINE\")\n",
    "print(\"=\"*60)\n",
    "print(\"Baseline (DistilBERT + SVM):\")\n",
    "print(f\"  Test Accuracy: 0.7946\")\n",
    "print(f\"\\nFine-tuned DistilBERT:\")\n",
    "print(f\"  Test Accuracy: {test_results.get('eval_accuracy', 'N/A'):.4f}\")\n",
    "\n",
    "if 'eval_accuracy' in test_results:\n",
    "    improvement = test_results['eval_accuracy'] - 0.7946\n",
    "    print(f\"\\nMiglioramento: {improvement:+.4f} ({improvement*100:+.2f}%)\")\n",
    "\n",
    "# 10. Salva il modello finale\n",
    "print(\"\\nSalvataggio del modello fine-tuned...\")\n",
    "trainer.save_model(\"./distilbert-finetuned-final\")\n",
    "tokenizer.save_pretrained(\"./distilbert-finetuned-final\")\n",
    "print(\"Modello salvato in ./distilbert-finetuned-final\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5529ae-dd37-4d71-b43b-60df63eaeb85",
   "metadata": {},
   "source": [
    "### Risultati Ottenuti\n",
    "\n",
    "| Epoca | Loss Training | Loss Validation | Accuratezza Validation |\n",
    "|-------|----------------|------------------|--------------------------|\n",
    "| 1     | 0.398          | 0.371            | 84.24%                   |\n",
    "| 2     | 0.269          | 0.414            | 83.77%                   |\n",
    "| 3     | 0.091          | 0.617            | 84.15%                   |\n",
    "\n",
    "- **Accuratezza finale sul test set**: **83.58%**  \n",
    "- **Loss sul test set**: **0.3988**  \n",
    "- **F1-Score**: **0.8358**\n",
    "\n",
    "---\n",
    "\n",
    "### Conclusioni\n",
    "\n",
    "- Il modello **fine-tuned** ha **superato chiaramente** la baseline ottenuta con l'SVM e le feature statiche di DistilBERT  \n",
    "  (**79.46% → 83.58%**, **+4.12%**)\n",
    "\n",
    "- Il **fine-tuning end-to-end** ha permesso al modello di adattare tutti i suoi **67M parametri** al compito specifico di **sentiment analysis**, ottenendo **rappresentazioni più efficaci** rispetto all’approccio frozen features\n",
    "\n",
    "- L’**andamento della loss di validation**  \n",
    "  (*da 0.371 → 0.414 → 0.617*)  \n",
    "  mentre quella di **training crolla**  \n",
    "  (*da 0.398 → 0.091*)  \n",
    "  indica chiari segni di **overfitting** dalla seconda epoca  \n",
    "  → **2 epoche** potrebbero rappresentare il **compromesso ottimale**\n",
    "\n",
    "- Le **performance bilanciate** (*precision/recall ~83.6%*) dimostrano che il modello **non è biased** verso una classe specifica, mantenendo **robustezza** su entrambi i sentiment\n",
    "\n",
    "In sintesi: il fine-tuning di DistilBERT offre un **netto vantaggio in accuratezza** e **maggiore adattabilità al task**, a fronte di un **maggiore impegno computazionale**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8376de-8554-4a13-aac3-59257f3eb3fd",
   "metadata": {},
   "source": [
    "-----\n",
    "### Exercise 3: Choose at Least One\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b55cf4d-e64b-47fc-b8d5-37288b72d90d",
   "metadata": {},
   "source": [
    "#### Exercise 3.1: Efficient Fine-tuning for Sentiment Analysis (easy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f183856-1111-4fe9-81f1-691fe7c1b706",
   "metadata": {},
   "source": [
    "In Exercise 2 we fine-tuned the *entire* Distilbert model on Rotten Tomatoes. This is expensive, even for a small model. Find an *efficient* way to fine-tune Distilbert on the Rotten Tomatoes dataset (or some other dataset).\n",
    "\n",
    "**Hint**: You could check out the [HuggingFace PEFT library](https://huggingface.co/docs/peft/en/index) for some state-of-the-art approaches that should \"just work\". How else might you go about making fine-tuning more efficient without having to change your training pipeline from above?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "9bb95bd8-70e4-4216-b60d-ce281f3c254f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting peft\n",
      "  Downloading peft-0.15.2-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in d:\\programmi\\anaconda3\\envs\\'transformers'\\lib\\site-packages (from peft) (2.2.4)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\programmi\\anaconda3\\envs\\'transformers'\\lib\\site-packages (from peft) (24.2)\n",
      "Requirement already satisfied: psutil in d:\\programmi\\anaconda3\\envs\\'transformers'\\lib\\site-packages (from peft) (7.0.0)\n",
      "Requirement already satisfied: pyyaml in d:\\programmi\\anaconda3\\envs\\'transformers'\\lib\\site-packages (from peft) (6.0.2)\n",
      "Requirement already satisfied: torch>=1.13.0 in d:\\programmi\\anaconda3\\envs\\'transformers'\\lib\\site-packages (from peft) (2.6.0)\n",
      "Requirement already satisfied: transformers in d:\\programmi\\anaconda3\\envs\\'transformers'\\lib\\site-packages (from peft) (4.51.1)\n",
      "Requirement already satisfied: tqdm in d:\\programmi\\anaconda3\\envs\\'transformers'\\lib\\site-packages (from peft) (4.67.1)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in d:\\programmi\\anaconda3\\envs\\'transformers'\\lib\\site-packages (from peft) (1.5.2)\n",
      "Requirement already satisfied: safetensors in d:\\programmi\\anaconda3\\envs\\'transformers'\\lib\\site-packages (from peft) (0.5.3)\n",
      "Requirement already satisfied: huggingface_hub>=0.25.0 in d:\\programmi\\anaconda3\\envs\\'transformers'\\lib\\site-packages (from peft) (0.30.2)\n",
      "Requirement already satisfied: filelock in d:\\programmi\\anaconda3\\envs\\'transformers'\\lib\\site-packages (from huggingface_hub>=0.25.0->peft) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in d:\\programmi\\anaconda3\\envs\\'transformers'\\lib\\site-packages (from huggingface_hub>=0.25.0->peft) (2024.12.0)\n",
      "Requirement already satisfied: requests in d:\\programmi\\anaconda3\\envs\\'transformers'\\lib\\site-packages (from huggingface_hub>=0.25.0->peft) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in d:\\programmi\\anaconda3\\envs\\'transformers'\\lib\\site-packages (from huggingface_hub>=0.25.0->peft) (4.13.2)\n",
      "Requirement already satisfied: networkx in d:\\programmi\\anaconda3\\envs\\'transformers'\\lib\\site-packages (from torch>=1.13.0->peft) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in d:\\programmi\\anaconda3\\envs\\'transformers'\\lib\\site-packages (from torch>=1.13.0->peft) (3.1.6)\n",
      "Requirement already satisfied: setuptools in d:\\programmi\\anaconda3\\envs\\'transformers'\\lib\\site-packages (from torch>=1.13.0->peft) (75.8.2)\n",
      "Requirement already satisfied: sympy!=1.13.2,>=1.13.1 in d:\\programmi\\anaconda3\\envs\\'transformers'\\lib\\site-packages (from torch>=1.13.0->peft) (1.13.3)\n",
      "Requirement already satisfied: colorama in d:\\programmi\\anaconda3\\envs\\'transformers'\\lib\\site-packages (from tqdm->peft) (0.4.6)\n",
      "Requirement already satisfied: regex!=2019.12.17 in d:\\programmi\\anaconda3\\envs\\'transformers'\\lib\\site-packages (from transformers->peft) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in d:\\programmi\\anaconda3\\envs\\'transformers'\\lib\\site-packages (from transformers->peft) (0.21.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in d:\\programmi\\anaconda3\\envs\\'transformers'\\lib\\site-packages (from sympy!=1.13.2,>=1.13.1->torch>=1.13.0->peft) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\programmi\\anaconda3\\envs\\'transformers'\\lib\\site-packages (from jinja2->torch>=1.13.0->peft) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in d:\\programmi\\anaconda3\\envs\\'transformers'\\lib\\site-packages (from requests->huggingface_hub>=0.25.0->peft) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\programmi\\anaconda3\\envs\\'transformers'\\lib\\site-packages (from requests->huggingface_hub>=0.25.0->peft) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\programmi\\anaconda3\\envs\\'transformers'\\lib\\site-packages (from requests->huggingface_hub>=0.25.0->peft) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\programmi\\anaconda3\\envs\\'transformers'\\lib\\site-packages (from requests->huggingface_hub>=0.25.0->peft) (2025.1.31)\n",
      "Downloading peft-0.15.2-py3-none-any.whl (411 kB)\n",
      "Installing collected packages: peft\n",
      "Successfully installed peft-0.15.2\n"
     ]
    }
   ],
   "source": [
    "!pip install peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "1ba3f780-0acc-452e-9bdc-f4cd1c2bd3dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "Caricamento modello base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configurazione LoRA\n"
     ]
    }
   ],
   "source": [
    "# Exercise 3.1: Efficient Fine-tuning con LoRA (PEFT)\n",
    "\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Device setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Carico il modello base (stesso di prima)\n",
    "print(\"\\nCaricamento modello base\")\n",
    "base_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\",\n",
    "    num_labels=2,\n",
    "    id2label={0: \"NEGATIVE\", 1: \"POSITIVE\"},\n",
    "    label2id={\"NEGATIVE\": 0, \"POSITIVE\": 1}\n",
    ").to(device)\n",
    "\n",
    "# Configurazione LoRA\n",
    "print(\"Configurazione LoRA\")\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_CLS,           # Sequence Classification task\n",
    "    inference_mode=False,                 # Training mode\n",
    "    r=16,                                 # Rank della decomposizione LoRA (default: 8-16)\n",
    "    lora_alpha=32,                        # Scaling parameter (spesso 2x di r)\n",
    "    lora_dropout=0.1,                     # Dropout per LoRA layers\n",
    "    target_modules=[\"q_lin\", \"v_lin\"],    # Target attention layers in DistilBERT\n",
    "    bias=\"none\",                          # Non adattare i bias\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "6ea6bca5-9b36-424e-898c-52c0777eae5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applicazione LoRA al modello\n",
      "\n",
      "============================================================\n",
      "ANALISI PARAMETRI - CONFRONTO EFFICIENZA\n",
      "============================================================\n",
      "FULL FINE-TUNING:\n",
      "  Parametri totali: 67,842,052\n",
      "  Parametri trainable: 67,842,052 (100%)\n",
      "\n",
      "LORA FINE-TUNING:\n",
      "trainable params: 887,042 || all params: 67,842,052 || trainable%: 1.3075\n",
      "\n",
      "EFFICIENZA GUADAGNATA:\n",
      "  Riduzione parametri trainable: 76.5x\n",
      "  Risparmio memoria: 98.7%\n"
     ]
    }
   ],
   "source": [
    "# Applico LoRA al modello\n",
    "print(\"Applicazione LoRA al modello\")\n",
    "lora_model = get_peft_model(base_model, lora_config)\n",
    "\n",
    "# Analisi parametri con metodo PEFT\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ANALISI PARAMETRI - CONFRONTO EFFICIENZA\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"FULL FINE-TUNING:\")\n",
    "full_total = sum(p.numel() for p in base_model.parameters())\n",
    "print(f\"  Parametri totali: {full_total:,}\")\n",
    "print(f\"  Parametri trainable: {full_total:,} (100%)\")\n",
    "\n",
    "print(\"\\nLORA FINE-TUNING:\")\n",
    "lora_model.print_trainable_parameters()\n",
    "\n",
    "# Per calcolare l'efficienza manualmente\n",
    "lora_trainable = sum(p.numel() for p in lora_model.parameters() if p.requires_grad)\n",
    "efficiency_gain = full_total / lora_trainable\n",
    "memory_reduction = (full_total - lora_trainable) / full_total * 100\n",
    "\n",
    "print(f\"\\nEFFICIENZA GUADAGNATA:\")\n",
    "print(f\"  Riduzione parametri trainable: {efficiency_gain:.1f}x\")\n",
    "print(f\"  Risparmio memoria: {memory_reduction:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "1c179c5b-ef84-4c1f-95c7-a7f9dcab388b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inizio training LoRA con Custom Trainer...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1602' max='1602' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1602/1602 01:28, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.415800</td>\n",
       "      <td>0.439776</td>\n",
       "      <td>0.823640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.331100</td>\n",
       "      <td>0.362820</td>\n",
       "      <td>0.840525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.274300</td>\n",
       "      <td>0.373496</td>\n",
       "      <td>0.841463</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completato!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='134' max='67' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [67/67 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Risultati validation: {'eval_loss': 0.37349554896354675, 'eval_accuracy': 0.8414634146341463, 'eval_runtime': 1.4363, 'eval_samples_per_second': 742.164, 'eval_steps_per_second': 46.646, 'epoch': 3.0}\n",
      "Risultati test: {'eval_loss': 0.43192407488822937, 'eval_accuracy': 0.8292682926829268, 'eval_runtime': 1.4608, 'eval_samples_per_second': 729.724, 'eval_steps_per_second': 45.864, 'epoch': 3.0}\n",
      "\n",
      "============================================================\n",
      "CONFRONTO FINALE: FULL vs LORA FINE-TUNING\n",
      "============================================================\n",
      "Full Fine-tuning:\n",
      "  Test Accuracy: 0.8358\n",
      "\n",
      "LoRA Fine-tuning:\n",
      "  Test Accuracy: 0.8293\n",
      "  Differenza accuracy: -0.0065\n"
     ]
    }
   ],
   "source": [
    "# Setup training con LoRA - versione compatibile con PEFT recente\n",
    "from transformers import DataCollatorWithPadding, TrainingArguments, Trainer\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# Custom Trainer per compatibilità PEFT\n",
    "class LoRACompatibleTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        \"\"\"\n",
    "        Override per rimuovere num_items_in_batch che causa problemi con PEFT\n",
    "        \"\"\"\n",
    "        labels = inputs.get(\"labels\")\n",
    "        # Forward pass SENZA num_items_in_batch\n",
    "        outputs = model(**inputs)\n",
    "        \n",
    "        if labels is not None:\n",
    "            loss = outputs.loss\n",
    "        else:\n",
    "            # Se non ci sono labels, calcola loss manualmente\n",
    "            if self.label_smoother is not None and \"labels\" in inputs:\n",
    "                loss = self.label_smoother(outputs, inputs[\"labels\"])\n",
    "            else:\n",
    "                loss = outputs.get(\"loss\")\n",
    "        \n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "# Data collator semplice\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# Compute metrics semplificato\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return {\"accuracy\": accuracy_score(labels, predictions)}\n",
    "\n",
    "# Training arguments semplificati\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./lora-results\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=5e-4,  # LR più alto con LoRA\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    logging_dir=\"./lora-logs\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    report_to=[],\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "# USA IL CUSTOM TRAINER invece del Trainer normale\n",
    "trainer = LoRACompatibleTrainer(\n",
    "    model=lora_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset_tokenized,\n",
    "    eval_dataset=val_dataset_tokenized,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "print(\"Inizio training LoRA con Custom Trainer...\")\n",
    "trainer.train()\n",
    "\n",
    "print(\"Training completato!\")\n",
    "eval_results = trainer.evaluate()\n",
    "print(\"Risultati validation:\", eval_results)\n",
    "\n",
    "test_results = trainer.evaluate(eval_dataset=test_dataset_tokenized)\n",
    "print(\"Risultati test:\", test_results)\n",
    "\n",
    "# Confronto finale\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CONFRONTO FINALE: FULL vs LORA FINE-TUNING\")\n",
    "print(\"=\"*60)\n",
    "print(\"Full Fine-tuning:\")\n",
    "print(f\"  Test Accuracy: 0.8358\")\n",
    "\n",
    "print(f\"\\nLoRA Fine-tuning:\")\n",
    "print(f\"  Test Accuracy: {test_results.get('eval_accuracy', 'N/A'):.4f}\")\n",
    "\n",
    "if 'eval_accuracy' in test_results:\n",
    "    accuracy_diff = test_results['eval_accuracy'] - 0.8358\n",
    "    print(f\"  Differenza accuracy: {accuracy_diff:+.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "341a5303-510e-45a6-92d0-d32cb3c2e326",
   "metadata": {},
   "source": [
    "## Considerazioni Finali \n",
    "\n",
    "### Risultati Ottenuti\n",
    "\n",
    "| Epoca | Loss Training | Loss Validation | Accuratezza Validation |\n",
    "|-------|----------------|------------------|--------------------------|\n",
    "| 1     | 0.416          | 0.440            | 82.36%                   |\n",
    "| 2     | 0.331          | 0.363            | 84.05%                   |\n",
    "| 3     | 0.274          | 0.373            | 84.15%                   |\n",
    "\n",
    "- **Accuratezza finale sul test set**: **82.93%**  \n",
    "- **Loss finale sul test set**: **0.4319**  \n",
    "- **Tempo di training**: **1:28 minuti**\n",
    "\n",
    "---\n",
    "\n",
    "### Efficienza LoRA\n",
    "\n",
    "- **Parametri trainable**: **887,042** su **67,842,052** totali (**1.31%**)  \n",
    "- **Riduzione parametri**: **76.5×** meno parametri da addestrare  \n",
    "- **Risparmio memoria**: **98.7%**\n",
    "\n",
    "---\n",
    "\n",
    "### Confronto delle Performance (SVM vs Full Fine-tuning vs LoRA Fine-tuning)\n",
    "\n",
    "| Approccio        | Tipo di Addestramento                  | Test Accuracy | Parametri Trainable | Tempo Training | Commento                                                                          |\n",
    "|------------------|----------------------------------------|----------------|----------------------|----------------|-----------------------------------------------------------------------------------|\n",
    "| Baseline (SVM)   | Feature extraction + classificatore    | 79.46%         | 0 (DistilBERT frozen)| ~40 secondi     | Veloce ma limitato, buon punto di partenza                                       |\n",
    "| Full Fine-tuning | End-to-end su tutto il modello         | 83.58%         | 67.8M (100%)         | ~3 minuti       | Performance ottimali ma costoso computazionalmente                               |\n",
    "| **LoRA Fine-tuning** | Parameter-efficient con adapter    | **82.93%**     | **0.9M (1.31%)**     | **~1.5 minuti** | **Compromesso ideale**: performance competitive con costi ridotti                |\n",
    "\n",
    "--\n",
    "\n",
    "### Conclusioni\n",
    "\n",
    "- **LoRA dimostra efficacia eccezionale**: raggiunge il **99.2%** delle performance del full fine-tuning utilizzando solo l’**1.31%** dei parametri trainabili\n",
    "- **Ottimo trade-off performance/efficienza**: la perdita di accuracy (**−0.65%** rispetto al full fine-tuning) è **trascurabile** a fronte di un enorme risparmio computazionale\n",
    "- **Velocità superiore**: training **2× più rapido**, con **98.7% di risparmio memoria**\n",
    "- **Approccio pratico**: ideale per applicazioni **production** con vincoli di risorse e necessità di **performance competitive**\n",
    "- **Implementazione robusta**: il **Custom Trainer** ha risolto i problemi di compatibilità tra versioni recenti di `PEFT` e `Transformers`, dimostrando la **flessibilità dell’approccio**\n",
    "\n",
    "**LoRA** si conferma una **tecnica fondamentale** per il **Parameter-Efficient Fine-Tuning**, permettendo l’adattamento efficiente dei modelli **transformer** a task specifici con **costi computazionali minimali**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eeca737-ee00-4d98-a3ae-f4d6eb3d264f",
   "metadata": {},
   "source": [
    "#### Exercise 3.2: Fine-tuning a CLIP Model (harder)\n",
    "\n",
    "Use a (small) CLIP model like [`openai/clip-vit-base-patch16`](https://huggingface.co/openai/clip-vit-base-patch16) and evaluate its zero-shot performance on a small image classification dataset like ImageNette or TinyImageNet. Fine-tune (using a parameter-efficient method!) the CLIP model to see how much improvement you can squeeze out of it.\n",
    "\n",
    "**Note**: There are several ways to adapt the CLIP model; you could fine-tune the image encoder, the text encoder, or both. Or, you could experiment with prompt learning.\n",
    "\n",
    "**Tip**: CLIP probably already works very well on ImageNet and ImageNet-like images. For extra fun, look for an image classification dataset with different image types (e.g. *sketches*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00b59ec2-4fe6-44c6-ab0b-0069f486bbb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe42ed48-444d-47d4-bd8b-839a99e7996a",
   "metadata": {},
   "source": [
    "#### Exercise 3.3: Choose your Own Adventure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196f9129-ef2e-45f7-9e8f-baa697ccd91e",
   "metadata": {},
   "source": [
    "There are a *ton* of interesting and fun models on the HuggingFace hub. Pick one that does something interesting and adapt it in some way to a new task. Or, combine two or more models into something more interesting or fun. The sky's the limit.\n",
    "\n",
    "**Note**: Reach out to me by email or on the Discord if you are unsure about anything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c150bd36-6535-4724-a06d-a61632d3132e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
